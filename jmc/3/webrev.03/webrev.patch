diff a/core/org.openjdk.jmc.flightrecorder.rules.jdk/src/main/resources/org/openjdk/jmc/flightrecorder/rules/jdk/messages/internal/messages.properties b/core/org.openjdk.jmc.flightrecorder.rules.jdk/src/main/resources/org/openjdk/jmc/flightrecorder/rules/jdk/messages/internal/messages.properties
--- a/core/org.openjdk.jmc.flightrecorder.rules.jdk/src/main/resources/org/openjdk/jmc/flightrecorder/rules/jdk/messages/internal/messages.properties
+++ b/core/org.openjdk.jmc.flightrecorder.rules.jdk/src/main/resources/org/openjdk/jmc/flightrecorder/rules/jdk/messages/internal/messages.properties
@@ -1,25 +1,25 @@
 #
 #  Copyright (c) 2018, Oracle and/or its affiliates. All rights reserved.
 #
 #  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 #
-#  The contents of this file are subject to the terms of either the Universal Permissive License 
+#  The contents of this file are subject to the terms of either the Universal Permissive License
 #  v 1.0 as shown at http://oss.oracle.com/licenses/upl
-#   
+#
 #  or the following license:
-#   
+#
 #  Redistribution and use in source and binary forms, with or without modification, are permitted
 #  provided that the following conditions are met:
-#   
+#
 #  1. Redistributions of source code must retain the above copyright notice, this list of conditions
 #  and the following disclaimer.
-#   
+#
 #  2. Redistributions in binary form must reproduce the above copyright notice, this list of
 #  conditions and the following disclaimer in the documentation and/or other materials provided with
 #  the distribution.
-#   
+#
 #  3. Neither the name of the copyright holder nor the names of its contributors may be used to
 #  endorse or promote products derived from this software without specific prior written permission.
 #
 #  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR
 #  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
@@ -33,29 +33,29 @@
 Preference_SHORT_RECORDING=Short recording limit
 Preference_SHORT_RECORDING_LONG=Minimum length of a recording to be considered useful for rules to work on.
 
 AllocationByClassRule_RULE_NAME=Allocated Classes
 # {0} is a class name
-AllocationByClassRule_TEXT_MESSAGE=The most allocated class is likely ''{0}''. This is the most common allocation path for that class: {1}
+AllocationByClassRule_TEXT_MESSAGE=The most allocated type is likely ''{0}'', most commonly allocated by: {1}
 AllocationByThreadRule_RULE_NAME=Threads Allocating
 # {0} is a thread name
-AllocationByThreadRule_TEXT_MESSAGE=The thread performing the most allocation is likely ''{0}''. This is the most common allocation path for that class: {1}
-AllocationRuleFactory_TEXT_CLASS_INFO_LONG=Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.
-AllocationRuleFactory_TEXT_THREAD_INFO_LONG=Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.
+AllocationByThreadRule_TEXT_MESSAGE=The most allocations were likely done by thread ''{0}'' at: {1}
+AllocationRuleFactory_TEXT_CLASS_INFO_LONG=Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.
+AllocationRuleFactory_TEXT_THREAD_INFO_LONG=Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.
 ApplicationHaltsRule_HALTS_INFO_LIMIT=Application halts info limit
 ApplicationHaltsRule_HALTS_INFO_LIMIT_DESC=Ratio between application halts and execution time within a time window needed to trigger an info notice
 ApplicationHaltsRule_HALTS_WARNING_LIMIT=Application halts warning limit
 ApplicationHaltsRule_HALTS_WARNING_LIMIT_DESC=Ratio between application halts and execution time within a time window needed to trigger a warning
 ApplicationHaltsRule_HALTS_WINDOW_SIZE=Application halts time window size
 ApplicationHaltsRule_HALTS_WINDOW_SIZE_DESC=The time window size used when evaluating the rule
 # {0} is a concatenated string of event type ids
-ApplicationHaltsRule_EXTRA_EVENT_TYPES=Enabling the following event types would improve the accuracy of this rule: {0}.
+ApplicationHaltsRule_EXTRA_EVENT_TYPES=Enabling the following event types would improve the accuracy of this rule: {0}
 ApplicationHaltsRule_RULE_NAME=Application Halts
-ApplicationHaltsRule_RULE_TEXT=Application efficiency was affected by halts.
+ApplicationHaltsRule_RULE_TEXT=Application efficiency was affected by halts
 # {0} is a percentage, {1} is a timespan, {2} is a timestamp, {3} is a percentage
-ApplicationHaltsRule_RULE_TEXT_LONG=The highest ratio between application halts and execution time was {0} % during {1} starting at {2}. {3} % of the halts during the timespan were because of other reasons than GCs.<p>The total halts ratio during the entire recording was {4} %. {5} % of the total halts were because of other reasons than GCs.
-ApplicationHaltsRule_RULE_TEXT_RECOMMENDATION=Application halts are often caused by garbage collections, but can also be caused by for example excessive thread dumps or heap dumps. Investigate the VM Operation information and possibly the safepoint specific information.
+ApplicationHaltsRule_RULE_TEXT_LONG=The highest ratio of application halts to execution time was {0} % for {1} at {2}. {3} % of the halts were for reasons other than GC.<p>The halts ratio for the entire recording was {4} %. {5} % of the total halts were for reasons other than GC.</p>
+ApplicationHaltsRule_RULE_TEXT_RECOMMENDATION=Application halts are often caused by garbage collections, but can also be caused by excessive thread dumps or heap dumps. Investigate the VM Operation information and possibly the safepoint specific information.
 ApplicationHaltsRule_RULE_TEXT_OK=Application efficiency was not highly affected by halts.
 
 AutoboxingRule_RULE_NAME=Primitive To Object Conversion
 AutoboxingRule_AUTOBOXING_RATIO_INFO_LIMIT=Primitive to object conversion allocation ratio info limit
 AutoboxingRule_AUTOBOXING_RATIO_INFO_LIMIT_DESC=The ratio between allocation caused by primitive to object conversion compared to the total allocation needed to trigger an info notice.
@@ -128,11 +128,11 @@
 CodeCacheRuleFactory_WARN_SEGMENTED_HEAPS_SHORT_DESCRIPTION=The {0} code heaps reached more than 50 % occupancy during the recording.
 CompareCpuRule_RULE_NAME=Competing CPU Ratio Usage
 CompareCpuRule_TEXT_INFO_LONG=The application performance can be affected when the machine is under heavy load and there are other processes that use CPU or other resources on the same computer. To profile representatively or get higher throughput, shut down other resource intensive processes running on the machine.
 CompareCpuRule_TEXT_TOO_FEW_SAMPLES=Not enough samples available to calculate result.
 # {0} is a time period, {1} is a time stamp, {2} is a percentage
-CompareCpuRule_TEXT_MESSAGE=An average CPU load of {2} was caused by other processes during {0} starting at {1}.
+CompareCpuRule_TEXT_MESSAGE=An average CPU load of {2} was caused by other processes for {0} at {1}.
 CompareCpuRule_INFO_LIMIT=Competing CPU Ratio usage info limit
 CompareCpuRule_INFO_LIMIT_LONG=The amount of CPU used by other processes times the overall CPU usage needed to trigger an info notice
 CompareCpuRule_WARNING_LIMIT=Competing CPU Ratio usage warning limit
 CompareCpuRule_WARNING_LIMIT_LONG=The amount of CPU used by other processes times the overall CPU usage needed to trigger a warning
 CompressedOopsRuleFactory_RULE_NAME=Compressed Oops
@@ -144,11 +144,11 @@
 ContextSwitchRule_AGGR_MAX_BLOCKS=Max Blocks
 ContextSwitchRule_CONFIG_WARNING_LIMIT=Context switch rate warning limit
 ContextSwitchRule_CONFIG_WARNING_LIMIT_LONG=The context switch rate (switches/second) needed to trigger a warning
 ContextSwitchRuleFactory_RULE_NAME=Context Switches
 ContextSwitchRuleFactory_TEXT_INFO=The program causes many context switches during the recording.
-ContextSwitchRuleFactory_TEXT_INFO_LONG=The program context switches a lot and many threads wait on the same monitor. Consider using fewer threads, or try to decrease lock contention by other means.
+ContextSwitchRuleFactory_TEXT_INFO_LONG=The program context switched a lot and many threads waited on the same monitor. Consider using fewer threads, or decreasing lock contention by other means.
 ContextSwitchRuleFactory_TEXT_OK=The program did not context switch excessively during the recording.
 DMSIncidentRule_AGGR_INCIDENTS_COUNT=DMS Incidents
 DMSIncidentRule_AGGR_INCIDENTS_COUNT_DESC=The number of DMS incidents
 DMSIncidentRule_CONFIG_WARNING_LIMIT=DMS incident warning limit
 DMSIncidentRule_CONFIG_WARNING_LIMIT_LONG=The number of DMS incidents needed to trigger a warning
@@ -190,31 +190,31 @@
 ErrorRule_RULE_NAME=Thrown Errors
 ErrorRule_TEXT_OK=The program did not generate any Errors.
 # {0} is a number, {1} is a time range
 ErrorRule_TEXT_WARN=The program generated an average of {0} errors per minute during {1}.
 # {0} is a number, {1} is a time range, {2} is a number, {3} is an error class name, {4} is a number
-ErrorRule_TEXT_WARN_LONG=The program generated an average of {0} errors per minute during {1}, {2} errors were thrown in total.<p>The most common error was ''{3}'', which was thrown {4} times.<p>Investigate the thrown errors to see if they can be avoided. Errors indicate that something went wrong with the code execution and should never be used for flow control.
+ErrorRule_TEXT_WARN_LONG=The program generated an average of {0} errors per minute during {1}. {2} errors were thrown in total.<p>The most common error was ''{3}'', which was thrown {4} times.<p>Investigate the thrown errors to see if they can be avoided. Errors indicate that something went wrong with the code execution and should never be used for flow control.
 # {0} is a regexp exclude string, {1} is a number
 ErrorRule_TEXT_WARN_EXCLUDED_INFO=The following regular expression was used to exclude {1} errors from this rule: ''{0}''.
 ExceptionRule_CONFIG_INFO_LIMIT=Exception rate info limit
 ExceptionRule_CONFIG_INFO_LIMIT_LONG=The number of thrown exceptions per second needed to trigger an info notice
 ExceptionRule_CONFIG_WARN_LIMIT=Exception rate warning limit
 ExceptionRule_CONFIG_WARN_LIMIT_LONG=The number of thrown exceptions per second needed to trigger a warning
 ExceptionRule_RULE_NAME=Thrown Exceptions
 ExceptionRule_TEXT_INFO_LONG=Throwing exceptions is more expensive than normal code execution, which means that they should only be used for exceptional situations. Investigate the thrown exceptions to see if any of them can be avoided with a non-exceptional control flow.
 # {0} is a time period, {1} is a time stamp, {2} is a number
-ExceptionRule_TEXT_MESSAGE=The program generated {2} exceptions per second during {0} starting at {1}.
+ExceptionRule_TEXT_MESSAGE=The program generated {2} exceptions per second for {0} at {1}.
 FatalErrorRule_RULE_NAME=Fatal Errors
 FatalErrorRule_TEXT_OK=The JVM shut down in a normal way.
 FatalErrorRule_TEXT_INFO=The JVM shut down due to there being no remaining non-daemon Java threads.
 FatalErrorRule_TEXT_WARN=The JVM shut down due to a fatal error. This indicates that the program terminated in an abnormal way and should be investigated.
 FasttimeRule_TEXT_WARN=This Flight Recording was made with a JVM with a bug in the Fast Time conversion.
 FasttimeRule_TEXT_WARN_LONG=The timestamps in this recording are unreliable and may not represent the actual data properly or it may cause problems for JMC. Upgrade to Java 7u60 or later to resolve this issue.
 FewSampledThreadsRule_AGGR_SAMPLES_PER_THREAD=Samples per thread
 FewSampledThreadsRule_AGGR_SAMPLES_PER_THREAD_DESC=Samples per thread
 # {0} is a percentage, {1} a time period, {2} a timestamp.
-FewSampledThreadsRule_APPLICATION_IDLE=The application seems to be idle. Max average JVM CPU usage was {0} % during a {1} time window starting at {2}.
+FewSampledThreadsRule_APPLICATION_IDLE=The application seems to be idle. JVM CPU usage was {0} % for {1} at {2}.
 FewSampledThreadsRule_APPLICATION_IDLE_LONG=This may be caused by application latencies, see information from other rules, or it could be that the application is not doing anything.
 FewSampledThreadsRule_CPU_WINDOW_SIZE=CPU window size
 FewSampledThreadsRule_CPU_WINDOW_SIZE_LONG=Sliding window size used when calculating CPU usage.
 FewSampledThreadsRule_MIN_CPU_RATIO=Min used CPU ratio
 FewSampledThreadsRule_MIN_CPU_RATIO_LONG=The minimum used CPU ratio to consider this application not being idle.
@@ -278,23 +278,23 @@
 GcFreedRatioRule_RESULT_OK=This is likely a reasonable amount.
 GcFreedRatioRule_RESULT_NOT_OK=This may be excessive.
 # {0} is a number
 GcFreedRatioRule_RESULT_SHORT_DESCRIPTION=The ratio between memory freed by garbage collections per second and liveset is {0}.
 # {0} is a memory amount, {1} is a timespan, {2} is a timestamp, {3} is a number, {4} is a memory amount
-GcFreedRatioRule_RESULT_LONG_DESCRIPTION={0} per second was freed by garbage collections during {1} starting at {2}. This is {3} times the average liveset which was {4}.
-GcFreedRatioRule_RESULT_MORE_INFO=If the garbage collector can free a lot of memory, it may be because the application allocates a lot of short lived objects. To decrease the allocation rate, investigate the allocation stack traces to see which code paths cause the most allocation.
+GcFreedRatioRule_RESULT_LONG_DESCRIPTION={0} per second was freed by garbage collections for {1} at {2}. This is {3} times the average liveset which was {4}.
+GcFreedRatioRule_RESULT_MORE_INFO=If the garbage collector can free a lot of memory, it may be because the application allocates a lot of short lived objects. Investigate the allocation stack traces to see which code paths cause the most allocations, and see if they can be reduced.
 
 GcPauseRatioRule_INFO_LIMIT=GC pause ratio info limit
 GcPauseRatioRule_INFO_LIMIT_DESC=Ratio between GC pauses and execution time within a time window needed to trigger an info notice
 GcPauseRatioRule_WARNING_LIMIT=GC pause ratio warning limit
 GcPauseRatioRule_WARNING_LIMIT_DESC=Ratio between GC pauses and execution time within a time window needed to trigger a warning
 GcPauseRatioRule_WINDOW_SIZE=GC pause time window size
 GcPauseRatioRule_WINDOW_SIZE_DESC=The time window size used when evaluating the rule
 GcPauseRatioRule_RULE_NAME=GC Pauses
 GcPauseRatioRule_RULE_TEXT=Application efficiency was affected by GC pauses.
 # {0} is a percentage, {1} is a timespan, {2} is a timestamp, {3} is a percentage
-GcPauseRatioRule_RULE_TEXT_LONG=The highest ratio between garbage collection pauses and execution time was {0} % during {1} starting at {2}. The garbage collection pause ratio of the entire recording was {3} %.
+GcPauseRatioRule_RULE_TEXT_LONG=The highest ratio between garbage collection pauses and execution time was {0} % for {1} at {2}. The garbage collection pause ratio of the entire recording was {3} %.
 GcPauseRatioRule_RULE_TEXT_RECOMMENDATION=Pause times may be reduced by increasing the heap size or by trying to reduce allocation.
 GcPauseRatioRule_RULE_TEXT_OK=Application efficiency was not highly affected by GC pauses.
 
 GcLockerRule_CONFIG_WARNING_LIMIT=GC locker ratio limit
 GcLockerRule_CONFIG_WARNING_LIMIT_LONG=Warning limit ratio between number of garbage collections caused by GC locker and total garbage collections
@@ -327,11 +327,11 @@
 HeapInspectionGcRuleFactory_TEXT_OK_LONG=The JVM did not perform any heap inspection GCs. This is good since they usually take a lot of time.
 HeapInspectionRule_CONFIG_WARNING_LIMIT=Heap inspection GC limit
 HeapInspectionRule_CONFIG_WARNING_LIMIT_LONG=Warning limit ratio between number of heap inspection garbage collections and total garbage collections
 HighGcRuleFactory_RULE_NAME=GC Pressure
 # {0} is a time period, {1} is a time stamp, {2} is a percentage
-HighGcRuleFactory_TEXT_INFO=The JVM was paused for {2} of the time during {0} starting at {1}.
+HighGcRuleFactory_TEXT_INFO=The JVM was paused for {2} of the {0} at {1}.
 HighGcRuleFactory_TEXT_INFO_LONG=The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.
 HighGcRuleFactory_TEXT_OK=The runtime did not spend much time performing garbage collections.
 HighJvmCpuRule_AGGR_MAX_ENDTIME=Max End Time
 HighJvmCpuRule_AGGR_MIN_ENDTIME=Min End Time
 HighJvmCpuRule_CONFIG_CPU_INFO_LIMIT=JVM CPU usage info limit
@@ -348,23 +348,23 @@
 HighJvmCpuRule_RULE_NAME=High JVM CPU Load
 HighJvmCpuRule_TEXT_OK=The JVM does not seem to cause a lot of CPU load.
 HighJvmCpuRule_TEXT_WARN=The JVM loads the CPU a lot.
 HotMethodsRuleFactory_NOT_ENOUGH_SAMPLES=Could not extract enough execution samples to calculate a score. Try generating more samples, either by increasing the load on your application, lowering the sample period, or extending the recording time.
 # {0} is a method name, {1} is a percentage, {2} is a duration of time
-HotMethodsRuleFactory_TEXT_INFO=The most sampled method was {0}, with {1} of the maximum possible samples during a {2} window of the recording, and {3} of the actual samples.
+HotMethodsRuleFactory_TEXT_INFO=The most sampled method was {0}, with {1} of the maximum possible samples for {2}, and {3} of the actual samples.
 # {0} is a list of method names with a percentage and a timespan, {1} is the hottest stack trace
-HotMethodsRuleFactory_TEXT_INFO_LONG=The following methods are the most interesting candidates for code optimization: {0}<p>These methods were the most sampled methods during their particular windows of the recording. The percentage shown for each method tells how many execution samples it was seen in compared to the maximum possible number of samples during that window. <p> The most common stack trace was: {1}
+HotMethodsRuleFactory_TEXT_INFO_LONG=The methods that used the most CPU are: {0}<p>Consider optimizing their code.<p> The most common stack trace was: {1}
 HotMethodsRuleFactory_TEXT_OK=No methods where optimization would be particularly efficient could be detected.
 IgnoreUnrecognizedVMOptionsRuleFactory_TEXT_INFO=The JVM ignored unrecognized VM options.
 IgnoreUnrecognizedVMOptionsRuleFactory_TEXT_INFO_LONG=The recording was performed on a JVM that ignored unrecognized VM options, which means that no checking was done on the presence of invalid VM flags. This means that there may be some VM behavior that you think is configured but which is not. This can be because the running VM does not support it, or because the option is misspelled. Unless it is necessary, avoid the '-XX:+IgnoreUnrecognizedVMOptions' command line option.
 # {0} is an object type with a field name, {1} is a number, {2} is another object type with a field name
 IncreasingLiveSetRule_LOADED_CLASSES_PERCENT=Classes Loaded
 IncreasingLiveSetRule_LOADED_CLASSES_PERCENT_DESC=The percentage of loaded classes which indicates that the warm up phase has completed. The rule ignores allocations made before this amount of classes have been loaded.
 IncreasingLiveSetRule_TEXT_INFO_BALANCED=There is no particular class that seems to be leaking more than any other.
 IncreasingLiveSetRule_TEXT_INFO_LONG=Perform a dump with the 'Trace Paths to GC Roots' option enabled to enable a more detailed analysis of the potential memory leak.
 # {0} is a timestamp
-IncreasingLiveSetRule_TEXT_INFO_NO_CANDIDATES=Could not find any leak candidates. This may be because all potential candidates are below the configured thresholds. It may also be a false positive due to long-lived allocations occurring after {0}, which is when the rule assumed the application would not allocate large long-lived objects anymore.
+IncreasingLiveSetRule_TEXT_INFO_NO_CANDIDATES=Did not find any memory leaks. This may be because all potential candidates are below the configured thresholds. It may also be a false positive due to long-lived allocations occurring after {0}, which is when the rule assumed the application would not allocate large long-lived objects anymore.
 IncreasingLiveSetRule_TEXT_INFO_UNBALANCED=There are some classes that seem to leak more than other classes.
 IncreasingLiveSetRule_TEXT_OK=No memory leaks were detected.
 IncreasingLiveSetRule_RELEVANCE_THRESHOLD=Leak Candidate Relevance Threshold
 IncreasingLiveSetRule_RELEVANCE_THRESHOLD_DESC=The calculated relevance threshold to use when determining whether or not a live object is to be considered a memory leak.
 IncreasingLiveSetRule_YOUNG_COLLECTION_THRESHOLD=Young Collection Threshold
@@ -387,16 +387,16 @@
 JavaBlockingRule_CONFIG_EXCLUDED_THREADS=Java blocked excluded thread names
 JavaBlockingRule_CONFIG_EXCLUDED_THREADS_LONG=Regular expression describing which thread names to exclude, for example '(.*weblogic\\.socket\\.Muxer.*|MyExcludedThreadName)'
 # {0} is a regexp exclude string
 JavaBlockingRule_TEXT_EXCLUDED_THREADS=The following regular expression was used to exclude threads from this rule: ''{0}''
 # {0} is a time period
-JavaBlockingRule_TEXT_INFO=Threads in the application were blocked on locks for a total time of {0}.
+JavaBlockingRule_TEXT_INFO=Threads in the application were blocked on locks for a total of {0}.
 JavaBlockingRule_TEXT_MESSAGE=No excessive problems with lock contention found.
 # {0} is a class name, {1} is a number, {2} is a time period
-JavaBlockingRule_TEXT_MOST_BLOCKED_CLASS=The most common monitor class was ''{0}'', which was blocked on {1} times for a total time of {2}.
+JavaBlockingRule_TEXT_MOST_BLOCKED_CLASS=The most blocking monitor class was ''{0}'', which was blocked {1} times for a total of {2}.
 # {0} is a thread name, {1} is a number, {2} is a time period
-JavaBlockingRule_TEXT_MOST_BLOCKED_THREAD=The most common thread was ''{0}'', which was blocked {1} times for a total time of {2}.
+JavaBlockingRule_TEXT_MOST_BLOCKED_THREAD=The most blocking thread was ''{0}'', which was blocked {1} times for a total of {2}.
 JavaBlockingRule_TEXT_OK=No problems with lock contention were found.
 LongGcPauseRuleFactory_RULE_NAME=GC Pause Peak Duration
 # {0} is a time period
 LongGcPauseRuleFactory_TEXT_INFO=The longest GC pause was {0}.
 LongGcPauseRuleFactory_TEXT_INFO_G1=You may want to use the G1 garbage collector which is built to reduce garbage collector pauses. To enable G1, add '-XX:+UseG1GC' to the command line.
@@ -408,13 +408,13 @@
 LongGcPauseRule_CONFIG_INFO_LIMIT_LONG=The GC pause time needed to trigger a warning
 LowOnPhysicalMemoryFactory_RULE_NAME=Free Physical Memory
 # {0} is a percentage
 LowOnPhysicalMemoryFactory_TEXT_INFO=The maximum amount of used memory was {0} of the physical memory available.
 # {0} is a size in bytes, {1} is a percentage, {2} is a size in bytes
-LowOnPhysicalMemoryFactory_TEXT_INFO_LONG=The maximum amount of used memory was {0}. This is {1} of the {2} of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.
+LowOnPhysicalMemoryFactory_TEXT_INFO_LONG=The maximum amount of memory used was {0}. This is {1} of the {2} of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.
 LowOnPhysicalMemoryFactory_TEXT_OK=The system did not run low on physical memory during this recording.
-ManagementAgentRule_TEXT_INFO=Management agent settings (port, authentication and/or SSL) were changed during runtime
+ManagementAgentRule_TEXT_INFO=Management agent settings (port, authentication and/or SSL) were changed during runtime.
 ManagementAgentRule_TEXT_INFO_LONG=Management agent settings (port, authentication and/or SSL) were changed during runtime, this is not likely to have had any effect, but could be useful to investigate.
 ManagmentAgentRuleFactory_RULE_NAME=Discouraged Management Agent Settings
 ManagmentAgentRuleFactory_TEXT_INFO_SSL_DISABLED=Insecure management agent settings: SSL disabled.
 ManagmentAgentRuleFactory_TEXT_INFO_SSL_DISABLED_LONG=The runtime management agent settings were insecure. SSL/TLS was disabled. This is discouraged in production systems, since the traffic will not be secure.
 ManagmentAgentRuleFactory_TEXT_OK=No problems were found with the management agent settings.
@@ -447,11 +447,11 @@
 MethodProfilingRule_WINDOW_SIZE_DESC=The size of the sliding window to use for evaluating the method profiling samples in this recording. If the evaluation of this rule takes a long time, consider increasing this parameter. Note an increased window size may reduce the accuracy of the rule.
 MethodProfilingRule_EXCLUDED_PACKAGES=Packages to exclude from the stack trace
 MethodProfilingRule_EXCLUDED_PACKAGES_DESC=The packages to exclude when traversing stack traces. Drop all frames matching the pattern until reaching the first frame not belonging to either. Count the first encountered frame instead as the hot one.
 # {0} is a number, {1} is a number
 NumberOfGcThreadsRuleFactory_TEXT_INFO=The runtime used {0} GC threads on a machine with {1} CPU cores.
-NumberOfGcThreadsRuleFactory_TEXT_INFO_LONG=It is not optimal to use more GC threads than available cores. Removing the '-XX:ParallelGCThreads' flag will allow the JVM to set the number of GC threads automatically.
+NumberOfGcThreadsRuleFactory_TEXT_INFO_LONG=It's suboptimal to use more GC threads than available cores. Removing the '-XX:ParallelGCThreads' flag will allow the JVM to set the number of GC threads automatically.
 ObjectStatisticsDataProvider_AGGR_LIVE_SIZE_INCREASE=Live Size Increase
 ObjectStatisticsDataProvider_AGGR_LIVE_SIZE_INCREASE_DESC=The difference in total size from after the first garbage collection to after the last
 OptionsCheckRule_CONFIG_ACCEPTED_OPTIONS=Accepted Options
 OptionsCheckRule_CONFIG_ACCEPTED_OPTIONS_LONG=The -XX (non validated) JVM option names in this comma separated list will not be checked by the rule
 OptionsCheckRule_RULE_NAME=Command Line Options Check
@@ -495,16 +495,16 @@
 OverAggressiveRecordingSettingRuleFactory_TEXT_INFO_LONG=Event types without threshold can lead to quite a lot of events being generated, possibly translating to higher overhead. If this was not intended, please check the settings in the template for future recordings.
 ParGcFewThreadsRuleFactory_TEXT_INFO=The JVM ran with a parallel GC but with only one GC thread.
 ParGcFewThreadsRuleFactory_TEXT_INFO_LONG=The JVM ran with a parallel GC but with only one GC thread. This is not optimal. To change number of GC threads, use the command line flag <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/parallel.html">-XX:ParallelGCThreads=X</a>. When run without the '-XX:ParallelGCThreads' flag, the JVM will use a suitable number of GC threads automatically.
 ParallelOnSingleCpuRuleFactory_TEXT_INFO=The runtime used a parallel GC on a single-core machine.
 ParallelOnSingleCpuRuleFactory_TEXT_INFO_LONG=The runtime used a parallel GC on a single-core machine. This is not optimal. Use the <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/collectors.html">Serial Collector</a> instead, which is optimized for single-core machines.
-VerifyNoneRule_WLS_TEXT_INFO=The application was running WebLogic Server with bytecode verification disabled.
-VerifyNoneRule_WLS_TEXT_INFO_LONG=The application was running WebLogic Server with bytecode verification disabled. While not generally recommended, it is considered OK for WLS.
-VerifyNoneRule_TEXT_INFO=The application was running with bytecode verification disabled.
-VerifyNoneRule_TEXT_INFO_LONG=The application was running with bytecode verification disabled. Disabling bytecode verification is unsafe and should not be done in a production system. If it is not necessary for the application, then don't use '-Xverify:none' or '-noverify' on the command line. See the <a href="https://www.securecoding.cert.org/confluence/display/java/ENV04-J.+Do+not+disable+bytecode+verification">Secure Coding Standard for Java</a>.
+VerifyNoneRule_WLS_TEXT_INFO=The application ran WebLogic Server with bytecode verification disabled.
+VerifyNoneRule_WLS_TEXT_INFO_LONG=The application ran WebLogic Server with bytecode verification disabled. While not generally recommended, it is considered OK for WLS.
+VerifyNoneRule_TEXT_INFO=The application ran with bytecode verification disabled.
+VerifyNoneRule_TEXT_INFO_LONG=The application ran with bytecode verification disabled. Disabling bytecode verification is unsafe and should not be done in a production system. If it is not necessary for the application, then don't use '-Xverify:none' or '-noverify' on the command line. See the <a href="https://www.securecoding.cert.org/confluence/display/java/ENV04-J.+Do+not+disable+bytecode+verification">Secure Coding Standard for Java</a>.
 VerifyNoneRule_RULE_NAME=Bytecode Verification
-VerifyNoneRule_TEXT_OK=The application was running with bytecode verification enabled.
+VerifyNoneRule_TEXT_OK=The application ran with bytecode verification enabled.
 DebugNonSafepointsRule_RULE_NAME=DebugNonSafepoints
 DebugNonSafepointsRule_DISABLED_TEXT_INFO=DebugNonSafepoints was explicitly disabled.
 DebugNonSafepointsRule_NOT_ENABLED_TEXT_INFO=DebugNonSafepoints was not enabled.
 DebugNonSafepointsRule_NOT_ENABLED_TEXT_INFO_LONG=If DebugNonSafepoints is not enabled, the method profiling data will be less accurate as threads that are not at safepoints will not be correctly sampled. Use the following JVM flags to enable this: '-XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints'. There is a slight performance overhead when enabling these flags. For more information see <a href="http://openjdk.java.net/groups/hotspot/docs/RuntimeOverview.html#Thread%20Management|outline">HotSpot Runtime Overview/Thread Management</a>.
 DebugNonSafepointsRule_TEXT_OK=DebugNonSafepoints was explicitly enabled.
@@ -566,11 +566,11 @@
 # {0} is a time period, {1} is a host name, {2} is a size in bytes
 SocketWriteRuleFactory_TEXT_WARN_LONG=The longest recorded socket write took {0} to write {2} to the host at {1}.
 StackdepthSettingRule_RULE_NAME=Stackdepth Setting
 StackdepthSettingRule_TEXT_INFO=Some stack traces were truncated in this recording.
 # {0} is a number, {1} is an empty String or StackdepthSettingRule_TEXT_INFO_LONG_DEFAULT, {2} is a percentage, {3} is an HTML list of event type names
-StackdepthSettingRule_TEXT_INFO_LONG=The Flight Recorder only records traces with a depth up to the maximum stack depth value set to {0}. {1}{2} of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the ''-XX:FlightRecorderOptions=stackdepth=&lt;value&gt;'' value.<p>Events of the following types have truncated stack traces:<ul>{3}</ul>
+StackdepthSettingRule_TEXT_INFO_LONG=The Flight Recorder is configured with a maximum captured stack depth of {0}. {1}{2} of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the ''-XX:FlightRecorderOptions=stackdepth=&lt;value&gt;'' value.<p>Events of the following types have truncated stack traces:<ul>{3}</ul>
 StackdepthSettingRule_TEXT_INFO_LONG_DEFAULT=This is the default depth.
 StackdepthSettingRule_TEXT_NA=No events with stack traces were recorded.
 StackdepthSettingRule_TEXT_OK=No stack traces were truncated in this recording.
 # {0} is an event type name, {1} is a percentage, this is a template that is reused inside <li> tags in the StackdepthSettingRule_TEXT_INFO_LONG {1} parameter
 StackdepthSettingRule_TYPE_LIST_TEMPLATE={0} ({1} truncated traces)
@@ -600,26 +600,25 @@
 SystemGcRule_CONFIG_WARNING_LIMIT=System.gc() ratio limit
 SystemGcRule_CONFIG_WARNING_LIMIT_LONG=Warning limit for ratio between System.gc() and total garbage collections
 SystemGcRuleFactory_RULE_NAME=GCs Caused by System.gc()
 # {0} is a percentage
 SystemGcRuleFactory_TEXT_INFO={0} of the garbage collections were caused by System.gc().
-SystemGcRuleFactory_TEXT_INFO_LONG=Calling System.gc() might not be optimal, since it can cause an unnecessary amount of garbage collections. The garbage collectors usually handles this fine by themselves without being explicitly called.<p>To fix this, remove unnecessary System.gc() calls in the code. If the calls are made by libraries where you can't change the source code, you can instead use the command line flag '-XX:+DisableExplicitGC'. This flag makes the JVM ignore all System.gc() calls.
+SystemGcRuleFactory_TEXT_INFO_LONG=Calling System.gc() can cause unnecessary garbage collections. If you're not sure that you need these calls, consider removing them.<p>If the calls are in libraries where you can't change the source code, you can use the command line flag '-XX:+DisableExplicitGC' to make the JVM ignore all System.gc() calls.
 SystemGcRuleFactory_TEXT_OK=No garbage collections were caused by System.gc().
 TlabAllocationRatioRuleFactory_RULE_NAME=TLAB Allocation Ratio
 # {0} is a percentage
 TlabAllocationRatioRuleFactory_TEXT_INFO=The program allocated {0} of the memory outside of TLABs.
 TlabAllocationRatioRuleFactory_TEXT_RECOMMEND_LESS_ALLOCATION=Allocating objects outside of Thread Local Allocation Buffers (TLABs) is more expensive than allocating inside TLABs. This may be acceptable if the individual allocations are intended to be larger than a reasonable TLAB. It may be possible to avoid this by decreasing the size of the individual allocations. There are some TLAB related JVM flags that you can experiment with, but it is usually better to let the JVM manage TLAB sizes automatically.
 TlabAllocationRatioRuleFactory_TEXT_INFO_ONLY_OUTSIDE=The program only allocated objects outside of TLABs.
 TlabAllocationRatioRuleFactory_TEXT_INFO_ONLY_OUTSIDE_LONG=The program only allocated objects outside of Thread Local Allocation Buffers (TLABs).
 TlabAllocationRatioRuleFactory_TEXT_OK_NO_OUTSIDE=No object allocations outside of TLABs detected.
 UnlockExperimentalVMOptionsRuleFactory_TEXT_INFO=The recording was performed on a JVM that had Experimental VM Options enabled.
-UnlockExperimentalVMOptionsRuleFactory_TEXT_INFO_LONG=Due to experimental VM options not being fully supported and may thus be unreliable they should not be used in a production environment. Unless you have to use an experimental option, you should avoid the '-XX:+UnlockExperimentalVMOptions' command line option.
+UnlockExperimentalVMOptionsRuleFactory_TEXT_INFO_LONG=Experimental VM options may be unreliable and should not be used in a production environment. Unless you have to use an experimental option, you should avoid the '-XX:+UnlockExperimentalVMOptions' command line option.
 VMOperations_RULE_NAME=VMOperation Peak Duration
 VMOperationRule_CONFIG_WARNING_LIMIT=Blocking VM operation duration warning limit
 VMOperationRule_CONFIG_WARNING_LIMIT_LONG=The minimum duration for a blocking VM operation needed to trigger a warning
 # {0} is a time period
 VMOperationRuleFactory_TEXT_OK=No excessively long VM operations were found in this recording (the longest was {0}).
 # {0} is a time period
 VMOperationRuleFactory_TEXT_WARN=There are long lasting blocking VM operations in this recording (the longest is {0}).
 # {0} is a time period, {1} is a time stamp, {2} is a thread name, {3} is a time stamp
 VMOperationRuleFactory_TEXT_WARN_LONG=There are long lasting blocking VM operations in this recording. The longest was of type {1} and lasted for {0}. It was initiated from thread ''{2}'' and happened at {3}. <p>VM operations are JVM internal operations. Some VM operations are executed synchronously (i.e. will block the calling thread), and some need to be executed at so called safe points. Safe point polling is a cooperative suspension mechanism that halts byte code execution in the JVM. A VM operation occurring at a safe point will effectively be "stopping the world", meaning that no Java code will be executing in any thread while executing VM operations at that safe point. Long lasting VM operations executing at safe points can decrease the responsiveness of an application. <p>If you do find such VM operations, then the type of operation and its caller thread provide vital information to understand why the VM operation happened. To find more details, check if there is an event in the caller thread intersecting this event time wise. Looking at the stack trace for such an event can help determining what caused it.</p><p>See <a href="http://openjdk.java.net/groups/hotspot/docs/RuntimeOverview.html">Runtime Overview</a> for further information.</p>
-
diff a/core/tests/org.openjdk.jmc.flightrecorder.rules.jdk.test/src/test/resources/baseline/JfrRuleBaseline.xml b/core/tests/org.openjdk.jmc.flightrecorder.rules.jdk.test/src/test/resources/baseline/JfrRuleBaseline.xml
--- a/core/tests/org.openjdk.jmc.flightrecorder.rules.jdk.test/src/test/resources/baseline/JfrRuleBaseline.xml
+++ b/core/tests/org.openjdk.jmc.flightrecorder.rules.jdk.test/src/test/resources/baseline/JfrRuleBaseline.xml
@@ -4,26 +4,26 @@
         <file>allocation_10s_before.jfr</file>
         <rule>
             <id>Allocations.class</id>
             <severity>Information</severity>
             <score>66.39548837411446</score>
-            <shortDescription>The most allocated class is likely 'java.lang.Integer'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Integer.valueOf(int) (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The most allocated class is likely 'java.lang.Integer'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Integer.valueOf(int) (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
+            <shortDescription>The most allocated type is likely 'java.lang.Integer', most commonly allocated by: &lt;ul&gt;&lt;li&gt;Integer.valueOf(int) (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocated type is likely 'java.lang.Integer', most commonly allocated by: &lt;ul&gt;&lt;li&gt;Integer.valueOf(int) (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
         </rule>
         <rule>
             <id>Allocations.thread</id>
             <severity>Information</severity>
             <score>66.40149861711905</score>
-            <shortDescription>The thread performing the most allocation is likely 'main'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Integer.valueOf(int) (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The thread performing the most allocation is likely 'main'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Integer.valueOf(int) (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
+            <shortDescription>The most allocations were likely done by thread 'main' at: &lt;ul&gt;&lt;li&gt;Integer.valueOf(int) (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocations were likely done by thread 'main' at: &lt;ul&gt;&lt;li&gt;Integer.valueOf(int) (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
         </rule>
         <rule>
             <id>ApplicationHalts</id>
             <severity>OK</severity>
             <score>0.9591288888888891</score>
             <shortDescription>Application efficiency was not highly affected by halts.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio between application halts and execution time was 0.192 % during 1 min starting at 4/26/18 12:10:29 PM. 28.068 % of the halts during the timespan were because of other reasons than GCs.&lt;p&gt;The total halts ratio during the entire recording was 1.162 %. 28.068 % of the total halts were because of other reasons than GCs.</longDescription>
+            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio of application halts to execution time was 0.192 % for 1 min at 4/26/18 12:10:29 PM. 28.068 % of the halts were for reasons other than GC.&lt;p&gt;The halts ratio for the entire recording was 1.162 %. 28.068 % of the total halts were for reasons other than GC.&lt;/p&gt;</longDescription>
         </rule>
         <rule>
             <id>BufferLost</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -32,12 +32,12 @@
         </rule>
         <rule>
             <id>BytecodeVerification</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The application was running with bytecode verification enabled.</shortDescription>
-            <longDescription>The application was running with bytecode verification enabled.</longDescription>
+            <shortDescription>The application ran with bytecode verification enabled.</shortDescription>
+            <longDescription>The application ran with bytecode verification enabled.</longDescription>
         </rule>
         <rule>
             <id>ClassLeak</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -60,12 +60,12 @@
         </rule>
         <rule>
             <id>CompareCpu</id>
             <severity>OK</severity>
             <score>0.05434774741663761</score>
-            <shortDescription>An average CPU load of 1 % was caused by other processes during 1.027 s starting at 4/26/18 12:10:33 PM.</shortDescription>
-            <longDescription>An average CPU load of 1 % was caused by other processes during 1.027 s starting at 4/26/18 12:10:33 PM.</longDescription>
+            <shortDescription>An average CPU load of 1 % was caused by other processes for 1.027 s at 4/26/18 12:10:33 PM.</shortDescription>
+            <longDescription>An average CPU load of 1 % was caused by other processes for 1.027 s at 4/26/18 12:10:33 PM.</longDescription>
         </rule>
         <rule>
             <id>CompressedOops</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -123,12 +123,12 @@
         </rule>
         <rule>
             <id>Exceptions</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The program generated 0 exceptions per second during 1.017 s starting at 4/26/18 12:10:30 PM.</shortDescription>
-            <longDescription>The program generated 0 exceptions per second during 1.017 s starting at 4/26/18 12:10:30 PM.</longDescription>
+            <shortDescription>The program generated 0 exceptions per second for 1.017 s at 4/26/18 12:10:30 PM.</shortDescription>
+            <longDescription>The program generated 0 exceptions per second for 1.017 s at 4/26/18 12:10:30 PM.</longDescription>
         </rule>
         <rule>
             <id>Fatal Errors</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -173,11 +173,11 @@
         <rule>
             <id>GcFreedRatio</id>
             <severity>Information</severity>
             <score>34.4635049904725</score>
             <shortDescription>The ratio between memory freed by garbage collections per second and liveset is 65. This may be excessive.</shortDescription>
-            <longDescription>242 MiB per second was freed by garbage collections during 10 s starting at 4/26/18 12:10:29 PM. This is 65.013 times the average liveset which was 3.72 MiB. This may be excessive.&lt;p&gt;If the garbage collector can free a lot of memory, it may be because the application allocates a lot of short lived objects. To decrease the allocation rate, investigate the allocation stack traces to see which code paths cause the most allocation.&lt;p&gt;This recording is only 9.903 s long, consider creating a recording longer than 20 s for improved rule accuracy.</longDescription>
+            <longDescription>242 MiB per second was freed by garbage collections for 10 s at 4/26/18 12:10:29 PM. This is 65.013 times the average liveset which was 3.72 MiB. This may be excessive.&lt;p&gt;If the garbage collector can free a lot of memory, it may be because the application allocates a lot of short lived objects. Investigate the allocation stack traces to see which code paths cause the most allocations, and see if they can be reduced.&lt;p&gt;This recording is only 9.903 s long, consider creating a recording longer than 20 s for improved rule accuracy.</longDescription>
         </rule>
         <rule>
             <id>GcLocker</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -194,11 +194,11 @@
         <rule>
             <id>GcPauseRatio</id>
             <severity>OK</severity>
             <score>0.6899182222222222</score>
             <shortDescription>Application efficiency was not highly affected by GC pauses.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0.138 % during 1 min starting at 4/26/18 12:10:29 PM. The garbage collection pause ratio of the entire recording was 0.836 %.</longDescription>
+            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0.138 % for 1 min at 4/26/18 12:10:29 PM. The garbage collection pause ratio of the entire recording was 0.836 %.</longDescription>
         </rule>
         <rule>
             <id>GcStall</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -221,12 +221,12 @@
         </rule>
         <rule>
             <id>HighGc</id>
             <severity>OK</severity>
             <score>2.5016205991264973</score>
-            <shortDescription>The JVM was paused for 100 % of the time during 11.003 ms starting at 4/26/18 12:10:36 PM.</shortDescription>
-            <longDescription>The JVM was paused for 100 % of the time during 11.003 ms starting at 4/26/18 12:10:36 PM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.</longDescription>
+            <shortDescription>The JVM was paused for 100 % of the 11.003 ms at 4/26/18 12:10:36 PM.</shortDescription>
+            <longDescription>The JVM was paused for 100 % of the 11.003 ms at 4/26/18 12:10:36 PM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.</longDescription>
         </rule>
         <rule>
             <id>HighJvmCpu</id>
             <severity>OK</severity>
             <score>1.3537208124606583</score>
@@ -406,26 +406,26 @@
         <file>allocation_10s_fixed.jfr</file>
         <rule>
             <id>Allocations.class</id>
             <severity>OK</severity>
             <score>0.06321740358971492</score>
-            <shortDescription>The most allocated class is likely 'java.util.HashMap$ValueIterator'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;HashMap$Values.iterator() (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The most allocated class is likely 'java.util.HashMap$ValueIterator'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;HashMap$Values.iterator() (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
+            <shortDescription>The most allocated type is likely 'java.util.HashMap$ValueIterator', most commonly allocated by: &lt;ul&gt;&lt;li&gt;HashMap$Values.iterator() (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocated type is likely 'java.util.HashMap$ValueIterator', most commonly allocated by: &lt;ul&gt;&lt;li&gt;HashMap$Values.iterator() (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
         </rule>
         <rule>
             <id>Allocations.thread</id>
             <severity>OK</severity>
             <score>0.06321740358971492</score>
-            <shortDescription>The thread performing the most allocation is likely 'main'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;HashMap$Values.iterator() (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The thread performing the most allocation is likely 'main'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;HashMap$Values.iterator() (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
+            <shortDescription>The most allocations were likely done by thread 'main' at: &lt;ul&gt;&lt;li&gt;HashMap$Values.iterator() (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocations were likely done by thread 'main' at: &lt;ul&gt;&lt;li&gt;HashMap$Values.iterator() (100 %)&lt;/li&gt;&lt;li&gt;Allocator.go()&lt;/li&gt;&lt;li&gt;Allocator.main(String[])&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
         </rule>
         <rule>
             <id>ApplicationHalts</id>
             <severity>OK</severity>
             <score>0.15440711111111113</score>
             <shortDescription>Application efficiency was not highly affected by halts.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio between application halts and execution time was 0.031 % during 1 min starting at 4/26/18 12:12:46 PM. 100 % of the halts during the timespan were because of other reasons than GCs.&lt;p&gt;The total halts ratio during the entire recording was 0.187 %. 100 % of the total halts were because of other reasons than GCs.</longDescription>
+            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio of application halts to execution time was 0.031 % for 1 min at 4/26/18 12:12:46 PM. 100 % of the halts were for reasons other than GC.&lt;p&gt;The halts ratio for the entire recording was 0.187 %. 100 % of the total halts were for reasons other than GC.&lt;/p&gt;</longDescription>
         </rule>
         <rule>
             <id>BufferLost</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -434,12 +434,12 @@
         </rule>
         <rule>
             <id>BytecodeVerification</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The application was running with bytecode verification enabled.</shortDescription>
-            <longDescription>The application was running with bytecode verification enabled.</longDescription>
+            <shortDescription>The application ran with bytecode verification enabled.</shortDescription>
+            <longDescription>The application ran with bytecode verification enabled.</longDescription>
         </rule>
         <rule>
             <id>ClassLeak</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -462,12 +462,12 @@
         </rule>
         <rule>
             <id>CompareCpu</id>
             <severity>OK</severity>
             <score>0.9473864515664224</score>
-            <shortDescription>An average CPU load of 7 % was caused by other processes during 1.212 s starting at 4/26/18 12:12:47 PM.</shortDescription>
-            <longDescription>An average CPU load of 7 % was caused by other processes during 1.212 s starting at 4/26/18 12:12:47 PM.</longDescription>
+            <shortDescription>An average CPU load of 7 % was caused by other processes for 1.212 s at 4/26/18 12:12:47 PM.</shortDescription>
+            <longDescription>An average CPU load of 7 % was caused by other processes for 1.212 s at 4/26/18 12:12:47 PM.</longDescription>
         </rule>
         <rule>
             <id>CompressedOops</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -525,12 +525,12 @@
         </rule>
         <rule>
             <id>Exceptions</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The program generated 0 exceptions per second during 1.210 s starting at 4/26/18 12:12:47 PM.</shortDescription>
-            <longDescription>The program generated 0 exceptions per second during 1.210 s starting at 4/26/18 12:12:47 PM.</longDescription>
+            <shortDescription>The program generated 0 exceptions per second for 1.210 s at 4/26/18 12:12:47 PM.</shortDescription>
+            <longDescription>The program generated 0 exceptions per second for 1.210 s at 4/26/18 12:12:47 PM.</longDescription>
         </rule>
         <rule>
             <id>Fatal Errors</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -596,11 +596,11 @@
         <rule>
             <id>GcPauseRatio</id>
             <severity>OK</severity>
             <score>0.0</score>
             <shortDescription>Application efficiency was not highly affected by GC pauses.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0 % during 1 min starting at 4/26/18 12:12:46 PM. The garbage collection pause ratio of the entire recording was 0 %.</longDescription>
+            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0 % for 1 min at 4/26/18 12:12:46 PM. The garbage collection pause ratio of the entire recording was 0 %.</longDescription>
         </rule>
         <rule>
             <id>GcStall</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -823,11 +823,11 @@
         <rule>
             <id>ApplicationHalts</id>
             <severity>OK</severity>
             <score>0.0</score>
             <shortDescription>Application efficiency was not highly affected by halts.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio between application halts and execution time was 0 % during 1 min starting at 4/24/18 10:16:27 AM. 0 % of the halts during the timespan were because of other reasons than GCs.&lt;p&gt;The total halts ratio during the entire recording was 0 %. 0 % of the total halts were because of other reasons than GCs.</longDescription>
+            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio of application halts to execution time was 0 % for 1 min at 4/24/18 10:16:27 AM. 0 % of the halts were for reasons other than GC.&lt;p&gt;The halts ratio for the entire recording was 0 %. 0 % of the total halts were for reasons other than GC.&lt;/p&gt;</longDescription>
         </rule>
         <rule>
             <id>BufferLost</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -836,12 +836,12 @@
         </rule>
         <rule>
             <id>BytecodeVerification</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The application was running with bytecode verification enabled.</shortDescription>
-            <longDescription>The application was running with bytecode verification enabled.</longDescription>
+            <shortDescription>The application ran with bytecode verification enabled.</shortDescription>
+            <longDescription>The application ran with bytecode verification enabled.</longDescription>
         </rule>
         <rule>
             <id>ClassLeak</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -998,11 +998,11 @@
         <rule>
             <id>GcPauseRatio</id>
             <severity>OK</severity>
             <score>0.0</score>
             <shortDescription>Application efficiency was not highly affected by GC pauses.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0 % during 1 min starting at 4/24/18 10:16:27 AM. The garbage collection pause ratio of the entire recording was 0 %.</longDescription>
+            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0 % for 1 min at 4/24/18 10:16:27 AM. The garbage collection pause ratio of the entire recording was 0 %.</longDescription>
         </rule>
         <rule>
             <id>GcStall</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -1068,11 +1068,11 @@
         <rule>
             <id>LowOnPhysicalMemory</id>
             <severity>Information</severity>
             <score>51.06889156143796</score>
             <shortDescription>The maximum amount of used memory was 88.9 % of the physical memory available.</shortDescription>
-            <longDescription>The maximum amount of used memory was 28.4 GiB. This is 88.9 % of the 31.9 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.</longDescription>
+            <longDescription>The maximum amount of memory used was 28.4 GiB. This is 88.9 % of the 31.9 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.</longDescription>
         </rule>
         <rule>
             <id>ManagementAgent</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -1210,19 +1210,19 @@
         <file>flight_recording_hidden.jfr</file>
         <rule>
             <id>Allocations.class</id>
             <severity>OK</severity>
             <score>0.09287686767432358</score>
-            <shortDescription>The most allocated class is likely 'byte[]'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;ByteArrayOutputStream.&amp;lt;init&amp;gt;(int) (50 %)&lt;/li&gt;&lt;li&gt;ByteArrayOutputStream.&amp;lt;init&amp;gt;()&lt;/li&gt;&lt;li&gt;JdpPacketWriter.&amp;lt;init&amp;gt;()&lt;/li&gt;&lt;li&gt;JdpJmxPacket.getPacketData()&lt;/li&gt;&lt;li&gt;JdpBroadcaster.sendPacket(JdpPacket)&lt;/li&gt;&lt;li&gt;JdpController$JDPControllerRunner.run()&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The most allocated class is likely 'byte[]'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;ByteArrayOutputStream.&amp;lt;init&amp;gt;(int) (50 %)&lt;/li&gt;&lt;li&gt;ByteArrayOutputStream.&amp;lt;init&amp;gt;()&lt;/li&gt;&lt;li&gt;JdpPacketWriter.&amp;lt;init&amp;gt;()&lt;/li&gt;&lt;li&gt;JdpJmxPacket.getPacketData()&lt;/li&gt;&lt;li&gt;JdpBroadcaster.sendPacket(JdpPacket)&lt;/li&gt;&lt;li&gt;JdpController$JDPControllerRunner.run()&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
+            <shortDescription>The most allocated type is likely 'byte[]', most commonly allocated by: &lt;ul&gt;&lt;li&gt;ByteArrayOutputStream.&amp;lt;init&amp;gt;(int) (50 %)&lt;/li&gt;&lt;li&gt;ByteArrayOutputStream.&amp;lt;init&amp;gt;()&lt;/li&gt;&lt;li&gt;JdpPacketWriter.&amp;lt;init&amp;gt;()&lt;/li&gt;&lt;li&gt;JdpJmxPacket.getPacketData()&lt;/li&gt;&lt;li&gt;JdpBroadcaster.sendPacket(JdpPacket)&lt;/li&gt;&lt;li&gt;JdpController$JDPControllerRunner.run()&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocated type is likely 'byte[]', most commonly allocated by: &lt;ul&gt;&lt;li&gt;ByteArrayOutputStream.&amp;lt;init&amp;gt;(int) (50 %)&lt;/li&gt;&lt;li&gt;ByteArrayOutputStream.&amp;lt;init&amp;gt;()&lt;/li&gt;&lt;li&gt;JdpPacketWriter.&amp;lt;init&amp;gt;()&lt;/li&gt;&lt;li&gt;JdpJmxPacket.getPacketData()&lt;/li&gt;&lt;li&gt;JdpBroadcaster.sendPacket(JdpPacket)&lt;/li&gt;&lt;li&gt;JdpController$JDPControllerRunner.run()&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
         </rule>
         <rule>
             <id>Allocations.thread</id>
             <severity>OK</severity>
             <score>0.1274345677812541</score>
-            <shortDescription>The thread performing the most allocation is likely 'AWT-EventQueue-0'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;EventQueue.dispatchEvent(AWTEvent) (60 %)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpOneEventForFilters(int)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEventsForFilter(int, Conditional, EventFilter)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEventsForHierarchy(int, Conditional, Component)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEvents(int, Conditional)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEvents(Conditional)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The thread performing the most allocation is likely 'AWT-EventQueue-0'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;EventQueue.dispatchEvent(AWTEvent) (60 %)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpOneEventForFilters(int)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEventsForFilter(int, Conditional, EventFilter)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEventsForHierarchy(int, Conditional, Component)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEvents(int, Conditional)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEvents(Conditional)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
+            <shortDescription>The most allocations were likely done by thread 'AWT-EventQueue-0' at: &lt;ul&gt;&lt;li&gt;EventQueue.dispatchEvent(AWTEvent) (60 %)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpOneEventForFilters(int)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEventsForFilter(int, Conditional, EventFilter)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEventsForHierarchy(int, Conditional, Component)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEvents(int, Conditional)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEvents(Conditional)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocations were likely done by thread 'AWT-EventQueue-0' at: &lt;ul&gt;&lt;li&gt;EventQueue.dispatchEvent(AWTEvent) (60 %)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpOneEventForFilters(int)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEventsForFilter(int, Conditional, EventFilter)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEventsForHierarchy(int, Conditional, Component)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEvents(int, Conditional)&lt;/li&gt;&lt;li&gt;EventDispatchThread.pumpEvents(Conditional)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
         </rule>
         <rule>
             <id>ApplicationHalts</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -1238,12 +1238,12 @@
         </rule>
         <rule>
             <id>BytecodeVerification</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The application was running with bytecode verification enabled.</shortDescription>
-            <longDescription>The application was running with bytecode verification enabled.</longDescription>
+            <shortDescription>The application ran with bytecode verification enabled.</shortDescription>
+            <longDescription>The application ran with bytecode verification enabled.</longDescription>
         </rule>
         <rule>
             <id>ClassLeak</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -1266,12 +1266,12 @@
         </rule>
         <rule>
             <id>CompareCpu</id>
             <severity>Warning</severity>
             <score>77.36067322482806</score>
-            <shortDescription>An average CPU load of 60 % was caused by other processes during 8.047 s starting at 4/25/18 11:38:36 AM.</shortDescription>
-            <longDescription>An average CPU load of 60 % was caused by other processes during 8.047 s starting at 4/25/18 11:38:36 AM.&lt;p&gt;The application performance can be affected when the machine is under heavy load and there are other processes that use CPU or other resources on the same computer. To profile representatively or get higher throughput, shut down other resource intensive processes running on the machine.</longDescription>
+            <shortDescription>An average CPU load of 60 % was caused by other processes for 8.047 s at 4/25/18 11:38:36 AM.</shortDescription>
+            <longDescription>An average CPU load of 60 % was caused by other processes for 8.047 s at 4/25/18 11:38:36 AM.&lt;p&gt;The application performance can be affected when the machine is under heavy load and there are other processes that use CPU or other resources on the same computer. To profile representatively or get higher throughput, shut down other resource intensive processes running on the machine.</longDescription>
         </rule>
         <rule>
             <id>CompressedOops</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -1329,12 +1329,12 @@
         </rule>
         <rule>
             <id>Exceptions</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The program generated 0 exceptions per second during 1.000 s starting at 4/25/18 11:38:36 AM.</shortDescription>
-            <longDescription>The program generated 0 exceptions per second during 1.000 s starting at 4/25/18 11:38:36 AM.</longDescription>
+            <shortDescription>The program generated 0 exceptions per second for 1.000 s at 4/25/18 11:38:36 AM.</shortDescription>
+            <longDescription>The program generated 0 exceptions per second for 1.000 s at 4/25/18 11:38:36 AM.</longDescription>
         </rule>
         <rule>
             <id>Fatal Errors</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -1470,11 +1470,11 @@
         <rule>
             <id>LowOnPhysicalMemory</id>
             <severity>Warning</severity>
             <score>85.24971081406179</score>
             <shortDescription>The maximum amount of used memory was 99.8 % of the physical memory available.</shortDescription>
-            <longDescription>The maximum amount of used memory was 16 GiB. This is 99.8 % of the 16 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.</longDescription>
+            <longDescription>The maximum amount of memory used was 16 GiB. This is 99.8 % of the 16 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.</longDescription>
         </rule>
         <rule>
             <id>ManagementAgent</id>
             <severity>Warning</severity>
             <score>100.0</score>
@@ -2416,26 +2416,26 @@
         <file>parallel-gc_cpu.jfr</file>
         <rule>
             <id>Allocations.class</id>
             <severity>OK</severity>
             <score>0.02670579099297634</score>
-            <shortDescription>The most allocated class is likely 'java.util.ArrayList'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;JFRImpl.getRecordings() (100 %)&lt;/li&gt;&lt;li&gt;MetaProducer.onNewChunk()&lt;/li&gt;&lt;li&gt;JFRImpl.onNewChunk()&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The most allocated class is likely 'java.util.ArrayList'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;JFRImpl.getRecordings() (100 %)&lt;/li&gt;&lt;li&gt;MetaProducer.onNewChunk()&lt;/li&gt;&lt;li&gt;JFRImpl.onNewChunk()&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
+            <shortDescription>The most allocated type is likely 'java.util.ArrayList', most commonly allocated by: &lt;ul&gt;&lt;li&gt;JFRImpl.getRecordings() (100 %)&lt;/li&gt;&lt;li&gt;MetaProducer.onNewChunk()&lt;/li&gt;&lt;li&gt;JFRImpl.onNewChunk()&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocated type is likely 'java.util.ArrayList', most commonly allocated by: &lt;ul&gt;&lt;li&gt;JFRImpl.getRecordings() (100 %)&lt;/li&gt;&lt;li&gt;MetaProducer.onNewChunk()&lt;/li&gt;&lt;li&gt;JFRImpl.onNewChunk()&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
         </rule>
         <rule>
             <id>Allocations.thread</id>
             <severity>OK</severity>
             <score>0.07261458520440227</score>
-            <shortDescription>The thread performing the most allocation is likely 'RMI TCP Connection(1)-127.0.0.1'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;ObjectInputStream$BlockDataInputStream.&amp;lt;init&amp;gt;(ObjectInputStream, InputStream) (50 %)&lt;/li&gt;&lt;li&gt;ObjectInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;MarshalInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;ConnectionInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;StreamRemoteCall.getInputStream()&lt;/li&gt;&lt;li&gt;Transport.serviceCall(RemoteCall)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The thread performing the most allocation is likely 'RMI TCP Connection(1)-127.0.0.1'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;ObjectInputStream$BlockDataInputStream.&amp;lt;init&amp;gt;(ObjectInputStream, InputStream) (50 %)&lt;/li&gt;&lt;li&gt;ObjectInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;MarshalInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;ConnectionInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;StreamRemoteCall.getInputStream()&lt;/li&gt;&lt;li&gt;Transport.serviceCall(RemoteCall)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
+            <shortDescription>The most allocations were likely done by thread 'RMI TCP Connection(1)-127.0.0.1' at: &lt;ul&gt;&lt;li&gt;ObjectInputStream$BlockDataInputStream.&amp;lt;init&amp;gt;(ObjectInputStream, InputStream) (50 %)&lt;/li&gt;&lt;li&gt;ObjectInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;MarshalInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;ConnectionInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;StreamRemoteCall.getInputStream()&lt;/li&gt;&lt;li&gt;Transport.serviceCall(RemoteCall)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocations were likely done by thread 'RMI TCP Connection(1)-127.0.0.1' at: &lt;ul&gt;&lt;li&gt;ObjectInputStream$BlockDataInputStream.&amp;lt;init&amp;gt;(ObjectInputStream, InputStream) (50 %)&lt;/li&gt;&lt;li&gt;ObjectInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;MarshalInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;ConnectionInputStream.&amp;lt;init&amp;gt;(InputStream)&lt;/li&gt;&lt;li&gt;StreamRemoteCall.getInputStream()&lt;/li&gt;&lt;li&gt;Transport.serviceCall(RemoteCall)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
         </rule>
         <rule>
             <id>ApplicationHalts</id>
             <severity>OK</severity>
             <score>0.0</score>
             <shortDescription>Application efficiency was not highly affected by halts.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio between application halts and execution time was 0 % during 1 min starting at 4/4/14 11:17:05 AM. 0 % of the halts during the timespan were because of other reasons than GCs.&lt;p&gt;The total halts ratio during the entire recording was 0 %. 0 % of the total halts were because of other reasons than GCs.&lt;p&gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin.</longDescription>
+            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio of application halts to execution time was 0 % for 1 min at 4/4/14 11:17:05 AM. 0 % of the halts were for reasons other than GC.&lt;p&gt;The halts ratio for the entire recording was 0 %. 0 % of the total halts were for reasons other than GC.&lt;/p&gt;&lt;p&gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin</longDescription>
         </rule>
         <rule>
             <id>BufferLost</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -2444,12 +2444,12 @@
         </rule>
         <rule>
             <id>BytecodeVerification</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The application was running with bytecode verification enabled.</shortDescription>
-            <longDescription>The application was running with bytecode verification enabled.</longDescription>
+            <shortDescription>The application ran with bytecode verification enabled.</shortDescription>
+            <longDescription>The application ran with bytecode verification enabled.</longDescription>
         </rule>
         <rule>
             <id>ClassLeak</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -2472,12 +2472,12 @@
         </rule>
         <rule>
             <id>CompareCpu</id>
             <severity>OK</severity>
             <score>9.313225746154785E-7</score>
-            <shortDescription>An average CPU load of 0 % was caused by other processes during 1.011 s starting at 4/4/14 11:17:07 AM.</shortDescription>
-            <longDescription>An average CPU load of 0 % was caused by other processes during 1.011 s starting at 4/4/14 11:17:07 AM.</longDescription>
+            <shortDescription>An average CPU load of 0 % was caused by other processes for 1.011 s at 4/4/14 11:17:07 AM.</shortDescription>
+            <longDescription>An average CPU load of 0 % was caused by other processes for 1.011 s at 4/4/14 11:17:07 AM.</longDescription>
         </rule>
         <rule>
             <id>CompressedOops</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -2535,12 +2535,12 @@
         </rule>
         <rule>
             <id>Exceptions</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The program generated 0 exceptions per second during 999.487 ms starting at 4/4/14 11:17:06 AM.</shortDescription>
-            <longDescription>The program generated 0 exceptions per second during 999.487 ms starting at 4/4/14 11:17:06 AM.</longDescription>
+            <shortDescription>The program generated 0 exceptions per second for 999.487 ms at 4/4/14 11:17:06 AM.</shortDescription>
+            <longDescription>The program generated 0 exceptions per second for 999.487 ms at 4/4/14 11:17:06 AM.</longDescription>
         </rule>
         <rule>
             <id>Fatal Errors</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -2599,18 +2599,18 @@
         <rule>
             <id>GcOptions</id>
             <severity>Information</severity>
             <score>50.0</score>
             <shortDescription>The runtime used 2 GC threads on a machine with 1 CPU cores.</shortDescription>
-            <longDescription>The runtime used 2 GC threads on a machine with 1 CPU cores. It is not optimal to use more GC threads than available cores. Removing the '-XX:ParallelGCThreads' flag will allow the JVM to set the number of GC threads automatically.</longDescription>
+            <longDescription>The runtime used 2 GC threads on a machine with 1 CPU cores. It's suboptimal to use more GC threads than available cores. Removing the '-XX:ParallelGCThreads' flag will allow the JVM to set the number of GC threads automatically.</longDescription>
         </rule>
         <rule>
             <id>GcPauseRatio</id>
             <severity>OK</severity>
             <score>0.0</score>
             <shortDescription>Application efficiency was not highly affected by GC pauses.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0 % during 1 min starting at 4/4/14 11:17:05 AM. The garbage collection pause ratio of the entire recording was 0 %.</longDescription>
+            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0 % for 1 min at 4/4/14 11:17:05 AM. The garbage collection pause ratio of the entire recording was 0 %.</longDescription>
         </rule>
         <rule>
             <id>GcStall</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -2818,26 +2818,26 @@
         <file>parallel-on-singlecpu.jfr</file>
         <rule>
             <id>Allocations.class</id>
             <severity>OK</severity>
             <score>0.031334794765092246</score>
-            <shortDescription>The most allocated class is likely 'java.lang.Object[]'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;AbstractCollection.toArray() (100 %)&lt;/li&gt;&lt;li&gt;ArrayList.&amp;lt;init&amp;gt;(Collection)&lt;/li&gt;&lt;li&gt;JFRImpl.getRecordings()&lt;/li&gt;&lt;li&gt;MetaProducer.onNewChunk()&lt;/li&gt;&lt;li&gt;JFRImpl.onNewChunk()&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The most allocated class is likely 'java.lang.Object[]'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;AbstractCollection.toArray() (100 %)&lt;/li&gt;&lt;li&gt;ArrayList.&amp;lt;init&amp;gt;(Collection)&lt;/li&gt;&lt;li&gt;JFRImpl.getRecordings()&lt;/li&gt;&lt;li&gt;MetaProducer.onNewChunk()&lt;/li&gt;&lt;li&gt;JFRImpl.onNewChunk()&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
+            <shortDescription>The most allocated type is likely 'java.lang.Object[]', most commonly allocated by: &lt;ul&gt;&lt;li&gt;AbstractCollection.toArray() (100 %)&lt;/li&gt;&lt;li&gt;ArrayList.&amp;lt;init&amp;gt;(Collection)&lt;/li&gt;&lt;li&gt;JFRImpl.getRecordings()&lt;/li&gt;&lt;li&gt;MetaProducer.onNewChunk()&lt;/li&gt;&lt;li&gt;JFRImpl.onNewChunk()&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocated type is likely 'java.lang.Object[]', most commonly allocated by: &lt;ul&gt;&lt;li&gt;AbstractCollection.toArray() (100 %)&lt;/li&gt;&lt;li&gt;ArrayList.&amp;lt;init&amp;gt;(Collection)&lt;/li&gt;&lt;li&gt;JFRImpl.getRecordings()&lt;/li&gt;&lt;li&gt;MetaProducer.onNewChunk()&lt;/li&gt;&lt;li&gt;JFRImpl.onNewChunk()&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
         </rule>
         <rule>
             <id>Allocations.thread</id>
             <severity>OK</severity>
             <score>0.031334794765092246</score>
-            <shortDescription>The thread performing the most allocation is likely 'RMI TCP Connection(1)-127.0.0.1'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Throwable.fillInStackTrace(int) (100 %)&lt;/li&gt;&lt;li&gt;Throwable.fillInStackTrace()&lt;/li&gt;&lt;li&gt;Throwable.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;Exception.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;ReflectiveOperationException.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;NoSuchFieldException.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The thread performing the most allocation is likely 'RMI TCP Connection(1)-127.0.0.1'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Throwable.fillInStackTrace(int) (100 %)&lt;/li&gt;&lt;li&gt;Throwable.fillInStackTrace()&lt;/li&gt;&lt;li&gt;Throwable.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;Exception.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;ReflectiveOperationException.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;NoSuchFieldException.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
+            <shortDescription>The most allocations were likely done by thread 'RMI TCP Connection(1)-127.0.0.1' at: &lt;ul&gt;&lt;li&gt;Throwable.fillInStackTrace(int) (100 %)&lt;/li&gt;&lt;li&gt;Throwable.fillInStackTrace()&lt;/li&gt;&lt;li&gt;Throwable.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;Exception.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;ReflectiveOperationException.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;NoSuchFieldException.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocations were likely done by thread 'RMI TCP Connection(1)-127.0.0.1' at: &lt;ul&gt;&lt;li&gt;Throwable.fillInStackTrace(int) (100 %)&lt;/li&gt;&lt;li&gt;Throwable.fillInStackTrace()&lt;/li&gt;&lt;li&gt;Throwable.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;Exception.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;ReflectiveOperationException.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;li&gt;NoSuchFieldException.&amp;lt;init&amp;gt;(String)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
         </rule>
         <rule>
             <id>ApplicationHalts</id>
             <severity>OK</severity>
             <score>0.0</score>
             <shortDescription>Application efficiency was not highly affected by halts.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio between application halts and execution time was 0 % during 1 min starting at 4/4/14 8:54:33 AM. 0 % of the halts during the timespan were because of other reasons than GCs.&lt;p&gt;The total halts ratio during the entire recording was 0 %. 0 % of the total halts were because of other reasons than GCs.&lt;p&gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin.</longDescription>
+            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio of application halts to execution time was 0 % for 1 min at 4/4/14 8:54:33 AM. 0 % of the halts were for reasons other than GC.&lt;p&gt;The halts ratio for the entire recording was 0 %. 0 % of the total halts were for reasons other than GC.&lt;/p&gt;&lt;p&gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin</longDescription>
         </rule>
         <rule>
             <id>BufferLost</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -2846,12 +2846,12 @@
         </rule>
         <rule>
             <id>BytecodeVerification</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The application was running with bytecode verification enabled.</shortDescription>
-            <longDescription>The application was running with bytecode verification enabled.</longDescription>
+            <shortDescription>The application ran with bytecode verification enabled.</shortDescription>
+            <longDescription>The application ran with bytecode verification enabled.</longDescription>
         </rule>
         <rule>
             <id>ClassLeak</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -2874,12 +2874,12 @@
         </rule>
         <rule>
             <id>CompareCpu</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>An average CPU load of 0 % was caused by other processes during 1.010 s starting at 4/4/14 8:54:36 AM.</shortDescription>
-            <longDescription>An average CPU load of 0 % was caused by other processes during 1.010 s starting at 4/4/14 8:54:36 AM.</longDescription>
+            <shortDescription>An average CPU load of 0 % was caused by other processes for 1.010 s at 4/4/14 8:54:36 AM.</shortDescription>
+            <longDescription>An average CPU load of 0 % was caused by other processes for 1.010 s at 4/4/14 8:54:36 AM.</longDescription>
         </rule>
         <rule>
             <id>CompressedOops</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -2937,12 +2937,12 @@
         </rule>
         <rule>
             <id>Exceptions</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The program generated 0 exceptions per second during 1.009 s starting at 4/4/14 8:54:34 AM.</shortDescription>
-            <longDescription>The program generated 0 exceptions per second during 1.009 s starting at 4/4/14 8:54:34 AM.</longDescription>
+            <shortDescription>The program generated 0 exceptions per second for 1.009 s at 4/4/14 8:54:34 AM.</shortDescription>
+            <longDescription>The program generated 0 exceptions per second for 1.009 s at 4/4/14 8:54:34 AM.</longDescription>
         </rule>
         <rule>
             <id>Fatal Errors</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -3008,11 +3008,11 @@
         <rule>
             <id>GcPauseRatio</id>
             <severity>OK</severity>
             <score>0.0</score>
             <shortDescription>Application efficiency was not highly affected by GC pauses.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0 % during 1 min starting at 4/4/14 8:54:33 AM. The garbage collection pause ratio of the entire recording was 0 %.</longDescription>
+            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0 % for 1 min at 4/4/14 8:54:33 AM. The garbage collection pause ratio of the entire recording was 0 %.</longDescription>
         </rule>
         <rule>
             <id>GcStall</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -3235,11 +3235,11 @@
         <rule>
             <id>ApplicationHalts</id>
             <severity>OK</severity>
             <score>0.4097173333333334</score>
             <shortDescription>Application efficiency was not highly affected by halts.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio between application halts and execution time was 0.082 % during 1 min starting at 4/24/18 10:08:52 AM. 51.552 % of the halts during the timespan were because of other reasons than GCs.&lt;p&gt;The total halts ratio during the entire recording was 1.003 %. 51.552 % of the total halts were because of other reasons than GCs.</longDescription>
+            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio of application halts to execution time was 0.082 % for 1 min at 4/24/18 10:08:52 AM. 51.552 % of the halts were for reasons other than GC.&lt;p&gt;The halts ratio for the entire recording was 1.003 %. 51.552 % of the total halts were for reasons other than GC.&lt;/p&gt;</longDescription>
         </rule>
         <rule>
             <id>BufferLost</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -3248,12 +3248,12 @@
         </rule>
         <rule>
             <id>BytecodeVerification</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The application was running with bytecode verification enabled.</shortDescription>
-            <longDescription>The application was running with bytecode verification enabled.</longDescription>
+            <shortDescription>The application ran with bytecode verification enabled.</shortDescription>
+            <longDescription>The application ran with bytecode verification enabled.</longDescription>
         </rule>
         <rule>
             <id>ClassLeak</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -3276,12 +3276,12 @@
         </rule>
         <rule>
             <id>CompareCpu</id>
             <severity>OK</severity>
             <score>3.3944677554909966</score>
-            <shortDescription>An average CPU load of 15 % was caused by other processes during 1.060 s starting at 4/24/18 10:08:54 AM.</shortDescription>
-            <longDescription>An average CPU load of 15 % was caused by other processes during 1.060 s starting at 4/24/18 10:08:54 AM.</longDescription>
+            <shortDescription>An average CPU load of 15 % was caused by other processes for 1.060 s at 4/24/18 10:08:54 AM.</shortDescription>
+            <longDescription>An average CPU load of 15 % was caused by other processes for 1.060 s at 4/24/18 10:08:54 AM.</longDescription>
         </rule>
         <rule>
             <id>CompressedOops</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -3339,12 +3339,12 @@
         </rule>
         <rule>
             <id>Exceptions</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The program generated 0 exceptions per second during 1.054 s starting at 4/24/18 10:08:53 AM.</shortDescription>
-            <longDescription>The program generated 0 exceptions per second during 1.054 s starting at 4/24/18 10:08:53 AM.</longDescription>
+            <shortDescription>The program generated 0 exceptions per second for 1.054 s at 4/24/18 10:08:53 AM.</shortDescription>
+            <longDescription>The program generated 0 exceptions per second for 1.054 s at 4/24/18 10:08:53 AM.</longDescription>
         </rule>
         <rule>
             <id>Fatal Errors</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -3410,11 +3410,11 @@
         <rule>
             <id>GcPauseRatio</id>
             <severity>OK</severity>
             <score>0.1985013333333333</score>
             <shortDescription>Application efficiency was not highly affected by GC pauses.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0.04 % during 1 min starting at 4/24/18 10:08:52 AM. The garbage collection pause ratio of the entire recording was 0.486 %.</longDescription>
+            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 0.04 % for 1 min at 4/24/18 10:08:52 AM. The garbage collection pause ratio of the entire recording was 0.486 %.</longDescription>
         </rule>
         <rule>
             <id>GcStall</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -3437,12 +3437,12 @@
         </rule>
         <rule>
             <id>HighGc</id>
             <severity>OK</severity>
             <score>2.054924803137792</score>
-            <shortDescription>The JVM was paused for 100 % of the time during 9.017 ms starting at 4/24/18 10:08:53 AM.</shortDescription>
-            <longDescription>The JVM was paused for 100 % of the time during 9.017 ms starting at 4/24/18 10:08:53 AM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.&lt;p&gt;To improve rule accuracy and/or get more details for further investigation, it is recommended to enable the following event types: 'Allocation in new TLAB', 'Allocation outside TLAB'.</longDescription>
+            <shortDescription>The JVM was paused for 100 % of the 9.017 ms at 4/24/18 10:08:53 AM.</shortDescription>
+            <longDescription>The JVM was paused for 100 % of the 9.017 ms at 4/24/18 10:08:53 AM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.&lt;p&gt;To improve rule accuracy and/or get more details for further investigation, it is recommended to enable the following event types: 'Allocation in new TLAB', 'Allocation outside TLAB'.</longDescription>
         </rule>
         <rule>
             <id>HighJvmCpu</id>
             <severity>OK</severity>
             <score>1.4885947328122275</score>
@@ -3480,11 +3480,11 @@
         <rule>
             <id>LowOnPhysicalMemory</id>
             <severity>Information</severity>
             <score>64.30361523113581</score>
             <shortDescription>The maximum amount of used memory was 91.8 % of the physical memory available.</shortDescription>
-            <longDescription>The maximum amount of used memory was 29.3 GiB. This is 91.8 % of the 31.9 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.</longDescription>
+            <longDescription>The maximum amount of memory used was 29.3 GiB. This is 91.8 % of the 31.9 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.</longDescription>
         </rule>
         <rule>
             <id>ManagementAgent</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -3622,26 +3622,26 @@
         <file>wldf.jfr</file>
         <rule>
             <id>Allocations.class</id>
             <severity>Information</severity>
             <score>46.750432285649964</score>
-            <shortDescription>The most allocated class is likely 'char[]'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Arrays.copyOfRange(char[], int, int) (44.9 %)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The most allocated class is likely 'char[]'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Arrays.copyOfRange(char[], int, int) (44.9 %)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
+            <shortDescription>The most allocated type is likely 'char[]', most commonly allocated by: &lt;ul&gt;&lt;li&gt;Arrays.copyOfRange(char[], int, int) (44.9 %)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocated type is likely 'char[]', most commonly allocated by: &lt;ul&gt;&lt;li&gt;Arrays.copyOfRange(char[], int, int) (44.9 %)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.</longDescription>
         </rule>
         <rule>
             <id>Allocations.thread</id>
             <severity>OK</severity>
             <score>24.595150095883856</score>
-            <shortDescription>The thread performing the most allocation is likely '[ACTIVE] ExecuteThread: &amp;#39;5&amp;#39; for queue: &amp;#39;weblogic.kernel.Default (self-tuning)&amp;#39;'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Arrays.copyOfRange(char[], int, int) (53.3 %)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
-            <longDescription>The thread performing the most allocation is likely '[ACTIVE] ExecuteThread: &amp;#39;5&amp;#39; for queue: &amp;#39;weblogic.kernel.Default (self-tuning)&amp;#39;'. This is the most common allocation path for that class: &lt;ul&gt;&lt;li&gt;Arrays.copyOfRange(char[], int, int) (53.3 %)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
+            <shortDescription>The most allocations were likely done by thread '[ACTIVE] ExecuteThread: &amp;#39;5&amp;#39; for queue: &amp;#39;weblogic.kernel.Default (self-tuning)&amp;#39;' at: &lt;ul&gt;&lt;li&gt;Arrays.copyOfRange(char[], int, int) (53.3 %)&lt;/li&gt;&lt;/ul&gt;</shortDescription>
+            <longDescription>The most allocations were likely done by thread '[ACTIVE] ExecuteThread: &amp;#39;5&amp;#39; for queue: &amp;#39;weblogic.kernel.Default (self-tuning)&amp;#39;' at: &lt;ul&gt;&lt;li&gt;Arrays.copyOfRange(char[], int, int) (53.3 %)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.</longDescription>
         </rule>
         <rule>
             <id>ApplicationHalts</id>
             <severity>OK</severity>
             <score>11.086075208333334</score>
             <shortDescription>Application efficiency was not highly affected by halts.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio between application halts and execution time was 2.217 % during 1 min starting at 9/24/15 10:08:56 AM. 0.027 % of the halts during the timespan were because of other reasons than GCs.&lt;p&gt;The total halts ratio during the entire recording was 1.882 %. 0.021 % of the total halts were because of other reasons than GCs.&lt;p&gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin.</longDescription>
+            <longDescription>Application efficiency was not highly affected by halts.&lt;p&gt;The highest ratio of application halts to execution time was 2.217 % for 1 min at 9/24/15 10:08:56 AM. 0.027 % of the halts were for reasons other than GC.&lt;p&gt;The halts ratio for the entire recording was 1.882 %. 0.021 % of the total halts were for reasons other than GC.&lt;/p&gt;&lt;p&gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin</longDescription>
         </rule>
         <rule>
             <id>BufferLost</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -3650,12 +3650,12 @@
         </rule>
         <rule>
             <id>BytecodeVerification</id>
             <severity>OK</severity>
             <score>1.0</score>
-            <shortDescription>The application was running WebLogic Server with bytecode verification disabled.</shortDescription>
-            <longDescription>The application was running WebLogic Server with bytecode verification disabled. While not generally recommended, it is considered OK for WLS.</longDescription>
+            <shortDescription>The application ran WebLogic Server with bytecode verification disabled.</shortDescription>
+            <longDescription>The application ran WebLogic Server with bytecode verification disabled. While not generally recommended, it is considered OK for WLS.</longDescription>
         </rule>
         <rule>
             <id>ClassLeak</id>
             <severity>Information</severity>
             <score>74.99995410111316</score>
@@ -3678,12 +3678,12 @@
         </rule>
         <rule>
             <id>CompareCpu</id>
             <severity>Warning</severity>
             <score>87.07809686085879</score>
-            <shortDescription>An average CPU load of 54 % was caused by other processes during 4.938 s starting at 9/24/15 10:08:17 AM.</shortDescription>
-            <longDescription>An average CPU load of 54 % was caused by other processes during 4.938 s starting at 9/24/15 10:08:17 AM.&lt;p&gt;The application performance can be affected when the machine is under heavy load and there are other processes that use CPU or other resources on the same computer. To profile representatively or get higher throughput, shut down other resource intensive processes running on the machine.</longDescription>
+            <shortDescription>An average CPU load of 54 % was caused by other processes for 4.938 s at 9/24/15 10:08:17 AM.</shortDescription>
+            <longDescription>An average CPU load of 54 % was caused by other processes for 4.938 s at 9/24/15 10:08:17 AM.&lt;p&gt;The application performance can be affected when the machine is under heavy load and there are other processes that use CPU or other resources on the same computer. To profile representatively or get higher throughput, shut down other resource intensive processes running on the machine.</longDescription>
         </rule>
         <rule>
             <id>CompressedOops</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -3735,18 +3735,18 @@
         <rule>
             <id>Errors</id>
             <severity>OK</severity>
             <score>14.166666666666668</score>
             <shortDescription>The program generated an average of 17 errors per minute during 9/24/2015 10:08:14 AM – 10:09:14 AM.</shortDescription>
-            <longDescription>The program generated an average of 17 errors per minute during 9/24/2015 10:08:14 AM – 10:09:14 AM, 17 errors were thrown in total.&lt;p&gt;The most common error was 'java.lang.NoSuchMethodError', which was thrown 13 times.&lt;p&gt;Investigate the thrown errors to see if they can be avoided. Errors indicate that something went wrong with the code execution and should never be used for flow control. The following regular expression was used to exclude 381 errors from this rule: '(com.sun.el.parser.ELParser\$LookaheadSuccess)'.</longDescription>
+            <longDescription>The program generated an average of 17 errors per minute during 9/24/2015 10:08:14 AM – 10:09:14 AM. 17 errors were thrown in total.&lt;p&gt;The most common error was 'java.lang.NoSuchMethodError', which was thrown 13 times.&lt;p&gt;Investigate the thrown errors to see if they can be avoided. Errors indicate that something went wrong with the code execution and should never be used for flow control. The following regular expression was used to exclude 381 errors from this rule: '(com.sun.el.parser.ELParser\$LookaheadSuccess)'.</longDescription>
         </rule>
         <rule>
             <id>Exceptions</id>
             <severity>OK</severity>
             <score>2.573808918421875</score>
-            <shortDescription>The program generated 515 exceptions per second during 28.060 s starting at 9/24/15 10:08:58 AM.</shortDescription>
-            <longDescription>The program generated 515 exceptions per second during 28.060 s starting at 9/24/15 10:08:58 AM.</longDescription>
+            <shortDescription>The program generated 515 exceptions per second for 28.060 s at 9/24/15 10:08:58 AM.</shortDescription>
+            <longDescription>The program generated 515 exceptions per second for 28.060 s at 9/24/15 10:08:58 AM.</longDescription>
         </rule>
         <rule>
             <id>Fatal Errors</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
@@ -3791,11 +3791,11 @@
         <rule>
             <id>GcFreedRatio</id>
             <severity>OK</severity>
             <score>1.4318707405222828</score>
             <shortDescription>The ratio between memory freed by garbage collections per second and liveset is 0.5. This is likely a reasonable amount.</shortDescription>
-            <longDescription>61.4 MiB per second was freed by garbage collections during 10 s starting at 9/24/15 10:09:18 AM. This is 0.474 times the average liveset which was 130 MiB. This is likely a reasonable amount.</longDescription>
+            <longDescription>61.4 MiB per second was freed by garbage collections for 10 s at 9/24/15 10:09:18 AM. This is 0.474 times the average liveset which was 130 MiB. This is likely a reasonable amount.</longDescription>
         </rule>
         <rule>
             <id>GcLocker</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -3812,11 +3812,11 @@
         <rule>
             <id>GcPauseRatio</id>
             <severity>OK</severity>
             <score>11.083072308333335</score>
             <shortDescription>Application efficiency was not highly affected by GC pauses.</shortDescription>
-            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 2.217 % during 1 min starting at 9/24/15 10:08:56 AM. The garbage collection pause ratio of the entire recording was 1.882 %.</longDescription>
+            <longDescription>Application efficiency was not highly affected by GC pauses.&lt;p&gt;The highest ratio between garbage collection pauses and execution time was 2.217 % for 1 min at 9/24/15 10:08:56 AM. The garbage collection pause ratio of the entire recording was 1.882 %.</longDescription>
         </rule>
         <rule>
             <id>GcStall</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -3839,12 +3839,12 @@
         </rule>
         <rule>
             <id>HighGc</id>
             <severity>Information</severity>
             <score>72.91379413109293</score>
-            <shortDescription>The JVM was paused for 100 % of the time during 567.258 ms starting at 9/24/15 10:07:58 AM.</shortDescription>
-            <longDescription>The JVM was paused for 100 % of the time during 567.258 ms starting at 9/24/15 10:07:58 AM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.</longDescription>
+            <shortDescription>The JVM was paused for 100 % of the 567.258 ms at 9/24/15 10:07:58 AM.</shortDescription>
+            <longDescription>The JVM was paused for 100 % of the 567.258 ms at 9/24/15 10:07:58 AM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.</longDescription>
         </rule>
         <rule>
             <id>HighJvmCpu</id>
             <severity>OK</severity>
             <score>13.536580012735357</score>
@@ -3867,12 +3867,12 @@
         </rule>
         <rule>
             <id>JavaBlocking</id>
             <severity>Information</severity>
             <score>62.05152332723372</score>
-            <shortDescription>Threads in the application were blocked on locks for a total time of 1 min 26 s.</shortDescription>
-            <longDescription>Threads in the application were blocked on locks for a total time of 1 min 26 s. The most common monitor class was 'Logger', which was blocked on 1,612 times for a total time of 1 min 23 s.&lt;p&gt;The following regular expression was used to exclude threads from this rule: '(.*weblogic\.socket\.Muxer.*)'</longDescription>
+            <shortDescription>Threads in the application were blocked on locks for a total of 1 min 26 s.</shortDescription>
+            <longDescription>Threads in the application were blocked on locks for a total of 1 min 26 s. The most blocking monitor class was 'Logger', which was blocked 1,612 times for a total of 1 min 23 s.&lt;p&gt;The following regular expression was used to exclude threads from this rule: '(.*weblogic\.socket\.Muxer.*)'</longDescription>
         </rule>
         <rule>
             <id>LongGcPause</id>
             <severity>OK</severity>
             <score>15.646056210732606</score>
@@ -3973,11 +3973,11 @@
         <rule>
             <id>StackdepthSetting</id>
             <severity>Warning</severity>
             <score>99.99654514135385</score>
             <shortDescription>Some stack traces were truncated in this recording.</shortDescription>
-            <longDescription>Some stack traces were truncated in this recording.&lt;p&gt;The Flight Recorder only records traces with a depth up to the maximum stack depth value set to 64. This is the default depth. 35.7 % of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the '-XX:FlightRecorderOptions=stackdepth=&amp;lt;value&amp;gt;' value.&lt;p&gt;Events of the following types have truncated stack traces:&lt;ul&gt;&lt;li&gt;Allocation in new TLAB (42.4 % truncated traces)&lt;/li&gt;&lt;li&gt;Class Load (42.8 % truncated traces)&lt;/li&gt;&lt;li&gt;Allocation outside TLAB (60.7 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Pool Manager Pre Invoke (47.1 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Invoke (47.1 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Post Invoke Cleanup (47.1 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Pool Manager Post Invoke (47.1 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Post Invoke (44.4 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Pre Invoke (44.4 % truncated traces)&lt;/li&gt;&lt;li&gt;Servlet Execute (26.2 % truncated traces)&lt;/li&gt;&lt;li&gt;Servlet Request Dispatch (45 % truncated traces)&lt;/li&gt;&lt;li&gt;Method Profiling Sample (24.3 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction Start (80 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Start (100 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Creation (54.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Prepare (54.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Execute (54.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Execute Begin (54.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Close (73 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction End (51.5 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction Commit (51 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Release (65 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Commit (65 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction End (65.6 % truncated traces)&lt;/li&gt;&lt;li&gt;Socket Read (41.6 % truncated traces)&lt;/li&gt;&lt;li&gt;File Write (32.6 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Error (29.1 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXRPC Client Request (45.4 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXRPC Client Response (100 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Monitor Blocked (1.09 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB PoolManager Create (32.7 % truncated traces)&lt;/li&gt;&lt;li&gt;Allocation Requiring GC (31.6 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Is Same RM (6.59 % truncated traces)&lt;/li&gt;&lt;li&gt;Servlet Response Write Headers (0.753 % truncated traces)&lt;/li&gt;&lt;li&gt;Socket Write (20.6 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Monitor Wait (0.134 % truncated traces)&lt;/li&gt;&lt;li&gt;File Read (100 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Thread Sleep (33.3 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Thread Park (0.223 % truncated traces)&lt;/li&gt;&lt;/ul&gt;</longDescription>
+            <longDescription>Some stack traces were truncated in this recording.&lt;p&gt;The Flight Recorder is configured with a maximum captured stack depth of 64. This is the default depth. 35.7 % of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the '-XX:FlightRecorderOptions=stackdepth=&amp;lt;value&amp;gt;' value.&lt;p&gt;Events of the following types have truncated stack traces:&lt;ul&gt;&lt;li&gt;Allocation in new TLAB (42.4 % truncated traces)&lt;/li&gt;&lt;li&gt;Class Load (42.8 % truncated traces)&lt;/li&gt;&lt;li&gt;Allocation outside TLAB (60.7 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Pool Manager Pre Invoke (47.1 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Invoke (47.1 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Post Invoke Cleanup (47.1 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Pool Manager Post Invoke (47.1 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Post Invoke (44.4 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Pre Invoke (44.4 % truncated traces)&lt;/li&gt;&lt;li&gt;Servlet Execute (26.2 % truncated traces)&lt;/li&gt;&lt;li&gt;Servlet Request Dispatch (45 % truncated traces)&lt;/li&gt;&lt;li&gt;Method Profiling Sample (24.3 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction Start (80 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Start (100 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Creation (54.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Prepare (54.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Execute (54.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Execute Begin (54.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Close (73 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction End (51.5 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction Commit (51 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Release (65 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Commit (65 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction End (65.6 % truncated traces)&lt;/li&gt;&lt;li&gt;Socket Read (41.6 % truncated traces)&lt;/li&gt;&lt;li&gt;File Write (32.6 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Error (29.1 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXRPC Client Request (45.4 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXRPC Client Response (100 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Monitor Blocked (1.09 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB PoolManager Create (32.7 % truncated traces)&lt;/li&gt;&lt;li&gt;Allocation Requiring GC (31.6 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Is Same RM (6.59 % truncated traces)&lt;/li&gt;&lt;li&gt;Servlet Response Write Headers (0.753 % truncated traces)&lt;/li&gt;&lt;li&gt;Socket Write (20.6 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Monitor Wait (0.134 % truncated traces)&lt;/li&gt;&lt;li&gt;File Read (100 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Thread Sleep (33.3 % truncated traces)&lt;/li&gt;&lt;li&gt;Java Thread Park (0.223 % truncated traces)&lt;/li&gt;&lt;/ul&gt;</longDescription>
         </rule>
         <rule>
             <id>StringDeduplication</id>
             <severity>OK</severity>
             <score>11.944505390960535</score>
@@ -4054,12 +4054,12 @@
         </rule>
         <rule>
             <id>BytecodeVerification</id>
             <severity>OK</severity>
             <score>0.0</score>
-            <shortDescription>The application was running with bytecode verification enabled.</shortDescription>
-            <longDescription>The application was running with bytecode verification enabled.</longDescription>
+            <shortDescription>The application ran with bytecode verification enabled.</shortDescription>
+            <longDescription>The application ran with bytecode verification enabled.</longDescription>
         </rule>
         <rule>
             <id>ClassLeak</id>
             <severity>OK</severity>
             <score>0.0</score>
@@ -4377,11 +4377,11 @@
         <rule>
             <id>StackdepthSetting</id>
             <severity>Warning</severity>
             <score>99.35136572520041</score>
             <shortDescription>Some stack traces were truncated in this recording.</shortDescription>
-            <longDescription>Some stack traces were truncated in this recording.&lt;p&gt;The Flight Recorder only records traces with a depth up to the maximum stack depth value set to 64. This is the default depth. 17.5 % of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the '-XX:FlightRecorderOptions=stackdepth=&amp;lt;value&amp;gt;' value.&lt;p&gt;Events of the following types have truncated stack traces:&lt;ul&gt;&lt;li&gt;EJB Pool Manager Pre Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Post Invoke Cleanup (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Pool Manager Post Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Post Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Pre Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Close (96.8 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Creation (100 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Prepare (100 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Execute (96.7 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Execute Begin (96.7 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction Start (16.7 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Start (100 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Pool Manager Create (75 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Release (85.7 % truncated traces)&lt;/li&gt;&lt;li&gt;Debug (1.59 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Commit (45 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction End (45 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction End (6.06 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction Commit (36.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Is Same RM (1.79 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXWS Resource (50 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXWS Endpoint (100 % truncated traces)&lt;/li&gt;&lt;li&gt;Servlet Response Write Headers (1.87 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXRPC Client Request (50 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXRPC Client Response (100 % truncated traces)&lt;/li&gt;&lt;/ul&gt;</longDescription>
+            <longDescription>Some stack traces were truncated in this recording.&lt;p&gt;The Flight Recorder is configured with a maximum captured stack depth of 64. This is the default depth. 17.5 % of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the '-XX:FlightRecorderOptions=stackdepth=&amp;lt;value&amp;gt;' value.&lt;p&gt;Events of the following types have truncated stack traces:&lt;ul&gt;&lt;li&gt;EJB Pool Manager Pre Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Post Invoke Cleanup (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Pool Manager Post Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Post Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Business Method Pre Invoke (71.3 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Close (96.8 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Creation (100 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Prepare (100 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Execute (96.7 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Statement Execute Begin (96.7 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction Start (16.7 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Start (100 % truncated traces)&lt;/li&gt;&lt;li&gt;EJB Pool Manager Create (75 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Connection Release (85.7 % truncated traces)&lt;/li&gt;&lt;li&gt;Debug (1.59 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Commit (45 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction End (45 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction End (6.06 % truncated traces)&lt;/li&gt;&lt;li&gt;JTA Transaction Commit (36.4 % truncated traces)&lt;/li&gt;&lt;li&gt;JDBC Transaction Is Same RM (1.79 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXWS Resource (50 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXWS Endpoint (100 % truncated traces)&lt;/li&gt;&lt;li&gt;Servlet Response Write Headers (1.87 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXRPC Client Request (50 % truncated traces)&lt;/li&gt;&lt;li&gt;Webservices JAXRPC Client Response (100 % truncated traces)&lt;/li&gt;&lt;/ul&gt;</longDescription>
         </rule>
         <rule>
             <id>StringDeduplication</id>
             <severity>Not Applicable</severity>
             <score>-1.0</score>
