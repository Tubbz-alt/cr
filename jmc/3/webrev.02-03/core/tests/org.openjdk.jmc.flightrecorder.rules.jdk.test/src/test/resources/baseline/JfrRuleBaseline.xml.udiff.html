<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff core/tests/org.openjdk.jmc.flightrecorder.rules.jdk.test/src/test/resources/baseline/JfrRuleBaseline.xml</title>
    <link rel="stylesheet" href="../../../../../../../style.css" />
  </head>
<body>
<center>&lt; prev <a href="../../../../../../../index.html" target="_top">index</a> next &gt;</center>    <h2>core/tests/org.openjdk.jmc.flightrecorder.rules.jdk.test/src/test/resources/baseline/JfrRuleBaseline.xml</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -4,26 +4,26 @@</span>
          &lt;file&gt;allocation_10s_before.jfr&lt;/file&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.class&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;66.39548837411446&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The most allocated class is likely &#39;java.lang.Integer&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Integer.valueOf(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The most allocated class is likely &#39;java.lang.Integer&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Integer.valueOf(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocated type is likely &#39;java.lang.Integer&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Integer.valueOf(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocated type is likely &#39;java.lang.Integer&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Integer.valueOf(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.thread&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;66.40149861711905&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The thread performing the most allocation is likely &#39;main&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Integer.valueOf(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The thread performing the most allocation is likely &#39;main&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Integer.valueOf(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocations were likely done by thread &#39;main&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Integer.valueOf(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocations were likely done by thread &#39;main&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Integer.valueOf(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ApplicationHalts&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.9591288888888891&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by halts.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio between application halts and execution time was 0.192 % during 1 min starting at 4/26/18 12:10:29 PM. 28.068 % of the halts during the timespan were because of other reasons than GCs.&amp;lt;p&amp;gt;The total halts ratio during the entire recording was 1.162 %. 28.068 % of the total halts were because of other reasons than GCs.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio of application halts to execution time was 0.192 % for 1 min at 4/26/18 12:10:29 PM. 28.068 % of the halts were for reasons other than GC.&amp;lt;p&amp;gt;The halts ratio for the entire recording was 1.162 %. 28.068 % of the total halts were for reasons other than GC.&amp;lt;/p&amp;gt;&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BufferLost&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -32,12 +32,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BytecodeVerification&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The application was running with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The application was running with bytecode verification enabled.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The application ran with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The application ran with bytecode verification enabled.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ClassLeak&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -60,12 +60,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompareCpu&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.05434774741663761&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;An average CPU load of 1 % was caused by other processes during 1.027 s starting at 4/26/18 12:10:33 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;An average CPU load of 1 % was caused by other processes during 1.027 s starting at 4/26/18 12:10:33 PM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;An average CPU load of 1 % was caused by other processes for 1.027 s at 4/26/18 12:10:33 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;An average CPU load of 1 % was caused by other processes for 1.027 s at 4/26/18 12:10:33 PM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompressedOops&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -123,12 +123,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Exceptions&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The program generated 0 exceptions per second during 1.017 s starting at 4/26/18 12:10:30 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The program generated 0 exceptions per second during 1.017 s starting at 4/26/18 12:10:30 PM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The program generated 0 exceptions per second for 1.017 s at 4/26/18 12:10:30 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The program generated 0 exceptions per second for 1.017 s at 4/26/18 12:10:30 PM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Fatal Errors&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -173,11 +173,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;GcFreedRatio&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;34.4635049904725&lt;/score&gt;
              &lt;shortDescription&gt;The ratio between memory freed by garbage collections per second and liveset is 65. This may be excessive.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;242 MiB per second was freed by garbage collections during 10 s starting at 4/26/18 12:10:29 PM. This is 65.013 times the average liveset which was 3.72 MiB. This may be excessive.&amp;lt;p&amp;gt;If the garbage collector can free a lot of memory, it may be because the application allocates a lot of short lived objects. To decrease the allocation rate, investigate the allocation stack traces to see which code paths cause the most allocation.&amp;lt;p&amp;gt;This recording is only 9.903 s long, consider creating a recording longer than 20 s for improved rule accuracy.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;242 MiB per second was freed by garbage collections for 10 s at 4/26/18 12:10:29 PM. This is 65.013 times the average liveset which was 3.72 MiB. This may be excessive.&amp;lt;p&amp;gt;If the garbage collector can free a lot of memory, it may be because the application allocates a lot of short lived objects. Investigate the allocation stack traces to see which code paths cause the most allocations, and see if they can be reduced.&amp;lt;p&amp;gt;This recording is only 9.903 s long, consider creating a recording longer than 20 s for improved rule accuracy.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcLocker&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -194,11 +194,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;GcPauseRatio&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.6899182222222222&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by GC pauses.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0.138 % during 1 min starting at 4/26/18 12:10:29 PM. The garbage collection pause ratio of the entire recording was 0.836 %.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0.138 % for 1 min at 4/26/18 12:10:29 PM. The garbage collection pause ratio of the entire recording was 0.836 %.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcStall&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -221,12 +221,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;HighGc&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;2.5016205991264973&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The JVM was paused for 100 % of the time during 11.003 ms starting at 4/26/18 12:10:36 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The JVM was paused for 100 % of the time during 11.003 ms starting at 4/26/18 12:10:36 PM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The JVM was paused for 100 % of the 11.003 ms at 4/26/18 12:10:36 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The JVM was paused for 100 % of the 11.003 ms at 4/26/18 12:10:36 PM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;HighJvmCpu&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;1.3537208124606583&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -406,26 +406,26 @@</span>
          &lt;file&gt;allocation_10s_fixed.jfr&lt;/file&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.class&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.06321740358971492&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The most allocated class is likely &#39;java.util.HashMap$ValueIterator&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;HashMap$Values.iterator() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The most allocated class is likely &#39;java.util.HashMap$ValueIterator&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;HashMap$Values.iterator() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocated type is likely &#39;java.util.HashMap$ValueIterator&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;HashMap$Values.iterator() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocated type is likely &#39;java.util.HashMap$ValueIterator&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;HashMap$Values.iterator() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.thread&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.06321740358971492&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The thread performing the most allocation is likely &#39;main&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;HashMap$Values.iterator() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The thread performing the most allocation is likely &#39;main&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;HashMap$Values.iterator() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocations were likely done by thread &#39;main&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;HashMap$Values.iterator() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocations were likely done by thread &#39;main&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;HashMap$Values.iterator() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.go()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocator.main(String[])&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ApplicationHalts&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.15440711111111113&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by halts.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio between application halts and execution time was 0.031 % during 1 min starting at 4/26/18 12:12:46 PM. 100 % of the halts during the timespan were because of other reasons than GCs.&amp;lt;p&amp;gt;The total halts ratio during the entire recording was 0.187 %. 100 % of the total halts were because of other reasons than GCs.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio of application halts to execution time was 0.031 % for 1 min at 4/26/18 12:12:46 PM. 100 % of the halts were for reasons other than GC.&amp;lt;p&amp;gt;The halts ratio for the entire recording was 0.187 %. 100 % of the total halts were for reasons other than GC.&amp;lt;/p&amp;gt;&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BufferLost&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -434,12 +434,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BytecodeVerification&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The application was running with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The application was running with bytecode verification enabled.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The application ran with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The application ran with bytecode verification enabled.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ClassLeak&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -462,12 +462,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompareCpu&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.9473864515664224&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;An average CPU load of 7 % was caused by other processes during 1.212 s starting at 4/26/18 12:12:47 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;An average CPU load of 7 % was caused by other processes during 1.212 s starting at 4/26/18 12:12:47 PM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;An average CPU load of 7 % was caused by other processes for 1.212 s at 4/26/18 12:12:47 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;An average CPU load of 7 % was caused by other processes for 1.212 s at 4/26/18 12:12:47 PM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompressedOops&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -525,12 +525,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Exceptions&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The program generated 0 exceptions per second during 1.210 s starting at 4/26/18 12:12:47 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The program generated 0 exceptions per second during 1.210 s starting at 4/26/18 12:12:47 PM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The program generated 0 exceptions per second for 1.210 s at 4/26/18 12:12:47 PM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The program generated 0 exceptions per second for 1.210 s at 4/26/18 12:12:47 PM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Fatal Errors&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -596,11 +596,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;GcPauseRatio&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by GC pauses.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0 % during 1 min starting at 4/26/18 12:12:46 PM. The garbage collection pause ratio of the entire recording was 0 %.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0 % for 1 min at 4/26/18 12:12:46 PM. The garbage collection pause ratio of the entire recording was 0 %.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcStall&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -823,11 +823,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;ApplicationHalts&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by halts.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio between application halts and execution time was 0 % during 1 min starting at 4/24/18 10:16:27 AM. 0 % of the halts during the timespan were because of other reasons than GCs.&amp;lt;p&amp;gt;The total halts ratio during the entire recording was 0 %. 0 % of the total halts were because of other reasons than GCs.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio of application halts to execution time was 0 % for 1 min at 4/24/18 10:16:27 AM. 0 % of the halts were for reasons other than GC.&amp;lt;p&amp;gt;The halts ratio for the entire recording was 0 %. 0 % of the total halts were for reasons other than GC.&amp;lt;/p&amp;gt;&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BufferLost&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -836,12 +836,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BytecodeVerification&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The application was running with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The application was running with bytecode verification enabled.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The application ran with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The application ran with bytecode verification enabled.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ClassLeak&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -998,11 +998,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;GcPauseRatio&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by GC pauses.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0 % during 1 min starting at 4/24/18 10:16:27 AM. The garbage collection pause ratio of the entire recording was 0 %.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0 % for 1 min at 4/24/18 10:16:27 AM. The garbage collection pause ratio of the entire recording was 0 %.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcStall&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1068,11 +1068,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;LowOnPhysicalMemory&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;51.06889156143796&lt;/score&gt;
              &lt;shortDescription&gt;The maximum amount of used memory was 88.9 % of the physical memory available.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The maximum amount of used memory was 28.4 GiB. This is 88.9 % of the 31.9 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The maximum amount of memory used was 28.4 GiB. This is 88.9 % of the 31.9 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ManagementAgent&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1210,19 +1210,19 @@</span>
          &lt;file&gt;flight_recording_hidden.jfr&lt;/file&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.class&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.09287686767432358&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The most allocated class is likely &#39;byte[]&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;ByteArrayOutputStream.&amp;amp;lt;init&amp;amp;gt;(int) (50 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ByteArrayOutputStream.&amp;amp;lt;init&amp;amp;gt;()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpPacketWriter.&amp;amp;lt;init&amp;amp;gt;()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpJmxPacket.getPacketData()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpBroadcaster.sendPacket(JdpPacket)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpController$JDPControllerRunner.run()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The most allocated class is likely &#39;byte[]&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;ByteArrayOutputStream.&amp;amp;lt;init&amp;amp;gt;(int) (50 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ByteArrayOutputStream.&amp;amp;lt;init&amp;amp;gt;()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpPacketWriter.&amp;amp;lt;init&amp;amp;gt;()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpJmxPacket.getPacketData()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpBroadcaster.sendPacket(JdpPacket)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpController$JDPControllerRunner.run()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocated type is likely &#39;byte[]&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;ByteArrayOutputStream.&amp;amp;lt;init&amp;amp;gt;(int) (50 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ByteArrayOutputStream.&amp;amp;lt;init&amp;amp;gt;()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpPacketWriter.&amp;amp;lt;init&amp;amp;gt;()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpJmxPacket.getPacketData()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpBroadcaster.sendPacket(JdpPacket)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpController$JDPControllerRunner.run()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocated type is likely &#39;byte[]&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;ByteArrayOutputStream.&amp;amp;lt;init&amp;amp;gt;(int) (50 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ByteArrayOutputStream.&amp;amp;lt;init&amp;amp;gt;()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpPacketWriter.&amp;amp;lt;init&amp;amp;gt;()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpJmxPacket.getPacketData()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpBroadcaster.sendPacket(JdpPacket)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JdpController$JDPControllerRunner.run()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.thread&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.1274345677812541&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The thread performing the most allocation is likely &#39;AWT-EventQueue-0&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;EventQueue.dispatchEvent(AWTEvent) (60 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpOneEventForFilters(int)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEventsForFilter(int, Conditional, EventFilter)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEventsForHierarchy(int, Conditional, Component)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEvents(int, Conditional)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEvents(Conditional)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The thread performing the most allocation is likely &#39;AWT-EventQueue-0&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;EventQueue.dispatchEvent(AWTEvent) (60 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpOneEventForFilters(int)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEventsForFilter(int, Conditional, EventFilter)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEventsForHierarchy(int, Conditional, Component)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEvents(int, Conditional)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEvents(Conditional)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocations were likely done by thread &#39;AWT-EventQueue-0&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;EventQueue.dispatchEvent(AWTEvent) (60 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpOneEventForFilters(int)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEventsForFilter(int, Conditional, EventFilter)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEventsForHierarchy(int, Conditional, Component)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEvents(int, Conditional)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEvents(Conditional)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocations were likely done by thread &#39;AWT-EventQueue-0&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;EventQueue.dispatchEvent(AWTEvent) (60 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpOneEventForFilters(int)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEventsForFilter(int, Conditional, EventFilter)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEventsForHierarchy(int, Conditional, Component)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEvents(int, Conditional)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EventDispatchThread.pumpEvents(Conditional)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ApplicationHalts&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1238,12 +1238,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BytecodeVerification&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The application was running with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The application was running with bytecode verification enabled.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The application ran with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The application ran with bytecode verification enabled.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ClassLeak&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1266,12 +1266,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompareCpu&lt;/id&gt;
              &lt;severity&gt;Warning&lt;/severity&gt;
              &lt;score&gt;77.36067322482806&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;An average CPU load of 60 % was caused by other processes during 8.047 s starting at 4/25/18 11:38:36 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;An average CPU load of 60 % was caused by other processes during 8.047 s starting at 4/25/18 11:38:36 AM.&amp;lt;p&amp;gt;The application performance can be affected when the machine is under heavy load and there are other processes that use CPU or other resources on the same computer. To profile representatively or get higher throughput, shut down other resource intensive processes running on the machine.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;An average CPU load of 60 % was caused by other processes for 8.047 s at 4/25/18 11:38:36 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;An average CPU load of 60 % was caused by other processes for 8.047 s at 4/25/18 11:38:36 AM.&amp;lt;p&amp;gt;The application performance can be affected when the machine is under heavy load and there are other processes that use CPU or other resources on the same computer. To profile representatively or get higher throughput, shut down other resource intensive processes running on the machine.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompressedOops&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1329,12 +1329,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Exceptions&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The program generated 0 exceptions per second during 1.000 s starting at 4/25/18 11:38:36 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The program generated 0 exceptions per second during 1.000 s starting at 4/25/18 11:38:36 AM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The program generated 0 exceptions per second for 1.000 s at 4/25/18 11:38:36 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The program generated 0 exceptions per second for 1.000 s at 4/25/18 11:38:36 AM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Fatal Errors&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1470,11 +1470,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;LowOnPhysicalMemory&lt;/id&gt;
              &lt;severity&gt;Warning&lt;/severity&gt;
              &lt;score&gt;85.24971081406179&lt;/score&gt;
              &lt;shortDescription&gt;The maximum amount of used memory was 99.8 % of the physical memory available.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The maximum amount of used memory was 16 GiB. This is 99.8 % of the 16 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The maximum amount of memory used was 16 GiB. This is 99.8 % of the 16 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ManagementAgent&lt;/id&gt;
              &lt;severity&gt;Warning&lt;/severity&gt;
              &lt;score&gt;100.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2416,26 +2416,26 @@</span>
          &lt;file&gt;parallel-gc_cpu.jfr&lt;/file&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.class&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.02670579099297634&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The most allocated class is likely &#39;java.util.ArrayList&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;JFRImpl.getRecordings() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MetaProducer.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The most allocated class is likely &#39;java.util.ArrayList&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;JFRImpl.getRecordings() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MetaProducer.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocated type is likely &#39;java.util.ArrayList&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;JFRImpl.getRecordings() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MetaProducer.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocated type is likely &#39;java.util.ArrayList&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;JFRImpl.getRecordings() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MetaProducer.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.thread&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.07261458520440227&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The thread performing the most allocation is likely &#39;RMI TCP Connection(1)-127.0.0.1&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;ObjectInputStream$BlockDataInputStream.&amp;amp;lt;init&amp;amp;gt;(ObjectInputStream, InputStream) (50 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ObjectInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MarshalInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ConnectionInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;StreamRemoteCall.getInputStream()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Transport.serviceCall(RemoteCall)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The thread performing the most allocation is likely &#39;RMI TCP Connection(1)-127.0.0.1&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;ObjectInputStream$BlockDataInputStream.&amp;amp;lt;init&amp;amp;gt;(ObjectInputStream, InputStream) (50 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ObjectInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MarshalInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ConnectionInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;StreamRemoteCall.getInputStream()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Transport.serviceCall(RemoteCall)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocations were likely done by thread &#39;RMI TCP Connection(1)-127.0.0.1&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;ObjectInputStream$BlockDataInputStream.&amp;amp;lt;init&amp;amp;gt;(ObjectInputStream, InputStream) (50 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ObjectInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MarshalInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ConnectionInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;StreamRemoteCall.getInputStream()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Transport.serviceCall(RemoteCall)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocations were likely done by thread &#39;RMI TCP Connection(1)-127.0.0.1&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;ObjectInputStream$BlockDataInputStream.&amp;amp;lt;init&amp;amp;gt;(ObjectInputStream, InputStream) (50 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ObjectInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MarshalInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ConnectionInputStream.&amp;amp;lt;init&amp;amp;gt;(InputStream)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;StreamRemoteCall.getInputStream()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Transport.serviceCall(RemoteCall)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ApplicationHalts&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by halts.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio between application halts and execution time was 0 % during 1 min starting at 4/4/14 11:17:05 AM. 0 % of the halts during the timespan were because of other reasons than GCs.&amp;lt;p&amp;gt;The total halts ratio during the entire recording was 0 %. 0 % of the total halts were because of other reasons than GCs.&amp;lt;p&amp;gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio of application halts to execution time was 0 % for 1 min at 4/4/14 11:17:05 AM. 0 % of the halts were for reasons other than GC.&amp;lt;p&amp;gt;The halts ratio for the entire recording was 0 %. 0 % of the total halts were for reasons other than GC.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BufferLost&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2444,12 +2444,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BytecodeVerification&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The application was running with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The application was running with bytecode verification enabled.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The application ran with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The application ran with bytecode verification enabled.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ClassLeak&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2472,12 +2472,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompareCpu&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;9.313225746154785E-7&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;An average CPU load of 0 % was caused by other processes during 1.011 s starting at 4/4/14 11:17:07 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;An average CPU load of 0 % was caused by other processes during 1.011 s starting at 4/4/14 11:17:07 AM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;An average CPU load of 0 % was caused by other processes for 1.011 s at 4/4/14 11:17:07 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;An average CPU load of 0 % was caused by other processes for 1.011 s at 4/4/14 11:17:07 AM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompressedOops&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2535,12 +2535,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Exceptions&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The program generated 0 exceptions per second during 999.487 ms starting at 4/4/14 11:17:06 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The program generated 0 exceptions per second during 999.487 ms starting at 4/4/14 11:17:06 AM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The program generated 0 exceptions per second for 999.487 ms at 4/4/14 11:17:06 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The program generated 0 exceptions per second for 999.487 ms at 4/4/14 11:17:06 AM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Fatal Errors&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2599,18 +2599,18 @@</span>
          &lt;rule&gt;
              &lt;id&gt;GcOptions&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;50.0&lt;/score&gt;
              &lt;shortDescription&gt;The runtime used 2 GC threads on a machine with 1 CPU cores.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The runtime used 2 GC threads on a machine with 1 CPU cores. It is not optimal to use more GC threads than available cores. Removing the &#39;-XX:ParallelGCThreads&#39; flag will allow the JVM to set the number of GC threads automatically.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The runtime used 2 GC threads on a machine with 1 CPU cores. It&#39;s suboptimal to use more GC threads than available cores. Removing the &#39;-XX:ParallelGCThreads&#39; flag will allow the JVM to set the number of GC threads automatically.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcPauseRatio&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by GC pauses.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0 % during 1 min starting at 4/4/14 11:17:05 AM. The garbage collection pause ratio of the entire recording was 0 %.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0 % for 1 min at 4/4/14 11:17:05 AM. The garbage collection pause ratio of the entire recording was 0 %.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcStall&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2818,26 +2818,26 @@</span>
          &lt;file&gt;parallel-on-singlecpu.jfr&lt;/file&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.class&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.031334794765092246&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The most allocated class is likely &#39;java.lang.Object[]&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;AbstractCollection.toArray() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ArrayList.&amp;amp;lt;init&amp;amp;gt;(Collection)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.getRecordings()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MetaProducer.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The most allocated class is likely &#39;java.lang.Object[]&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;AbstractCollection.toArray() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ArrayList.&amp;amp;lt;init&amp;amp;gt;(Collection)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.getRecordings()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MetaProducer.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocated type is likely &#39;java.lang.Object[]&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;AbstractCollection.toArray() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ArrayList.&amp;amp;lt;init&amp;amp;gt;(Collection)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.getRecordings()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MetaProducer.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocated type is likely &#39;java.lang.Object[]&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;AbstractCollection.toArray() (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ArrayList.&amp;amp;lt;init&amp;amp;gt;(Collection)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.getRecordings()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;MetaProducer.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JFRImpl.onNewChunk()&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.thread&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.031334794765092246&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The thread performing the most allocation is likely &#39;RMI TCP Connection(1)-127.0.0.1&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Throwable.fillInStackTrace(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Throwable.fillInStackTrace()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Throwable.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Exception.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ReflectiveOperationException.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;NoSuchFieldException.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The thread performing the most allocation is likely &#39;RMI TCP Connection(1)-127.0.0.1&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Throwable.fillInStackTrace(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Throwable.fillInStackTrace()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Throwable.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Exception.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ReflectiveOperationException.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;NoSuchFieldException.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocations were likely done by thread &#39;RMI TCP Connection(1)-127.0.0.1&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Throwable.fillInStackTrace(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Throwable.fillInStackTrace()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Throwable.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Exception.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ReflectiveOperationException.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;NoSuchFieldException.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocations were likely done by thread &#39;RMI TCP Connection(1)-127.0.0.1&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Throwable.fillInStackTrace(int) (100 %)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Throwable.fillInStackTrace()&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Throwable.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Exception.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;ReflectiveOperationException.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;NoSuchFieldException.&amp;amp;lt;init&amp;amp;gt;(String)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ApplicationHalts&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by halts.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio between application halts and execution time was 0 % during 1 min starting at 4/4/14 8:54:33 AM. 0 % of the halts during the timespan were because of other reasons than GCs.&amp;lt;p&amp;gt;The total halts ratio during the entire recording was 0 %. 0 % of the total halts were because of other reasons than GCs.&amp;lt;p&amp;gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio of application halts to execution time was 0 % for 1 min at 4/4/14 8:54:33 AM. 0 % of the halts were for reasons other than GC.&amp;lt;p&amp;gt;The halts ratio for the entire recording was 0 %. 0 % of the total halts were for reasons other than GC.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BufferLost&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2846,12 +2846,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BytecodeVerification&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The application was running with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The application was running with bytecode verification enabled.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The application ran with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The application ran with bytecode verification enabled.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ClassLeak&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2874,12 +2874,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompareCpu&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;An average CPU load of 0 % was caused by other processes during 1.010 s starting at 4/4/14 8:54:36 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;An average CPU load of 0 % was caused by other processes during 1.010 s starting at 4/4/14 8:54:36 AM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;An average CPU load of 0 % was caused by other processes for 1.010 s at 4/4/14 8:54:36 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;An average CPU load of 0 % was caused by other processes for 1.010 s at 4/4/14 8:54:36 AM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompressedOops&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2937,12 +2937,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Exceptions&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The program generated 0 exceptions per second during 1.009 s starting at 4/4/14 8:54:34 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The program generated 0 exceptions per second during 1.009 s starting at 4/4/14 8:54:34 AM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The program generated 0 exceptions per second for 1.009 s at 4/4/14 8:54:34 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The program generated 0 exceptions per second for 1.009 s at 4/4/14 8:54:34 AM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Fatal Errors&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3008,11 +3008,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;GcPauseRatio&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by GC pauses.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0 % during 1 min starting at 4/4/14 8:54:33 AM. The garbage collection pause ratio of the entire recording was 0 %.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0 % for 1 min at 4/4/14 8:54:33 AM. The garbage collection pause ratio of the entire recording was 0 %.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcStall&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3235,11 +3235,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;ApplicationHalts&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.4097173333333334&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by halts.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio between application halts and execution time was 0.082 % during 1 min starting at 4/24/18 10:08:52 AM. 51.552 % of the halts during the timespan were because of other reasons than GCs.&amp;lt;p&amp;gt;The total halts ratio during the entire recording was 1.003 %. 51.552 % of the total halts were because of other reasons than GCs.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio of application halts to execution time was 0.082 % for 1 min at 4/24/18 10:08:52 AM. 51.552 % of the halts were for reasons other than GC.&amp;lt;p&amp;gt;The halts ratio for the entire recording was 1.003 %. 51.552 % of the total halts were for reasons other than GC.&amp;lt;/p&amp;gt;&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BufferLost&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3248,12 +3248,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BytecodeVerification&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The application was running with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The application was running with bytecode verification enabled.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The application ran with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The application ran with bytecode verification enabled.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ClassLeak&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3276,12 +3276,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompareCpu&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;3.3944677554909966&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;An average CPU load of 15 % was caused by other processes during 1.060 s starting at 4/24/18 10:08:54 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;An average CPU load of 15 % was caused by other processes during 1.060 s starting at 4/24/18 10:08:54 AM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;An average CPU load of 15 % was caused by other processes for 1.060 s at 4/24/18 10:08:54 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;An average CPU load of 15 % was caused by other processes for 1.060 s at 4/24/18 10:08:54 AM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompressedOops&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3339,12 +3339,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Exceptions&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The program generated 0 exceptions per second during 1.054 s starting at 4/24/18 10:08:53 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The program generated 0 exceptions per second during 1.054 s starting at 4/24/18 10:08:53 AM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The program generated 0 exceptions per second for 1.054 s at 4/24/18 10:08:53 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The program generated 0 exceptions per second for 1.054 s at 4/24/18 10:08:53 AM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Fatal Errors&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3410,11 +3410,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;GcPauseRatio&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.1985013333333333&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by GC pauses.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0.04 % during 1 min starting at 4/24/18 10:08:52 AM. The garbage collection pause ratio of the entire recording was 0.486 %.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 0.04 % for 1 min at 4/24/18 10:08:52 AM. The garbage collection pause ratio of the entire recording was 0.486 %.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcStall&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3437,12 +3437,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;HighGc&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;2.054924803137792&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The JVM was paused for 100 % of the time during 9.017 ms starting at 4/24/18 10:08:53 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The JVM was paused for 100 % of the time during 9.017 ms starting at 4/24/18 10:08:53 AM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.&amp;lt;p&amp;gt;To improve rule accuracy and/or get more details for further investigation, it is recommended to enable the following event types: &#39;Allocation in new TLAB&#39;, &#39;Allocation outside TLAB&#39;.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The JVM was paused for 100 % of the 9.017 ms at 4/24/18 10:08:53 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The JVM was paused for 100 % of the 9.017 ms at 4/24/18 10:08:53 AM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.&amp;lt;p&amp;gt;To improve rule accuracy and/or get more details for further investigation, it is recommended to enable the following event types: &#39;Allocation in new TLAB&#39;, &#39;Allocation outside TLAB&#39;.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;HighJvmCpu&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;1.4885947328122275&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3480,11 +3480,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;LowOnPhysicalMemory&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;64.30361523113581&lt;/score&gt;
              &lt;shortDescription&gt;The maximum amount of used memory was 91.8 % of the physical memory available.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The maximum amount of used memory was 29.3 GiB. This is 91.8 % of the 31.9 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The maximum amount of memory used was 29.3 GiB. This is 91.8 % of the 31.9 GiB of physical memory available. Having little free memory may lead to swapping, which is very expensive. To avoid this, either decrease the memory usage or increase the amount of available memory.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ManagementAgent&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3622,26 +3622,26 @@</span>
          &lt;file&gt;wldf.jfr&lt;/file&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.class&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;46.750432285649964&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The most allocated class is likely &#39;char[]&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Arrays.copyOfRange(char[], int, int) (44.9 %)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The most allocated class is likely &#39;char[]&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Arrays.copyOfRange(char[], int, int) (44.9 %)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated classes are likely good places to start when trying to reduce garbage collections. Look at the aggregated stack traces of the most commonly allocated classes to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocated type is likely &#39;char[]&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Arrays.copyOfRange(char[], int, int) (44.9 %)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocated type is likely &#39;char[]&#39;, most commonly allocated by: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Arrays.copyOfRange(char[], int, int) (44.9 %)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Frequently allocated types are good places to start when trying to reduce garbage collections. Look at where the most common types are being allocated to see if many instances are created along the same call path. Try to reduce the number of instances created by invoking the most commonly taken paths less.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Allocations.thread&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;24.595150095883856&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The thread performing the most allocation is likely &#39;[ACTIVE] ExecuteThread: &amp;amp;#39;5&amp;amp;#39; for queue: &amp;amp;#39;weblogic.kernel.Default (self-tuning)&amp;amp;#39;&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Arrays.copyOfRange(char[], int, int) (53.3 %)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The thread performing the most allocation is likely &#39;[ACTIVE] ExecuteThread: &amp;amp;#39;5&amp;amp;#39; for queue: &amp;amp;#39;weblogic.kernel.Default (self-tuning)&amp;amp;#39;&#39;. This is the most common allocation path for that class: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Arrays.copyOfRange(char[], int, int) (53.3 %)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the aggregated stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The most allocations were likely done by thread &#39;[ACTIVE] ExecuteThread: &amp;amp;#39;5&amp;amp;#39; for queue: &amp;amp;#39;weblogic.kernel.Default (self-tuning)&amp;amp;#39;&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Arrays.copyOfRange(char[], int, int) (53.3 %)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The most allocations were likely done by thread &#39;[ACTIVE] ExecuteThread: &amp;amp;#39;5&amp;amp;#39; for queue: &amp;amp;#39;weblogic.kernel.Default (self-tuning)&amp;amp;#39;&#39; at: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Arrays.copyOfRange(char[], int, int) (53.3 %)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;p&amp;gt;Many allocations performed by the same thread might indicate a problem in a multi-threaded program. Look at the stack traces for the thread with the highest allocation rate. See if the allocation rate can be brought down, or balanced among the active threads.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ApplicationHalts&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;11.086075208333334&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by halts.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio between application halts and execution time was 2.217 % during 1 min starting at 9/24/15 10:08:56 AM. 0.027 % of the halts during the timespan were because of other reasons than GCs.&amp;lt;p&amp;gt;The total halts ratio during the entire recording was 1.882 %. 0.021 % of the total halts were because of other reasons than GCs.&amp;lt;p&amp;gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by halts.&amp;lt;p&amp;gt;The highest ratio of application halts to execution time was 2.217 % for 1 min at 9/24/15 10:08:56 AM. 0.027 % of the halts were for reasons other than GC.&amp;lt;p&amp;gt;The halts ratio for the entire recording was 1.882 %. 0.021 % of the total halts were for reasons other than GC.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Enabling the following event types would improve the accuracy of this rule: jdk.SafepointBegin&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BufferLost&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3650,12 +3650,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BytecodeVerification&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;1.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The application was running WebLogic Server with bytecode verification disabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The application was running WebLogic Server with bytecode verification disabled. While not generally recommended, it is considered OK for WLS.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The application ran WebLogic Server with bytecode verification disabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The application ran WebLogic Server with bytecode verification disabled. While not generally recommended, it is considered OK for WLS.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ClassLeak&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;74.99995410111316&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3678,12 +3678,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompareCpu&lt;/id&gt;
              &lt;severity&gt;Warning&lt;/severity&gt;
              &lt;score&gt;87.07809686085879&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;An average CPU load of 54 % was caused by other processes during 4.938 s starting at 9/24/15 10:08:17 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;An average CPU load of 54 % was caused by other processes during 4.938 s starting at 9/24/15 10:08:17 AM.&amp;lt;p&amp;gt;The application performance can be affected when the machine is under heavy load and there are other processes that use CPU or other resources on the same computer. To profile representatively or get higher throughput, shut down other resource intensive processes running on the machine.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;An average CPU load of 54 % was caused by other processes for 4.938 s at 9/24/15 10:08:17 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;An average CPU load of 54 % was caused by other processes for 4.938 s at 9/24/15 10:08:17 AM.&amp;lt;p&amp;gt;The application performance can be affected when the machine is under heavy load and there are other processes that use CPU or other resources on the same computer. To profile representatively or get higher throughput, shut down other resource intensive processes running on the machine.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;CompressedOops&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3735,18 +3735,18 @@</span>
          &lt;rule&gt;
              &lt;id&gt;Errors&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;14.166666666666668&lt;/score&gt;
              &lt;shortDescription&gt;The program generated an average of 17 errors per minute during 9/24/2015 10:08:14 AM – 10:09:14 AM.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The program generated an average of 17 errors per minute during 9/24/2015 10:08:14 AM – 10:09:14 AM, 17 errors were thrown in total.&amp;lt;p&amp;gt;The most common error was &#39;java.lang.NoSuchMethodError&#39;, which was thrown 13 times.&amp;lt;p&amp;gt;Investigate the thrown errors to see if they can be avoided. Errors indicate that something went wrong with the code execution and should never be used for flow control. The following regular expression was used to exclude 381 errors from this rule: &#39;(com.sun.el.parser.ELParser\$LookaheadSuccess)&#39;.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The program generated an average of 17 errors per minute during 9/24/2015 10:08:14 AM – 10:09:14 AM. 17 errors were thrown in total.&amp;lt;p&amp;gt;The most common error was &#39;java.lang.NoSuchMethodError&#39;, which was thrown 13 times.&amp;lt;p&amp;gt;Investigate the thrown errors to see if they can be avoided. Errors indicate that something went wrong with the code execution and should never be used for flow control. The following regular expression was used to exclude 381 errors from this rule: &#39;(com.sun.el.parser.ELParser\$LookaheadSuccess)&#39;.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Exceptions&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;2.573808918421875&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The program generated 515 exceptions per second during 28.060 s starting at 9/24/15 10:08:58 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The program generated 515 exceptions per second during 28.060 s starting at 9/24/15 10:08:58 AM.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The program generated 515 exceptions per second for 28.060 s at 9/24/15 10:08:58 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The program generated 515 exceptions per second for 28.060 s at 9/24/15 10:08:58 AM.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;Fatal Errors&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3791,11 +3791,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;GcFreedRatio&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;1.4318707405222828&lt;/score&gt;
              &lt;shortDescription&gt;The ratio between memory freed by garbage collections per second and liveset is 0.5. This is likely a reasonable amount.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;61.4 MiB per second was freed by garbage collections during 10 s starting at 9/24/15 10:09:18 AM. This is 0.474 times the average liveset which was 130 MiB. This is likely a reasonable amount.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;61.4 MiB per second was freed by garbage collections for 10 s at 9/24/15 10:09:18 AM. This is 0.474 times the average liveset which was 130 MiB. This is likely a reasonable amount.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcLocker&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3812,11 +3812,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;GcPauseRatio&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;11.083072308333335&lt;/score&gt;
              &lt;shortDescription&gt;Application efficiency was not highly affected by GC pauses.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 2.217 % during 1 min starting at 9/24/15 10:08:56 AM. The garbage collection pause ratio of the entire recording was 1.882 %.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Application efficiency was not highly affected by GC pauses.&amp;lt;p&amp;gt;The highest ratio between garbage collection pauses and execution time was 2.217 % for 1 min at 9/24/15 10:08:56 AM. The garbage collection pause ratio of the entire recording was 1.882 %.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;GcStall&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3839,12 +3839,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;HighGc&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;72.91379413109293&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The JVM was paused for 100 % of the time during 567.258 ms starting at 9/24/15 10:07:58 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The JVM was paused for 100 % of the time during 567.258 ms starting at 9/24/15 10:07:58 AM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The JVM was paused for 100 % of the 567.258 ms at 9/24/15 10:07:58 AM.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The JVM was paused for 100 % of the 567.258 ms at 9/24/15 10:07:58 AM. The time spent performing garbage collection may be reduced by increasing the heap size or by trying to reduce allocation.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;HighJvmCpu&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;13.536580012735357&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3867,12 +3867,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;JavaBlocking&lt;/id&gt;
              &lt;severity&gt;Information&lt;/severity&gt;
              &lt;score&gt;62.05152332723372&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;Threads in the application were blocked on locks for a total time of 1 min 26 s.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Threads in the application were blocked on locks for a total time of 1 min 26 s. The most common monitor class was &#39;Logger&#39;, which was blocked on 1,612 times for a total time of 1 min 23 s.&amp;lt;p&amp;gt;The following regular expression was used to exclude threads from this rule: &#39;(.*weblogic\.socket\.Muxer.*)&#39;&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;Threads in the application were blocked on locks for a total of 1 min 26 s.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Threads in the application were blocked on locks for a total of 1 min 26 s. The most blocking monitor class was &#39;Logger&#39;, which was blocked 1,612 times for a total of 1 min 23 s.&amp;lt;p&amp;gt;The following regular expression was used to exclude threads from this rule: &#39;(.*weblogic\.socket\.Muxer.*)&#39;&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;LongGcPause&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;15.646056210732606&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3973,11 +3973,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;StackdepthSetting&lt;/id&gt;
              &lt;severity&gt;Warning&lt;/severity&gt;
              &lt;score&gt;99.99654514135385&lt;/score&gt;
              &lt;shortDescription&gt;Some stack traces were truncated in this recording.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Some stack traces were truncated in this recording.&amp;lt;p&amp;gt;The Flight Recorder only records traces with a depth up to the maximum stack depth value set to 64. This is the default depth. 35.7 % of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the &#39;-XX:FlightRecorderOptions=stackdepth=&amp;amp;lt;value&amp;amp;gt;&#39; value.&amp;lt;p&amp;gt;Events of the following types have truncated stack traces:&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Allocation in new TLAB (42.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Class Load (42.8 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocation outside TLAB (60.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Pre Invoke (47.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Invoke (47.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Post Invoke Cleanup (47.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Post Invoke (47.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Post Invoke (44.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Pre Invoke (44.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Servlet Execute (26.2 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Servlet Request Dispatch (45 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Method Profiling Sample (24.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction Start (80 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Start (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Creation (54.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Prepare (54.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Execute (54.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Execute Begin (54.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Close (73 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction End (51.5 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction Commit (51 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Release (65 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Commit (65 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction End (65.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Socket Read (41.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;File Write (32.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Error (29.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXRPC Client Request (45.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXRPC Client Response (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Monitor Blocked (1.09 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB PoolManager Create (32.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocation Requiring GC (31.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Is Same RM (6.59 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Servlet Response Write Headers (0.753 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Socket Write (20.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Monitor Wait (0.134 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;File Read (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Thread Sleep (33.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Thread Park (0.223 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Some stack traces were truncated in this recording.&amp;lt;p&amp;gt;The Flight Recorder is configured with a maximum captured stack depth of 64. This is the default depth. 35.7 % of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the &#39;-XX:FlightRecorderOptions=stackdepth=&amp;amp;lt;value&amp;amp;gt;&#39; value.&amp;lt;p&amp;gt;Events of the following types have truncated stack traces:&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Allocation in new TLAB (42.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Class Load (42.8 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocation outside TLAB (60.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Pre Invoke (47.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Invoke (47.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Post Invoke Cleanup (47.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Post Invoke (47.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Post Invoke (44.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Pre Invoke (44.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Servlet Execute (26.2 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Servlet Request Dispatch (45 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Method Profiling Sample (24.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction Start (80 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Start (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Creation (54.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Prepare (54.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Execute (54.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Execute Begin (54.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Close (73 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction End (51.5 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction Commit (51 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Release (65 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Commit (65 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction End (65.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Socket Read (41.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;File Write (32.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Error (29.1 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXRPC Client Request (45.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXRPC Client Response (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Monitor Blocked (1.09 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB PoolManager Create (32.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Allocation Requiring GC (31.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Is Same RM (6.59 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Servlet Response Write Headers (0.753 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Socket Write (20.6 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Monitor Wait (0.134 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;File Read (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Thread Sleep (33.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Java Thread Park (0.223 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;StringDeduplication&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;11.944505390960535&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4054,12 +4054,12 @@</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;BytecodeVerification&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
<span class="udiff-line-modified-removed">-             &lt;shortDescription&gt;The application was running with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;The application was running with bytecode verification enabled.&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;shortDescription&gt;The application ran with bytecode verification enabled.&lt;/shortDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;The application ran with bytecode verification enabled.&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;ClassLeak&lt;/id&gt;
              &lt;severity&gt;OK&lt;/severity&gt;
              &lt;score&gt;0.0&lt;/score&gt;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4377,11 +4377,11 @@</span>
          &lt;rule&gt;
              &lt;id&gt;StackdepthSetting&lt;/id&gt;
              &lt;severity&gt;Warning&lt;/severity&gt;
              &lt;score&gt;99.35136572520041&lt;/score&gt;
              &lt;shortDescription&gt;Some stack traces were truncated in this recording.&lt;/shortDescription&gt;
<span class="udiff-line-modified-removed">-             &lt;longDescription&gt;Some stack traces were truncated in this recording.&amp;lt;p&amp;gt;The Flight Recorder only records traces with a depth up to the maximum stack depth value set to 64. This is the default depth. 17.5 % of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the &#39;-XX:FlightRecorderOptions=stackdepth=&amp;amp;lt;value&amp;amp;gt;&#39; value.&amp;lt;p&amp;gt;Events of the following types have truncated stack traces:&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Pre Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Post Invoke Cleanup (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Post Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Post Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Pre Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Close (96.8 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Creation (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Prepare (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Execute (96.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Execute Begin (96.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction Start (16.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Start (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Create (75 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Release (85.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Debug (1.59 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Commit (45 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction End (45 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction End (6.06 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction Commit (36.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Is Same RM (1.79 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXWS Resource (50 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXWS Endpoint (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Servlet Response Write Headers (1.87 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXRPC Client Request (50 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXRPC Client Response (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/longDescription&gt;</span>
<span class="udiff-line-modified-added">+             &lt;longDescription&gt;Some stack traces were truncated in this recording.&amp;lt;p&amp;gt;The Flight Recorder is configured with a maximum captured stack depth of 64. This is the default depth. 17.5 % of all traces were larger than this option, and were therefore truncated. If more detailed traces are required, increase the &#39;-XX:FlightRecorderOptions=stackdepth=&amp;amp;lt;value&amp;amp;gt;&#39; value.&amp;lt;p&amp;gt;Events of the following types have truncated stack traces:&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Pre Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Post Invoke Cleanup (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Post Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Post Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Business Method Pre Invoke (71.3 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Close (96.8 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Creation (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Prepare (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Execute (96.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Statement Execute Begin (96.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction Start (16.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Start (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;EJB Pool Manager Create (75 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Connection Release (85.7 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Debug (1.59 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Commit (45 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction End (45 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction End (6.06 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JTA Transaction Commit (36.4 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;JDBC Transaction Is Same RM (1.79 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXWS Resource (50 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXWS Endpoint (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Servlet Response Write Headers (1.87 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXRPC Client Request (50 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Webservices JAXRPC Client Response (100 % truncated traces)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;/longDescription&gt;</span>
          &lt;/rule&gt;
          &lt;rule&gt;
              &lt;id&gt;StringDeduplication&lt;/id&gt;
              &lt;severity&gt;Not Applicable&lt;/severity&gt;
              &lt;score&gt;-1.0&lt;/score&gt;
</pre>
<center>&lt; prev <a href="../../../../../../../index.html" target="_top">index</a> next &gt;</center>  </body>
</html>