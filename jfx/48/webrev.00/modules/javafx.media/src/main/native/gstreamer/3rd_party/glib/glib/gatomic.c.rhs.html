<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.media/src/main/native/gstreamer/3rd_party/glib/glib/gatomic.c</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
<a name="1" id="anc1"></a><span class="line-modified">  2  * Copyright (C) 2011 Ryan Lortie</span>
  3  *
  4  * This library is free software; you can redistribute it and/or
  5  * modify it under the terms of the GNU Lesser General Public
  6  * License as published by the Free Software Foundation; either
  7  * version 2.1 of the License, or (at your option) any later version.
  8  *
  9  * This library is distributed in the hope that it will be useful, but
 10  * WITHOUT ANY WARRANTY; without even the implied warranty of
 11  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 12  * Lesser General Public License for more details.
 13  *
 14  * You should have received a copy of the GNU Lesser General Public
 15  * License along with this library; if not, see &lt;http://www.gnu.org/licenses/&gt;.
 16  *
 17  * Author: Ryan Lortie &lt;desrt@desrt.ca&gt;
 18  */
 19 
 20 #include &quot;config.h&quot;
 21 
 22 #include &quot;gatomic.h&quot;
 23 
 24 /**
 25  * SECTION:atomic_operations
 26  * @title: Atomic Operations
 27  * @short_description: basic atomic integer and pointer operations
 28  * @see_also: #GMutex
 29  *
 30  * The following is a collection of compiler macros to provide atomic
 31  * access to integer and pointer-sized values.
 32  *
 33  * The macros that have &#39;int&#39; in the name will operate on pointers to
 34  * #gint and #guint.  The macros with &#39;pointer&#39; in the name will operate
 35  * on pointers to any pointer-sized value, including #gsize.  There is
 36  * no support for 64bit operations on platforms with 32bit pointers
 37  * because it is not generally possible to perform these operations
 38  * atomically.
 39  *
 40  * The get, set and exchange operations for integers and pointers
 41  * nominally operate on #gint and #gpointer, respectively.  Of the
 42  * arithmetic operations, the &#39;add&#39; operation operates on (and returns)
 43  * signed integer values (#gint and #gssize) and the &#39;and&#39;, &#39;or&#39;, and
 44  * &#39;xor&#39; operations operate on (and return) unsigned integer values
 45  * (#guint and #gsize).
 46  *
 47  * All of the operations act as a full compiler and (where appropriate)
 48  * hardware memory barrier.  Acquire and release or producer and
 49  * consumer barrier semantics are not available through this API.
 50  *
 51  * It is very important that all accesses to a particular integer or
 52  * pointer be performed using only this API and that different sizes of
 53  * operation are not mixed or used on overlapping memory regions.  Never
 54  * read or assign directly from or to a value -- always use this API.
 55  *
 56  * For simple reference counting purposes you should use
 57  * g_atomic_int_inc() and g_atomic_int_dec_and_test().  Other uses that
 58  * fall outside of simple reference counting patterns are prone to
 59  * subtle bugs and occasionally undefined behaviour.  It is also worth
 60  * noting that since all of these operations require global
 61  * synchronisation of the entire machine, they can be quite slow.  In
 62  * the case of performing multiple atomic operations it can often be
 63  * faster to simply acquire a mutex lock around the critical area,
 64  * perform the operations normally and then release the lock.
 65  **/
 66 
 67 /**
 68  * G_ATOMIC_LOCK_FREE:
 69  *
 70  * This macro is defined if the atomic operations of GLib are
 71  * implemented using real hardware atomic operations.  This means that
 72  * the GLib atomic API can be used between processes and safely mixed
 73  * with other (hardware) atomic APIs.
 74  *
 75  * If this macro is not defined, the atomic operations may be
 76  * emulated using a mutex.  In that case, the GLib atomic operations are
 77  * only atomic relative to themselves and within a single process.
 78  **/
 79 
 80 /* NOTE CAREFULLY:
 81  *
 82  * This file is the lowest-level part of GLib.
 83  *
 84  * Other lowlevel parts of GLib (threads, slice allocator, g_malloc,
 85  * messages, etc) call into these functions and macros to get work done.
 86  *
 87  * As such, these functions can not call back into any part of GLib
 88  * without risking recursion.
 89  */
 90 
 91 #ifdef G_ATOMIC_LOCK_FREE
 92 
<a name="2" id="anc2"></a><span class="line-modified"> 93 /* if G_ATOMIC_LOCK_FREE was defined by `meson configure` then we MUST</span>
 94  * implement the atomic operations in a lock-free manner.
 95  */
 96 
 97 #if defined (__GCC_HAVE_SYNC_COMPARE_AND_SWAP_4)
 98 
 99 #if defined(__ATOMIC_SEQ_CST) &amp;&amp; !defined(__clang__)
100 /* The implementation used in this code path in gatomic.h assumes
101  * 4-byte int */
102 G_STATIC_ASSERT (sizeof (gint) == 4);
103 
104 /* The implementations in gatomic.h assume 4- or 8-byte pointers */
105 G_STATIC_ASSERT (sizeof (void *) == 4 || sizeof (void *) == 8);
106 #endif
107 
108 /**
109  * g_atomic_int_get:
110  * @atomic: a pointer to a #gint or #guint
111  *
112  * Gets the current value of @atomic.
113  *
114  * This call acts as a full compiler and hardware
115  * memory barrier (before the get).
116  *
117  * Returns: the value of the integer
118  *
119  * Since: 2.4
120  **/
121 gint
122 (g_atomic_int_get) (const volatile gint *atomic)
123 {
124   return g_atomic_int_get (atomic);
125 }
126 
127 /**
128  * g_atomic_int_set:
129  * @atomic: a pointer to a #gint or #guint
130  * @newval: a new value to store
131  *
132  * Sets the value of @atomic to @newval.
133  *
134  * This call acts as a full compiler and hardware
135  * memory barrier (after the set).
136  *
137  * Since: 2.4
138  */
139 void
140 (g_atomic_int_set) (volatile gint *atomic,
141                     gint           newval)
142 {
143   g_atomic_int_set (atomic, newval);
144 }
145 
146 /**
147  * g_atomic_int_inc:
148  * @atomic: a pointer to a #gint or #guint
149  *
150  * Increments the value of @atomic by 1.
151  *
152  * Think of this operation as an atomic version of `{ *atomic += 1; }`.
153  *
154  * This call acts as a full compiler and hardware memory barrier.
155  *
156  * Since: 2.4
157  **/
158 void
159 (g_atomic_int_inc) (volatile gint *atomic)
160 {
161   g_atomic_int_inc (atomic);
162 }
163 
164 /**
165  * g_atomic_int_dec_and_test:
166  * @atomic: a pointer to a #gint or #guint
167  *
168  * Decrements the value of @atomic by 1.
169  *
170  * Think of this operation as an atomic version of
171  * `{ *atomic -= 1; return (*atomic == 0); }`.
172  *
173  * This call acts as a full compiler and hardware memory barrier.
174  *
175  * Returns: %TRUE if the resultant value is zero
176  *
177  * Since: 2.4
178  **/
179 gboolean
180 (g_atomic_int_dec_and_test) (volatile gint *atomic)
181 {
182   return g_atomic_int_dec_and_test (atomic);
183 }
184 
185 /**
186  * g_atomic_int_compare_and_exchange:
187  * @atomic: a pointer to a #gint or #guint
188  * @oldval: the value to compare with
189  * @newval: the value to conditionally replace with
190  *
191  * Compares @atomic to @oldval and, if equal, sets it to @newval.
192  * If @atomic was not equal to @oldval then no change occurs.
193  *
194  * This compare and exchange is done atomically.
195  *
196  * Think of this operation as an atomic version of
197  * `{ if (*atomic == oldval) { *atomic = newval; return TRUE; } else return FALSE; }`.
198  *
199  * This call acts as a full compiler and hardware memory barrier.
200  *
201  * Returns: %TRUE if the exchange took place
202  *
203  * Since: 2.4
204  **/
205 gboolean
206 (g_atomic_int_compare_and_exchange) (volatile gint *atomic,
207                                      gint           oldval,
208                                      gint           newval)
209 {
210   return g_atomic_int_compare_and_exchange (atomic, oldval, newval);
211 }
212 
213 /**
214  * g_atomic_int_add:
215  * @atomic: a pointer to a #gint or #guint
216  * @val: the value to add
217  *
218  * Atomically adds @val to the value of @atomic.
219  *
220  * Think of this operation as an atomic version of
221  * `{ tmp = *atomic; *atomic += val; return tmp; }`.
222  *
223  * This call acts as a full compiler and hardware memory barrier.
224  *
225  * Before version 2.30, this function did not return a value
226  * (but g_atomic_int_exchange_and_add() did, and had the same meaning).
227  *
228  * Returns: the value of @atomic before the add, signed
229  *
230  * Since: 2.4
231  **/
232 gint
233 (g_atomic_int_add) (volatile gint *atomic,
234                     gint           val)
235 {
236   return g_atomic_int_add (atomic, val);
237 }
238 
239 /**
240  * g_atomic_int_and:
241  * @atomic: a pointer to a #gint or #guint
242  * @val: the value to &#39;and&#39;
243  *
244  * Performs an atomic bitwise &#39;and&#39; of the value of @atomic and @val,
245  * storing the result back in @atomic.
246  *
247  * This call acts as a full compiler and hardware memory barrier.
248  *
249  * Think of this operation as an atomic version of
250  * `{ tmp = *atomic; *atomic &amp;= val; return tmp; }`.
251  *
252  * Returns: the value of @atomic before the operation, unsigned
253  *
254  * Since: 2.30
255  **/
256 guint
257 (g_atomic_int_and) (volatile guint *atomic,
258                     guint           val)
259 {
260   return g_atomic_int_and (atomic, val);
261 }
262 
263 /**
264  * g_atomic_int_or:
265  * @atomic: a pointer to a #gint or #guint
266  * @val: the value to &#39;or&#39;
267  *
268  * Performs an atomic bitwise &#39;or&#39; of the value of @atomic and @val,
269  * storing the result back in @atomic.
270  *
271  * Think of this operation as an atomic version of
272  * `{ tmp = *atomic; *atomic |= val; return tmp; }`.
273  *
274  * This call acts as a full compiler and hardware memory barrier.
275  *
276  * Returns: the value of @atomic before the operation, unsigned
277  *
278  * Since: 2.30
279  **/
280 guint
281 (g_atomic_int_or) (volatile guint *atomic,
282                    guint           val)
283 {
284   return g_atomic_int_or (atomic, val);
285 }
286 
287 /**
288  * g_atomic_int_xor:
289  * @atomic: a pointer to a #gint or #guint
290  * @val: the value to &#39;xor&#39;
291  *
292  * Performs an atomic bitwise &#39;xor&#39; of the value of @atomic and @val,
293  * storing the result back in @atomic.
294  *
295  * Think of this operation as an atomic version of
296  * `{ tmp = *atomic; *atomic ^= val; return tmp; }`.
297  *
298  * This call acts as a full compiler and hardware memory barrier.
299  *
300  * Returns: the value of @atomic before the operation, unsigned
301  *
302  * Since: 2.30
303  **/
304 guint
305 (g_atomic_int_xor) (volatile guint *atomic,
306                     guint           val)
307 {
308   return g_atomic_int_xor (atomic, val);
309 }
310 
311 
312 /**
313  * g_atomic_pointer_get:
314  * @atomic: (not nullable): a pointer to a #gpointer-sized value
315  *
316  * Gets the current value of @atomic.
317  *
318  * This call acts as a full compiler and hardware
319  * memory barrier (before the get).
320  *
321  * Returns: the value of the pointer
322  *
323  * Since: 2.4
324  **/
325 gpointer
326 (g_atomic_pointer_get) (const volatile void *atomic)
327 {
328   return g_atomic_pointer_get ((const volatile gpointer *) atomic);
329 }
330 
331 /**
332  * g_atomic_pointer_set:
333  * @atomic: (not nullable): a pointer to a #gpointer-sized value
334  * @newval: a new value to store
335  *
336  * Sets the value of @atomic to @newval.
337  *
338  * This call acts as a full compiler and hardware
339  * memory barrier (after the set).
340  *
341  * Since: 2.4
342  **/
343 void
344 (g_atomic_pointer_set) (volatile void *atomic,
345                         gpointer       newval)
346 {
347   g_atomic_pointer_set ((volatile gpointer *) atomic, newval);
348 }
349 
350 /**
351  * g_atomic_pointer_compare_and_exchange:
352  * @atomic: (not nullable): a pointer to a #gpointer-sized value
353  * @oldval: the value to compare with
354  * @newval: the value to conditionally replace with
355  *
356  * Compares @atomic to @oldval and, if equal, sets it to @newval.
357  * If @atomic was not equal to @oldval then no change occurs.
358  *
359  * This compare and exchange is done atomically.
360  *
361  * Think of this operation as an atomic version of
362  * `{ if (*atomic == oldval) { *atomic = newval; return TRUE; } else return FALSE; }`.
363  *
364  * This call acts as a full compiler and hardware memory barrier.
365  *
366  * Returns: %TRUE if the exchange took place
367  *
368  * Since: 2.4
369  **/
370 gboolean
371 (g_atomic_pointer_compare_and_exchange) (volatile void *atomic,
372                                          gpointer       oldval,
373                                          gpointer       newval)
374 {
375   return g_atomic_pointer_compare_and_exchange ((volatile gpointer *) atomic,
376                                                 oldval, newval);
377 }
378 
379 /**
380  * g_atomic_pointer_add:
381  * @atomic: (not nullable): a pointer to a #gpointer-sized value
382  * @val: the value to add
383  *
384  * Atomically adds @val to the value of @atomic.
385  *
386  * Think of this operation as an atomic version of
387  * `{ tmp = *atomic; *atomic += val; return tmp; }`.
388  *
389  * This call acts as a full compiler and hardware memory barrier.
390  *
391  * Returns: the value of @atomic before the add, signed
392  *
393  * Since: 2.30
394  **/
395 gssize
396 (g_atomic_pointer_add) (volatile void *atomic,
397                         gssize         val)
398 {
399   return g_atomic_pointer_add ((volatile gpointer *) atomic, val);
400 }
401 
402 /**
403  * g_atomic_pointer_and:
404  * @atomic: (not nullable): a pointer to a #gpointer-sized value
405  * @val: the value to &#39;and&#39;
406  *
407  * Performs an atomic bitwise &#39;and&#39; of the value of @atomic and @val,
408  * storing the result back in @atomic.
409  *
410  * Think of this operation as an atomic version of
411  * `{ tmp = *atomic; *atomic &amp;= val; return tmp; }`.
412  *
413  * This call acts as a full compiler and hardware memory barrier.
414  *
415  * Returns: the value of @atomic before the operation, unsigned
416  *
417  * Since: 2.30
418  **/
419 gsize
420 (g_atomic_pointer_and) (volatile void *atomic,
421                         gsize          val)
422 {
423   return g_atomic_pointer_and ((volatile gpointer *) atomic, val);
424 }
425 
426 /**
427  * g_atomic_pointer_or:
428  * @atomic: (not nullable): a pointer to a #gpointer-sized value
429  * @val: the value to &#39;or&#39;
430  *
431  * Performs an atomic bitwise &#39;or&#39; of the value of @atomic and @val,
432  * storing the result back in @atomic.
433  *
434  * Think of this operation as an atomic version of
435  * `{ tmp = *atomic; *atomic |= val; return tmp; }`.
436  *
437  * This call acts as a full compiler and hardware memory barrier.
438  *
439  * Returns: the value of @atomic before the operation, unsigned
440  *
441  * Since: 2.30
442  **/
443 gsize
444 (g_atomic_pointer_or) (volatile void *atomic,
445                        gsize          val)
446 {
447   return g_atomic_pointer_or ((volatile gpointer *) atomic, val);
448 }
449 
450 /**
451  * g_atomic_pointer_xor:
452  * @atomic: (not nullable): a pointer to a #gpointer-sized value
453  * @val: the value to &#39;xor&#39;
454  *
455  * Performs an atomic bitwise &#39;xor&#39; of the value of @atomic and @val,
456  * storing the result back in @atomic.
457  *
458  * Think of this operation as an atomic version of
459  * `{ tmp = *atomic; *atomic ^= val; return tmp; }`.
460  *
461  * This call acts as a full compiler and hardware memory barrier.
462  *
463  * Returns: the value of @atomic before the operation, unsigned
464  *
465  * Since: 2.30
466  **/
467 gsize
468 (g_atomic_pointer_xor) (volatile void *atomic,
469                         gsize          val)
470 {
471   return g_atomic_pointer_xor ((volatile gpointer *) atomic, val);
472 }
473 
474 #elif defined (G_PLATFORM_WIN32)
475 
476 #include &lt;windows.h&gt;
477 #if !defined(_M_AMD64) &amp;&amp; !defined (_M_IA64) &amp;&amp; !defined(_M_X64) &amp;&amp; !(defined _MSC_VER &amp;&amp; _MSC_VER &lt;= 1200)
478 #define InterlockedAnd _InterlockedAnd
479 #define InterlockedOr _InterlockedOr
480 #define InterlockedXor _InterlockedXor
481 #endif
482 
483 #if !defined (_MSC_VER) || _MSC_VER &lt;= 1200
484 #include &quot;gmessages.h&quot;
485 /* Inlined versions for older compiler */
486 static LONG
487 _gInterlockedAnd (volatile guint *atomic,
488                   guint           val)
489 {
490   LONG i, j;
491 
492   j = *atomic;
493   do {
494     i = j;
495     j = InterlockedCompareExchange(atomic, i &amp; val, i);
496   } while (i != j);
497 
498   return j;
499 }
500 #define InterlockedAnd(a,b) _gInterlockedAnd(a,b)
501 static LONG
502 _gInterlockedOr (volatile guint *atomic,
503                  guint           val)
504 {
505   LONG i, j;
506 
507   j = *atomic;
508   do {
509     i = j;
510     j = InterlockedCompareExchange(atomic, i | val, i);
511   } while (i != j);
512 
513   return j;
514 }
515 #define InterlockedOr(a,b) _gInterlockedOr(a,b)
516 static LONG
517 _gInterlockedXor (volatile guint *atomic,
518                   guint           val)
519 {
520   LONG i, j;
521 
522   j = *atomic;
523   do {
524     i = j;
525     j = InterlockedCompareExchange(atomic, i ^ val, i);
526   } while (i != j);
527 
528   return j;
529 }
530 #define InterlockedXor(a,b) _gInterlockedXor(a,b)
531 #endif
532 
533 /*
534  * http://msdn.microsoft.com/en-us/library/ms684122(v=vs.85).aspx
535  */
536 gint
537 (g_atomic_int_get) (const volatile gint *atomic)
538 {
539   MemoryBarrier ();
540   return *atomic;
541 }
542 
543 void
544 (g_atomic_int_set) (volatile gint *atomic,
545                     gint           newval)
546 {
547   *atomic = newval;
548   MemoryBarrier ();
549 }
550 
551 void
552 (g_atomic_int_inc) (volatile gint *atomic)
553 {
554   InterlockedIncrement (atomic);
555 }
556 
557 gboolean
558 (g_atomic_int_dec_and_test) (volatile gint *atomic)
559 {
560   return InterlockedDecrement (atomic) == 0;
561 }
562 
563 gboolean
564 (g_atomic_int_compare_and_exchange) (volatile gint *atomic,
565                                      gint           oldval,
566                                      gint           newval)
567 {
568   return InterlockedCompareExchange (atomic, newval, oldval) == oldval;
569 }
570 
571 gint
572 (g_atomic_int_add) (volatile gint *atomic,
573                     gint           val)
574 {
575   return InterlockedExchangeAdd (atomic, val);
576 }
577 
578 guint
579 (g_atomic_int_and) (volatile guint *atomic,
580                     guint           val)
581 {
582   return InterlockedAnd (atomic, val);
583 }
584 
585 guint
586 (g_atomic_int_or) (volatile guint *atomic,
587                    guint           val)
588 {
589   return InterlockedOr (atomic, val);
590 }
591 
592 guint
593 (g_atomic_int_xor) (volatile guint *atomic,
594                     guint           val)
595 {
596   return InterlockedXor (atomic, val);
597 }
598 
599 
600 gpointer
601 (g_atomic_pointer_get) (const volatile void *atomic)
602 {
603   const volatile gpointer *ptr = atomic;
604 
605   MemoryBarrier ();
606   return *ptr;
607 }
608 
609 void
610 (g_atomic_pointer_set) (volatile void *atomic,
611                         gpointer       newval)
612 {
613   volatile gpointer *ptr = atomic;
614 
615   *ptr = newval;
616   MemoryBarrier ();
617 }
618 
619 gboolean
620 (g_atomic_pointer_compare_and_exchange) (volatile void *atomic,
621                                          gpointer       oldval,
622                                          gpointer       newval)
623 {
624   return InterlockedCompareExchangePointer (atomic, newval, oldval) == oldval;
625 }
626 
627 gssize
628 (g_atomic_pointer_add) (volatile void *atomic,
629                         gssize         val)
630 {
631 #if GLIB_SIZEOF_VOID_P == 8
632   return InterlockedExchangeAdd64 (atomic, val);
633 #else
634   return InterlockedExchangeAdd (atomic, val);
635 #endif
636 }
637 
638 gsize
639 (g_atomic_pointer_and) (volatile void *atomic,
640                         gsize          val)
641 {
642 #if GLIB_SIZEOF_VOID_P == 8
643   return InterlockedAnd64 (atomic, val);
644 #else
645   return InterlockedAnd (atomic, val);
646 #endif
647 }
648 
649 gsize
650 (g_atomic_pointer_or) (volatile void *atomic,
651                        gsize          val)
652 {
653 #if GLIB_SIZEOF_VOID_P == 8
654   return InterlockedOr64 (atomic, val);
655 #else
656   return InterlockedOr (atomic, val);
657 #endif
658 }
659 
660 gsize
661 (g_atomic_pointer_xor) (volatile void *atomic,
662                         gsize          val)
663 {
664 #if GLIB_SIZEOF_VOID_P == 8
665   return InterlockedXor64 (atomic, val);
666 #else
667   return InterlockedXor (atomic, val);
668 #endif
669 }
670 #else
671 
<a name="3" id="anc3"></a><span class="line-modified">672 /* This error occurs when `meson configure` decided that we should be capable</span>
673  * of lock-free atomics but we find at compile-time that we are not.
674  */
675 #error G_ATOMIC_LOCK_FREE defined, but incapable of lock-free atomics.
676 
677 #endif /* defined (__GCC_HAVE_SYNC_COMPARE_AND_SWAP_4) */
678 
679 #else /* G_ATOMIC_LOCK_FREE */
680 
681 /* We are not permitted to call into any GLib functions from here, so we
682  * can not use GMutex.
683  *
684  * Fortunately, we already take care of the Windows case above, and all
685  * non-Windows platforms on which glib runs have pthreads.  Use those.
686  */
687 #include &lt;pthread.h&gt;
688 
689 static pthread_mutex_t g_atomic_lock = PTHREAD_MUTEX_INITIALIZER;
690 
691 gint
692 (g_atomic_int_get) (const volatile gint *atomic)
693 {
694   gint value;
695 
696   pthread_mutex_lock (&amp;g_atomic_lock);
697   value = *atomic;
698   pthread_mutex_unlock (&amp;g_atomic_lock);
699 
700   return value;
701 }
702 
703 void
704 (g_atomic_int_set) (volatile gint *atomic,
705                     gint           value)
706 {
707   pthread_mutex_lock (&amp;g_atomic_lock);
708   *atomic = value;
709   pthread_mutex_unlock (&amp;g_atomic_lock);
710 }
711 
712 void
713 (g_atomic_int_inc) (volatile gint *atomic)
714 {
715   pthread_mutex_lock (&amp;g_atomic_lock);
716   (*atomic)++;
717   pthread_mutex_unlock (&amp;g_atomic_lock);
718 }
719 
720 gboolean
721 (g_atomic_int_dec_and_test) (volatile gint *atomic)
722 {
723   gboolean is_zero;
724 
725   pthread_mutex_lock (&amp;g_atomic_lock);
726   is_zero = --(*atomic) == 0;
727   pthread_mutex_unlock (&amp;g_atomic_lock);
728 
729   return is_zero;
730 }
731 
732 gboolean
733 (g_atomic_int_compare_and_exchange) (volatile gint *atomic,
734                                      gint           oldval,
735                                      gint           newval)
736 {
737   gboolean success;
738 
739   pthread_mutex_lock (&amp;g_atomic_lock);
740 
741   if ((success = (*atomic == oldval)))
742     *atomic = newval;
743 
744   pthread_mutex_unlock (&amp;g_atomic_lock);
745 
746   return success;
747 }
748 
749 gint
750 (g_atomic_int_add) (volatile gint *atomic,
751                     gint           val)
752 {
753   gint oldval;
754 
755 #ifdef GSTREAMER_LITE
756   if (atomic == NULL) {
757     return;
758   }
759 #endif // GSTREAMER_LITE
760 
761   pthread_mutex_lock (&amp;g_atomic_lock);
762   oldval = *atomic;
763   *atomic = oldval + val;
764   pthread_mutex_unlock (&amp;g_atomic_lock);
765 
766   return oldval;
767 }
768 
769 guint
770 (g_atomic_int_and) (volatile guint *atomic,
771                     guint           val)
772 {
773   guint oldval;
774 
775 #ifdef GSTREAMER_LITE
776   if (atomic == NULL) {
777     return;
778   }
779 #endif // GSTREAMER_LITE
780 
781   pthread_mutex_lock (&amp;g_atomic_lock);
782   oldval = *atomic;
783   *atomic = oldval &amp; val;
784   pthread_mutex_unlock (&amp;g_atomic_lock);
785 
786   return oldval;
787 }
788 
789 guint
790 (g_atomic_int_or) (volatile guint *atomic,
791                    guint           val)
792 {
793   guint oldval;
794 
795   pthread_mutex_lock (&amp;g_atomic_lock);
796   oldval = *atomic;
797   *atomic = oldval | val;
798   pthread_mutex_unlock (&amp;g_atomic_lock);
799 
800   return oldval;
801 }
802 
803 guint
804 (g_atomic_int_xor) (volatile guint *atomic,
805                     guint           val)
806 {
807   guint oldval;
808 
809   pthread_mutex_lock (&amp;g_atomic_lock);
810   oldval = *atomic;
811   *atomic = oldval ^ val;
812   pthread_mutex_unlock (&amp;g_atomic_lock);
813 
814   return oldval;
815 }
816 
817 
818 gpointer
819 (g_atomic_pointer_get) (const volatile void *atomic)
820 {
821   const volatile gpointer *ptr = atomic;
822   gpointer value;
823 
824   pthread_mutex_lock (&amp;g_atomic_lock);
825   value = *ptr;
826   pthread_mutex_unlock (&amp;g_atomic_lock);
827 
828   return value;
829 }
830 
831 void
832 (g_atomic_pointer_set) (volatile void *atomic,
833                         gpointer       newval)
834 {
835   volatile gpointer *ptr = atomic;
836 
837   pthread_mutex_lock (&amp;g_atomic_lock);
838   *ptr = newval;
839   pthread_mutex_unlock (&amp;g_atomic_lock);
840 }
841 
842 gboolean
843 (g_atomic_pointer_compare_and_exchange) (volatile void *atomic,
844                                          gpointer       oldval,
845                                          gpointer       newval)
846 {
847   volatile gpointer *ptr = atomic;
848   gboolean success;
849 
850   pthread_mutex_lock (&amp;g_atomic_lock);
851 
852   if ((success = (*ptr == oldval)))
853     *ptr = newval;
854 
855   pthread_mutex_unlock (&amp;g_atomic_lock);
856 
857   return success;
858 }
859 
860 gssize
861 (g_atomic_pointer_add) (volatile void *atomic,
862                         gssize         val)
863 {
864   volatile gssize *ptr = atomic;
865   gssize oldval;
866 
867   pthread_mutex_lock (&amp;g_atomic_lock);
868   oldval = *ptr;
869   *ptr = oldval + val;
870   pthread_mutex_unlock (&amp;g_atomic_lock);
871 
872   return oldval;
873 }
874 
875 gsize
876 (g_atomic_pointer_and) (volatile void *atomic,
877                         gsize          val)
878 {
879   volatile gsize *ptr = atomic;
880   gsize oldval;
881 
882   pthread_mutex_lock (&amp;g_atomic_lock);
883   oldval = *ptr;
884   *ptr = oldval &amp; val;
885   pthread_mutex_unlock (&amp;g_atomic_lock);
886 
887   return oldval;
888 }
889 
890 gsize
891 (g_atomic_pointer_or) (volatile void *atomic,
892                        gsize          val)
893 {
894   volatile gsize *ptr = atomic;
895   gsize oldval;
896 
897   pthread_mutex_lock (&amp;g_atomic_lock);
898   oldval = *ptr;
899   *ptr = oldval | val;
900   pthread_mutex_unlock (&amp;g_atomic_lock);
901 
902   return oldval;
903 }
904 
905 gsize
906 (g_atomic_pointer_xor) (volatile void *atomic,
907                         gsize          val)
908 {
909   volatile gsize *ptr = atomic;
910   gsize oldval;
911 
912   pthread_mutex_lock (&amp;g_atomic_lock);
913   oldval = *ptr;
914   *ptr = oldval ^ val;
915   pthread_mutex_unlock (&amp;g_atomic_lock);
916 
917   return oldval;
918 }
919 
920 #endif
921 
922 /**
923  * g_atomic_int_exchange_and_add:
924  * @atomic: a pointer to a #gint
925  * @val: the value to add
926  *
927  * This function existed before g_atomic_int_add() returned the prior
928  * value of the integer (which it now does).  It is retained only for
929  * compatibility reasons.  Don&#39;t use this function in new code.
930  *
931  * Returns: the value of @atomic before the add, signed
932  * Since: 2.4
933  * Deprecated: 2.30: Use g_atomic_int_add() instead.
934  **/
935 gint
936 g_atomic_int_exchange_and_add (volatile gint *atomic,
937                                gint           val)
938 {
939   return (g_atomic_int_add) (atomic, val);
940 }
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>