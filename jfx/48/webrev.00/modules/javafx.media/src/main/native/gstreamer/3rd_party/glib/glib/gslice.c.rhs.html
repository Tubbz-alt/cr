<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.media/src/main/native/gstreamer/3rd_party/glib/glib/gslice.c</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /* GLIB sliced memory - fast concurrent memory chunk allocator
   2  * Copyright (C) 2005 Tim Janik
   3  *
   4  * This library is free software; you can redistribute it and/or
   5  * modify it under the terms of the GNU Lesser General Public
   6  * License as published by the Free Software Foundation; either
   7  * version 2.1 of the License, or (at your option) any later version.
   8  *
   9  * This library is distributed in the hope that it will be useful,
  10  * but WITHOUT ANY WARRANTY; without even the implied warranty of
  11  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
  12  * Lesser General Public License for more details.
  13  *
  14  * You should have received a copy of the GNU Lesser General Public
  15  * License along with this library; if not, see &lt;http://www.gnu.org/licenses/&gt;.
  16  */
  17 /* MT safe */
  18 
  19 #include &quot;config.h&quot;
  20 #include &quot;glibconfig.h&quot;
  21 
  22 #if defined(HAVE_POSIX_MEMALIGN) &amp;&amp; !defined(_XOPEN_SOURCE)
  23 #define _XOPEN_SOURCE 600       /* posix_memalign() */
  24 #endif
  25 #include &lt;stdlib.h&gt;             /* posix_memalign() */
  26 #include &lt;string.h&gt;
  27 #include &lt;errno.h&gt;
  28 
  29 #ifdef G_OS_UNIX
  30 #include &lt;unistd.h&gt;             /* sysconf() */
  31 #endif
  32 #ifdef G_OS_WIN32
  33 #include &lt;windows.h&gt;
  34 #include &lt;process.h&gt;
  35 #endif
  36 
  37 #include &lt;stdio.h&gt;              /* fputs */
  38 
  39 #include &quot;gslice.h&quot;
  40 
  41 #include &quot;gmain.h&quot;
  42 #include &quot;gmem.h&quot;               /* gslice.h */
  43 #include &quot;gstrfuncs.h&quot;
  44 #include &quot;gutils.h&quot;
  45 #include &quot;gtrashstack.h&quot;
  46 #include &quot;gtestutils.h&quot;
  47 #include &quot;gthread.h&quot;
<a name="1" id="anc1"></a><span class="line-added">  48 #include &quot;gthreadprivate.h&quot;</span>
  49 #include &quot;glib_trace.h&quot;
  50 #include &quot;gprintf.h&quot;
  51 
  52 #include &quot;gvalgrind.h&quot;
  53 
  54 /**
  55  * SECTION:memory_slices
  56  * @title: Memory Slices
  57  * @short_description: efficient way to allocate groups of equal-sized
  58  *     chunks of memory
  59  *
  60  * Memory slices provide a space-efficient and multi-processing scalable
  61  * way to allocate equal-sized pieces of memory, just like the original
  62  * #GMemChunks (from GLib 2.8), while avoiding their excessive
  63  * memory-waste, scalability and performance problems.
  64  *
  65  * To achieve these goals, the slice allocator uses a sophisticated,
  66  * layered design that has been inspired by Bonwick&#39;s slab allocator
  67  * ([Bonwick94](http://citeseer.ist.psu.edu/bonwick94slab.html)
  68  * Jeff Bonwick, The slab allocator: An object-caching kernel
  69  * memory allocator. USENIX 1994, and
  70  * [Bonwick01](http://citeseer.ist.psu.edu/bonwick01magazines.html)
  71  * Bonwick and Jonathan Adams, Magazines and vmem: Extending the
  72  * slab allocator to many cpu&#39;s and arbitrary resources. USENIX 2001)
  73  *
  74  * It uses posix_memalign() to optimize allocations of many equally-sized
  75  * chunks, and has per-thread free lists (the so-called magazine layer)
  76  * to quickly satisfy allocation requests of already known structure sizes.
  77  * This is accompanied by extra caching logic to keep freed memory around
  78  * for some time before returning it to the system. Memory that is unused
  79  * due to alignment constraints is used for cache colorization (random
  80  * distribution of chunk addresses) to improve CPU cache utilization. The
  81  * caching layer of the slice allocator adapts itself to high lock contention
  82  * to improve scalability.
  83  *
  84  * The slice allocator can allocate blocks as small as two pointers, and
  85  * unlike malloc(), it does not reserve extra space per block. For large block
  86  * sizes, g_slice_new() and g_slice_alloc() will automatically delegate to the
  87  * system malloc() implementation. For newly written code it is recommended
  88  * to use the new `g_slice` API instead of g_malloc() and
  89  * friends, as long as objects are not resized during their lifetime and the
  90  * object size used at allocation time is still available when freeing.
  91  *
  92  * Here is an example for using the slice allocator:
  93  * |[&lt;!-- language=&quot;C&quot; --&gt;
  94  * gchar *mem[10000];
  95  * gint i;
  96  *
  97  * // Allocate 10000 blocks.
  98  * for (i = 0; i &lt; 10000; i++)
  99  *   {
 100  *     mem[i] = g_slice_alloc (50);
 101  *
 102  *     // Fill in the memory with some junk.
 103  *     for (j = 0; j &lt; 50; j++)
 104  *       mem[i][j] = i * j;
 105  *   }
 106  *
 107  * // Now free all of the blocks.
 108  * for (i = 0; i &lt; 10000; i++)
 109  *   g_slice_free1 (50, mem[i]);
 110  * ]|
 111  *
 112  * And here is an example for using the using the slice allocator
 113  * with data structures:
 114  * |[&lt;!-- language=&quot;C&quot; --&gt;
 115  * GRealArray *array;
 116  *
 117  * // Allocate one block, using the g_slice_new() macro.
 118  * array = g_slice_new (GRealArray);
 119  *
 120  * // We can now use array just like a normal pointer to a structure.
 121  * array-&gt;data            = NULL;
 122  * array-&gt;len             = 0;
 123  * array-&gt;alloc           = 0;
 124  * array-&gt;zero_terminated = (zero_terminated ? 1 : 0);
 125  * array-&gt;clear           = (clear ? 1 : 0);
 126  * array-&gt;elt_size        = elt_size;
 127  *
 128  * // We can free the block, so it can be reused.
 129  * g_slice_free (GRealArray, array);
 130  * ]|
 131  */
 132 
 133 /* the GSlice allocator is split up into 4 layers, roughly modelled after the slab
 134  * allocator and magazine extensions as outlined in:
 135  * + [Bonwick94] Jeff Bonwick, The slab allocator: An object-caching kernel
 136  *   memory allocator. USENIX 1994, http://citeseer.ist.psu.edu/bonwick94slab.html
 137  * + [Bonwick01] Bonwick and Jonathan Adams, Magazines and vmem: Extending the
 138  *   slab allocator to many cpu&#39;s and arbitrary resources.
 139  *   USENIX 2001, http://citeseer.ist.psu.edu/bonwick01magazines.html
 140  * the layers are:
 141  * - the thread magazines. for each (aligned) chunk size, a magazine (a list)
 142  *   of recently freed and soon to be allocated chunks is maintained per thread.
 143  *   this way, most alloc/free requests can be quickly satisfied from per-thread
 144  *   free lists which only require one g_private_get() call to retrive the
 145  *   thread handle.
 146  * - the magazine cache. allocating and freeing chunks to/from threads only
 147  *   occours at magazine sizes from a global depot of magazines. the depot
 148  *   maintaines a 15 second working set of allocated magazines, so full
 149  *   magazines are not allocated and released too often.
 150  *   the chunk size dependent magazine sizes automatically adapt (within limits,
 151  *   see [3]) to lock contention to properly scale performance across a variety
 152  *   of SMP systems.
 153  * - the slab allocator. this allocator allocates slabs (blocks of memory) close
 154  *   to the system page size or multiples thereof which have to be page aligned.
 155  *   the blocks are divided into smaller chunks which are used to satisfy
 156  *   allocations from the upper layers. the space provided by the reminder of
 157  *   the chunk size division is used for cache colorization (random distribution
 158  *   of chunk addresses) to improve processor cache utilization. multiple slabs
 159  *   with the same chunk size are kept in a partially sorted ring to allow O(1)
 160  *   freeing and allocation of chunks (as long as the allocation of an entirely
 161  *   new slab can be avoided).
 162  * - the page allocator. on most modern systems, posix_memalign(3) or
 163  *   memalign(3) should be available, so this is used to allocate blocks with
 164  *   system page size based alignments and sizes or multiples thereof.
 165  *   if no memalign variant is provided, valloc() is used instead and
 166  *   block sizes are limited to the system page size (no multiples thereof).
 167  *   as a fallback, on system without even valloc(), a malloc(3)-based page
 168  *   allocator with alloc-only behaviour is used.
 169  *
 170  * NOTES:
 171  * [1] some systems memalign(3) implementations may rely on boundary tagging for
 172  *     the handed out memory chunks. to avoid excessive page-wise fragmentation,
 173  *     we reserve 2 * sizeof (void*) per block size for the systems memalign(3),
 174  *     specified in NATIVE_MALLOC_PADDING.
 175  * [2] using the slab allocator alone already provides for a fast and efficient
 176  *     allocator, it doesn&#39;t properly scale beyond single-threaded uses though.
 177  *     also, the slab allocator implements eager free(3)-ing, i.e. does not
 178  *     provide any form of caching or working set maintenance. so if used alone,
 179  *     it&#39;s vulnerable to trashing for sequences of balanced (alloc, free) pairs
 180  *     at certain thresholds.
 181  * [3] magazine sizes are bound by an implementation specific minimum size and
 182  *     a chunk size specific maximum to limit magazine storage sizes to roughly
 183  *     16KB.
 184  * [4] allocating ca. 8 chunks per block/page keeps a good balance between
 185  *     external and internal fragmentation (&lt;= 12.5%). [Bonwick94]
 186  */
 187 
 188 /* --- macros and constants --- */
 189 #define LARGEALIGNMENT          (256)
 190 #define P2ALIGNMENT             (2 * sizeof (gsize))                            /* fits 2 pointers (assumed to be 2 * GLIB_SIZEOF_SIZE_T below) */
 191 #define ALIGN(size, base)       ((base) * (gsize) (((size) + (base) - 1) / (base)))
 192 #define NATIVE_MALLOC_PADDING   P2ALIGNMENT                                     /* per-page padding left for native malloc(3) see [1] */
 193 #define SLAB_INFO_SIZE          P2ALIGN (sizeof (SlabInfo) + NATIVE_MALLOC_PADDING)
 194 #define MAX_MAGAZINE_SIZE       (256)                                           /* see [3] and allocator_get_magazine_threshold() for this */
 195 #define MIN_MAGAZINE_SIZE       (4)
 196 #define MAX_STAMP_COUNTER       (7)                                             /* distributes the load of gettimeofday() */
 197 #define MAX_SLAB_CHUNK_SIZE(al) (((al)-&gt;max_page_size - SLAB_INFO_SIZE) / 8)    /* we want at last 8 chunks per page, see [4] */
 198 #define MAX_SLAB_INDEX(al)      (SLAB_INDEX (al, MAX_SLAB_CHUNK_SIZE (al)) + 1)
 199 #define SLAB_INDEX(al, asize)   ((asize) / P2ALIGNMENT - 1)                     /* asize must be P2ALIGNMENT aligned */
 200 #define SLAB_CHUNK_SIZE(al, ix) (((ix) + 1) * P2ALIGNMENT)
 201 #define SLAB_BPAGE_SIZE(al,csz) (8 * (csz) + SLAB_INFO_SIZE)
 202 
 203 /* optimized version of ALIGN (size, P2ALIGNMENT) */
 204 #if     GLIB_SIZEOF_SIZE_T * 2 == 8  /* P2ALIGNMENT */
 205 #define P2ALIGN(size)   (((size) + 0x7) &amp; ~(gsize) 0x7)
 206 #elif   GLIB_SIZEOF_SIZE_T * 2 == 16 /* P2ALIGNMENT */
 207 #define P2ALIGN(size)   (((size) + 0xf) &amp; ~(gsize) 0xf)
 208 #else
 209 #define P2ALIGN(size)   ALIGN (size, P2ALIGNMENT)
 210 #endif
 211 
 212 /* special helpers to avoid gmessage.c dependency */
 213 static void mem_error (const char *format, ...) G_GNUC_PRINTF (1,2);
 214 #define mem_assert(cond)    do { if (G_LIKELY (cond)) ; else mem_error (&quot;assertion failed: %s&quot;, #cond); } while (0)
 215 
 216 /* --- structures --- */
 217 typedef struct _ChunkLink      ChunkLink;
 218 typedef struct _SlabInfo       SlabInfo;
 219 typedef struct _CachedMagazine CachedMagazine;
 220 struct _ChunkLink {
 221   ChunkLink *next;
 222   ChunkLink *data;
 223 };
 224 struct _SlabInfo {
 225   ChunkLink *chunks;
 226   guint n_allocated;
 227   SlabInfo *next, *prev;
 228 };
 229 typedef struct {
 230   ChunkLink *chunks;
 231   gsize      count;                     /* approximative chunks list length */
 232 } Magazine;
 233 typedef struct {
 234   Magazine   *magazine1;                /* array of MAX_SLAB_INDEX (allocator) */
 235   Magazine   *magazine2;                /* array of MAX_SLAB_INDEX (allocator) */
 236 } ThreadMemory;
 237 typedef struct {
 238   gboolean always_malloc;
 239   gboolean bypass_magazines;
 240   gboolean debug_blocks;
 241   gsize    working_set_msecs;
 242   guint    color_increment;
 243 } SliceConfig;
 244 typedef struct {
 245   /* const after initialization */
 246   gsize         min_page_size, max_page_size;
 247   SliceConfig   config;
 248   gsize         max_slab_chunk_size_for_magazine_cache;
 249   /* magazine cache */
 250   GMutex        magazine_mutex;
 251   ChunkLink   **magazines;                /* array of MAX_SLAB_INDEX (allocator) */
 252   guint        *contention_counters;      /* array of MAX_SLAB_INDEX (allocator) */
 253   gint          mutex_counter;
 254   guint         stamp_counter;
 255   guint         last_stamp;
 256   /* slab allocator */
 257   GMutex        slab_mutex;
 258   SlabInfo    **slab_stack;                /* array of MAX_SLAB_INDEX (allocator) */
 259   guint        color_accu;
 260 } Allocator;
 261 
 262 /* --- g-slice prototypes --- */
 263 static gpointer     slab_allocator_alloc_chunk       (gsize      chunk_size);
 264 static void         slab_allocator_free_chunk        (gsize      chunk_size,
 265                                                       gpointer   mem);
 266 static void         private_thread_memory_cleanup    (gpointer   data);
 267 static gpointer     allocator_memalign               (gsize      alignment,
 268                                                       gsize      memsize);
 269 static void         allocator_memfree                (gsize      memsize,
 270                                                       gpointer   mem);
 271 static inline void  magazine_cache_update_stamp      (void);
 272 static inline gsize allocator_get_magazine_threshold (Allocator *allocator,
 273                                                       guint      ix);
 274 
 275 /* --- g-slice memory checker --- */
 276 static void     smc_notify_alloc  (void   *pointer,
 277                                    size_t  size);
 278 static int      smc_notify_free   (void   *pointer,
 279                                    size_t  size);
 280 
 281 /* --- variables --- */
 282 static GPrivate    private_thread_memory = G_PRIVATE_INIT (private_thread_memory_cleanup);
 283 static gsize       sys_page_size = 0;
 284 static Allocator   allocator[1] = { { 0, }, };
 285 static SliceConfig slice_config = {
 286   FALSE,        /* always_malloc */
 287   FALSE,        /* bypass_magazines */
 288   FALSE,        /* debug_blocks */
 289   15 * 1000,    /* working_set_msecs */
 290   1,            /* color increment, alt: 0x7fffffff */
 291 };
 292 static GMutex      smc_tree_mutex; /* mutex for G_SLICE=debug-blocks */
 293 
 294 /* --- auxiliary funcitons --- */
 295 void
 296 g_slice_set_config (GSliceConfig ckey,
 297                     gint64       value)
 298 {
 299   g_return_if_fail (sys_page_size == 0);
 300   switch (ckey)
 301     {
 302     case G_SLICE_CONFIG_ALWAYS_MALLOC:
 303       slice_config.always_malloc = value != 0;
 304       break;
 305     case G_SLICE_CONFIG_BYPASS_MAGAZINES:
 306       slice_config.bypass_magazines = value != 0;
 307       break;
 308     case G_SLICE_CONFIG_WORKING_SET_MSECS:
 309       slice_config.working_set_msecs = value;
 310       break;
 311     case G_SLICE_CONFIG_COLOR_INCREMENT:
 312       slice_config.color_increment = value;
 313     default: ;
 314     }
 315 }
 316 
 317 gint64
 318 g_slice_get_config (GSliceConfig ckey)
 319 {
 320   switch (ckey)
 321     {
 322     case G_SLICE_CONFIG_ALWAYS_MALLOC:
 323       return slice_config.always_malloc;
 324     case G_SLICE_CONFIG_BYPASS_MAGAZINES:
 325       return slice_config.bypass_magazines;
 326     case G_SLICE_CONFIG_WORKING_SET_MSECS:
 327       return slice_config.working_set_msecs;
 328     case G_SLICE_CONFIG_CHUNK_SIZES:
 329       return MAX_SLAB_INDEX (allocator);
 330     case G_SLICE_CONFIG_COLOR_INCREMENT:
 331       return slice_config.color_increment;
 332     default:
 333       return 0;
 334     }
 335 }
 336 
 337 gint64*
 338 g_slice_get_config_state (GSliceConfig ckey,
 339                           gint64       address,
 340                           guint       *n_values)
 341 {
 342   guint i = 0;
 343   g_return_val_if_fail (n_values != NULL, NULL);
 344   *n_values = 0;
 345   switch (ckey)
 346     {
 347       gint64 array[64];
 348     case G_SLICE_CONFIG_CONTENTION_COUNTER:
 349       array[i++] = SLAB_CHUNK_SIZE (allocator, address);
 350       array[i++] = allocator-&gt;contention_counters[address];
 351       array[i++] = allocator_get_magazine_threshold (allocator, address);
 352       *n_values = i;
 353       return g_memdup (array, sizeof (array[0]) * *n_values);
 354     default:
 355       return NULL;
 356     }
 357 }
 358 
 359 static void
 360 slice_config_init (SliceConfig *config)
 361 {
 362 #ifndef GSTREAMER_LITE
 363   const gchar *val;
 364 
 365   *config = slice_config;
 366 
 367   val = getenv (&quot;G_SLICE&quot;);
 368   if (val != NULL)
 369     {
 370       gint flags;
 371       const GDebugKey keys[] = {
 372         { &quot;always-malloc&quot;, 1 &lt;&lt; 0 },
 373         { &quot;debug-blocks&quot;,  1 &lt;&lt; 1 },
 374       };
 375 
 376       flags = g_parse_debug_string (val, keys, G_N_ELEMENTS (keys));
 377       if (flags &amp; (1 &lt;&lt; 0))
 378         config-&gt;always_malloc = TRUE;
 379       if (flags &amp; (1 &lt;&lt; 1))
 380         config-&gt;debug_blocks = TRUE;
 381     }
 382   else
 383     {
 384       /* G_SLICE was not specified, so check if valgrind is running and
 385        * disable ourselves if it is.
 386        *
 387        * This way it&#39;s possible to force gslice to be enabled under
 388        * valgrind just by setting G_SLICE to the empty string.
 389        */
 390 #ifdef ENABLE_VALGRIND
 391       if (RUNNING_ON_VALGRIND)
 392         config-&gt;always_malloc = TRUE;
 393 #endif
 394     }
 395 #else // GSTREAMER_LITE
 396   *config = slice_config;
 397   config-&gt;always_malloc = TRUE;
 398 #endif // GSTREAMER_LITE
 399 }
 400 
 401 static void
 402 g_slice_init_nomessage (void)
 403 {
 404   /* we may not use g_error() or friends here */
 405   mem_assert (sys_page_size == 0);
 406   mem_assert (MIN_MAGAZINE_SIZE &gt;= 4);
 407 
 408 #ifdef G_OS_WIN32
 409   {
 410     SYSTEM_INFO system_info;
 411     GetSystemInfo (&amp;system_info);
 412     sys_page_size = system_info.dwPageSize;
 413   }
 414 #else
 415   sys_page_size = sysconf (_SC_PAGESIZE); /* = sysconf (_SC_PAGE_SIZE); = getpagesize(); */
 416 #endif
 417   mem_assert (sys_page_size &gt;= 2 * LARGEALIGNMENT);
 418   mem_assert ((sys_page_size &amp; (sys_page_size - 1)) == 0);
 419   slice_config_init (&amp;allocator-&gt;config);
 420   allocator-&gt;min_page_size = sys_page_size;
 421 #if HAVE_POSIX_MEMALIGN || HAVE_MEMALIGN
 422   /* allow allocation of pages up to 8KB (with 8KB alignment).
 423    * this is useful because many medium to large sized structures
 424    * fit less than 8 times (see [4]) into 4KB pages.
 425    * we allow very small page sizes here, to reduce wastage in
 426    * threads if only small allocations are required (this does
 427    * bear the risk of increasing allocation times and fragmentation
 428    * though).
 429    */
 430   allocator-&gt;min_page_size = MAX (allocator-&gt;min_page_size, 4096);
 431   allocator-&gt;max_page_size = MAX (allocator-&gt;min_page_size, 8192);
 432   allocator-&gt;min_page_size = MIN (allocator-&gt;min_page_size, 128);
 433 #else
 434   /* we can only align to system page size */
 435   allocator-&gt;max_page_size = sys_page_size;
 436 #endif
 437   if (allocator-&gt;config.always_malloc)
 438     {
 439       allocator-&gt;contention_counters = NULL;
 440       allocator-&gt;magazines = NULL;
 441       allocator-&gt;slab_stack = NULL;
 442     }
 443   else
 444     {
 445       allocator-&gt;contention_counters = g_new0 (guint, MAX_SLAB_INDEX (allocator));
 446       allocator-&gt;magazines = g_new0 (ChunkLink*, MAX_SLAB_INDEX (allocator));
 447       allocator-&gt;slab_stack = g_new0 (SlabInfo*, MAX_SLAB_INDEX (allocator));
 448     }
 449 
 450   allocator-&gt;mutex_counter = 0;
 451   allocator-&gt;stamp_counter = MAX_STAMP_COUNTER; /* force initial update */
 452   allocator-&gt;last_stamp = 0;
 453   allocator-&gt;color_accu = 0;
 454   magazine_cache_update_stamp();
 455   /* values cached for performance reasons */
 456   allocator-&gt;max_slab_chunk_size_for_magazine_cache = MAX_SLAB_CHUNK_SIZE (allocator);
 457   if (allocator-&gt;config.always_malloc || allocator-&gt;config.bypass_magazines)
 458     allocator-&gt;max_slab_chunk_size_for_magazine_cache = 0;      /* non-optimized cases */
 459 }
 460 
 461 static inline guint
 462 allocator_categorize (gsize aligned_chunk_size)
 463 {
 464   /* speed up the likely path */
 465   if (G_LIKELY (aligned_chunk_size &amp;&amp; aligned_chunk_size &lt;= allocator-&gt;max_slab_chunk_size_for_magazine_cache))
 466     return 1;           /* use magazine cache */
 467 
 468   if (!allocator-&gt;config.always_malloc &amp;&amp;
 469       aligned_chunk_size &amp;&amp;
 470       aligned_chunk_size &lt;= MAX_SLAB_CHUNK_SIZE (allocator))
 471     {
 472       if (allocator-&gt;config.bypass_magazines)
 473         return 2;       /* use slab allocator, see [2] */
 474       return 1;         /* use magazine cache */
 475     }
 476   return 0;             /* use malloc() */
 477 }
 478 
 479 static inline void
 480 g_mutex_lock_a (GMutex *mutex,
 481                 guint  *contention_counter)
 482 {
 483   gboolean contention = FALSE;
 484   if (!g_mutex_trylock (mutex))
 485     {
 486       g_mutex_lock (mutex);
 487       contention = TRUE;
 488     }
 489   if (contention)
 490     {
 491       allocator-&gt;mutex_counter++;
 492       if (allocator-&gt;mutex_counter &gt;= 1)        /* quickly adapt to contention */
 493         {
 494           allocator-&gt;mutex_counter = 0;
 495           *contention_counter = MIN (*contention_counter + 1, MAX_MAGAZINE_SIZE);
 496         }
 497     }
 498   else /* !contention */
 499     {
 500       allocator-&gt;mutex_counter--;
 501       if (allocator-&gt;mutex_counter &lt; -11)       /* moderately recover magazine sizes */
 502         {
 503           allocator-&gt;mutex_counter = 0;
 504           *contention_counter = MAX (*contention_counter, 1) - 1;
 505         }
 506     }
 507 }
 508 
 509 static inline ThreadMemory*
 510 thread_memory_from_self (void)
 511 {
 512   ThreadMemory *tmem = g_private_get (&amp;private_thread_memory);
 513   if (G_UNLIKELY (!tmem))
 514     {
 515       static GMutex init_mutex;
 516       guint n_magazines;
 517 
 518       g_mutex_lock (&amp;init_mutex);
 519       if G_UNLIKELY (sys_page_size == 0)
 520         g_slice_init_nomessage ();
 521       g_mutex_unlock (&amp;init_mutex);
 522 
 523       n_magazines = MAX_SLAB_INDEX (allocator);
<a name="2" id="anc2"></a><span class="line-modified"> 524       tmem = g_private_set_alloc0 (&amp;private_thread_memory, sizeof (ThreadMemory) + sizeof (Magazine) * 2 * n_magazines);</span>
 525 #ifdef GSTREAMER_LITE
 526       if (tmem == NULL)
<a name="3" id="anc3"></a><span class="line-modified"> 527         return NULL;</span>
 528 #endif // GSTREAMER_LITE
 529       tmem-&gt;magazine1 = (Magazine*) (tmem + 1);
 530       tmem-&gt;magazine2 = &amp;tmem-&gt;magazine1[n_magazines];
<a name="4" id="anc4"></a>
 531     }
 532   return tmem;
 533 }
 534 
 535 static inline ChunkLink*
 536 magazine_chain_pop_head (ChunkLink **magazine_chunks)
 537 {
 538   /* magazine chains are linked via ChunkLink-&gt;next.
 539    * each ChunkLink-&gt;data of the toplevel chain may point to a subchain,
 540    * linked via ChunkLink-&gt;next. ChunkLink-&gt;data of the subchains just
 541    * contains uninitialized junk.
 542    */
 543   ChunkLink *chunk = (*magazine_chunks)-&gt;data;
 544   if (G_UNLIKELY (chunk))
 545     {
 546       /* allocating from freed list */
 547       (*magazine_chunks)-&gt;data = chunk-&gt;next;
 548     }
 549   else
 550     {
 551       chunk = *magazine_chunks;
 552       *magazine_chunks = chunk-&gt;next;
 553     }
 554   return chunk;
 555 }
 556 
 557 #if 0 /* useful for debugging */
 558 static guint
 559 magazine_count (ChunkLink *head)
 560 {
 561   guint count = 0;
 562   if (!head)
 563     return 0;
 564   while (head)
 565     {
 566       ChunkLink *child = head-&gt;data;
 567       count += 1;
 568       for (child = head-&gt;data; child; child = child-&gt;next)
 569         count += 1;
 570       head = head-&gt;next;
 571     }
 572   return count;
 573 }
 574 #endif
 575 
 576 static inline gsize
 577 allocator_get_magazine_threshold (Allocator *allocator,
 578                                   guint      ix)
 579 {
 580   /* the magazine size calculated here has a lower bound of MIN_MAGAZINE_SIZE,
 581    * which is required by the implementation. also, for moderately sized chunks
 582    * (say &gt;= 64 bytes), magazine sizes shouldn&#39;t be much smaller then the number
 583    * of chunks available per page/2 to avoid excessive traffic in the magazine
 584    * cache for small to medium sized structures.
 585    * the upper bound of the magazine size is effectively provided by
 586    * MAX_MAGAZINE_SIZE. for larger chunks, this number is scaled down so that
 587    * the content of a single magazine doesn&#39;t exceed ca. 16KB.
 588    */
 589   gsize chunk_size = SLAB_CHUNK_SIZE (allocator, ix);
 590   guint threshold = MAX (MIN_MAGAZINE_SIZE, allocator-&gt;max_page_size / MAX (5 * chunk_size, 5 * 32));
 591   guint contention_counter = allocator-&gt;contention_counters[ix];
 592   if (G_UNLIKELY (contention_counter))  /* single CPU bias */
 593     {
 594       /* adapt contention counter thresholds to chunk sizes */
 595       contention_counter = contention_counter * 64 / chunk_size;
 596       threshold = MAX (threshold, contention_counter);
 597     }
 598   return threshold;
 599 }
 600 
 601 /* --- magazine cache --- */
 602 static inline void
 603 magazine_cache_update_stamp (void)
 604 {
 605   if (allocator-&gt;stamp_counter &gt;= MAX_STAMP_COUNTER)
 606     {
<a name="5" id="anc5"></a><span class="line-modified"> 607       gint64 now_us = g_get_real_time ();</span>
<span class="line-modified"> 608       allocator-&gt;last_stamp = now_us / 1000; /* milli seconds */</span>

 609       allocator-&gt;stamp_counter = 0;
 610     }
 611   else
 612     allocator-&gt;stamp_counter++;
 613 }
 614 
 615 static inline ChunkLink*
 616 magazine_chain_prepare_fields (ChunkLink *magazine_chunks)
 617 {
 618   ChunkLink *chunk1;
 619   ChunkLink *chunk2;
 620   ChunkLink *chunk3;
 621   ChunkLink *chunk4;
 622   /* checked upon initialization: mem_assert (MIN_MAGAZINE_SIZE &gt;= 4); */
 623   /* ensure a magazine with at least 4 unused data pointers */
 624   chunk1 = magazine_chain_pop_head (&amp;magazine_chunks);
 625   chunk2 = magazine_chain_pop_head (&amp;magazine_chunks);
 626   chunk3 = magazine_chain_pop_head (&amp;magazine_chunks);
 627   chunk4 = magazine_chain_pop_head (&amp;magazine_chunks);
 628   chunk4-&gt;next = magazine_chunks;
 629   chunk3-&gt;next = chunk4;
 630   chunk2-&gt;next = chunk3;
 631   chunk1-&gt;next = chunk2;
 632   return chunk1;
 633 }
 634 
 635 /* access the first 3 fields of a specially prepared magazine chain */
 636 #define magazine_chain_prev(mc)         ((mc)-&gt;data)
 637 #define magazine_chain_stamp(mc)        ((mc)-&gt;next-&gt;data)
 638 #define magazine_chain_uint_stamp(mc)   GPOINTER_TO_UINT ((mc)-&gt;next-&gt;data)
 639 #define magazine_chain_next(mc)         ((mc)-&gt;next-&gt;next-&gt;data)
 640 #define magazine_chain_count(mc)        ((mc)-&gt;next-&gt;next-&gt;next-&gt;data)
 641 
 642 static void
 643 magazine_cache_trim (Allocator *allocator,
 644                      guint      ix,
 645                      guint      stamp)
 646 {
 647   /* g_mutex_lock (allocator-&gt;mutex); done by caller */
 648   /* trim magazine cache from tail */
 649   ChunkLink *current = magazine_chain_prev (allocator-&gt;magazines[ix]);
 650   ChunkLink *trash = NULL;
<a name="6" id="anc6"></a><span class="line-modified"> 651   while (!G_APPROX_VALUE(stamp, magazine_chain_uint_stamp (current),</span>
<span class="line-added"> 652                          allocator-&gt;config.working_set_msecs))</span>
 653     {
 654       /* unlink */
 655       ChunkLink *prev = magazine_chain_prev (current);
 656       ChunkLink *next = magazine_chain_next (current);
 657       magazine_chain_next (prev) = next;
 658       magazine_chain_prev (next) = prev;
 659       /* clear special fields, put on trash stack */
 660       magazine_chain_next (current) = NULL;
 661       magazine_chain_count (current) = NULL;
 662       magazine_chain_stamp (current) = NULL;
 663       magazine_chain_prev (current) = trash;
 664       trash = current;
 665       /* fixup list head if required */
 666       if (current == allocator-&gt;magazines[ix])
 667         {
 668           allocator-&gt;magazines[ix] = NULL;
 669           break;
 670         }
 671       current = prev;
 672     }
 673   g_mutex_unlock (&amp;allocator-&gt;magazine_mutex);
 674   /* free trash */
 675   if (trash)
 676     {
 677       const gsize chunk_size = SLAB_CHUNK_SIZE (allocator, ix);
 678       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
 679       while (trash)
 680         {
 681           current = trash;
 682           trash = magazine_chain_prev (current);
 683           magazine_chain_prev (current) = NULL; /* clear special field */
 684           while (current)
 685             {
 686               ChunkLink *chunk = magazine_chain_pop_head (&amp;current);
 687               slab_allocator_free_chunk (chunk_size, chunk);
 688             }
 689         }
 690       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
 691     }
 692 }
 693 
 694 static void
 695 magazine_cache_push_magazine (guint      ix,
 696                               ChunkLink *magazine_chunks,
 697                               gsize      count) /* must be &gt;= MIN_MAGAZINE_SIZE */
 698 {
 699   ChunkLink *current = magazine_chain_prepare_fields (magazine_chunks);
 700   ChunkLink *next, *prev;
 701   g_mutex_lock (&amp;allocator-&gt;magazine_mutex);
 702   /* add magazine at head */
 703   next = allocator-&gt;magazines[ix];
 704   if (next)
 705     prev = magazine_chain_prev (next);
 706   else
 707     next = prev = current;
 708   magazine_chain_next (prev) = current;
 709   magazine_chain_prev (next) = current;
 710   magazine_chain_prev (current) = prev;
 711   magazine_chain_next (current) = next;
 712   magazine_chain_count (current) = (gpointer) count;
 713   /* stamp magazine */
 714   magazine_cache_update_stamp();
 715   magazine_chain_stamp (current) = GUINT_TO_POINTER (allocator-&gt;last_stamp);
 716   allocator-&gt;magazines[ix] = current;
 717   /* free old magazines beyond a certain threshold */
 718   magazine_cache_trim (allocator, ix, allocator-&gt;last_stamp);
 719   /* g_mutex_unlock (allocator-&gt;mutex); was done by magazine_cache_trim() */
 720 }
 721 
 722 static ChunkLink*
 723 magazine_cache_pop_magazine (guint  ix,
 724                              gsize *countp)
 725 {
 726   g_mutex_lock_a (&amp;allocator-&gt;magazine_mutex, &amp;allocator-&gt;contention_counters[ix]);
 727   if (!allocator-&gt;magazines[ix])
 728     {
 729       guint magazine_threshold = allocator_get_magazine_threshold (allocator, ix);
 730       gsize i, chunk_size = SLAB_CHUNK_SIZE (allocator, ix);
 731       ChunkLink *chunk, *head;
 732       g_mutex_unlock (&amp;allocator-&gt;magazine_mutex);
 733       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
 734       head = slab_allocator_alloc_chunk (chunk_size);
 735       head-&gt;data = NULL;
 736       chunk = head;
 737       for (i = 1; i &lt; magazine_threshold; i++)
 738         {
 739           chunk-&gt;next = slab_allocator_alloc_chunk (chunk_size);
 740           chunk = chunk-&gt;next;
 741           chunk-&gt;data = NULL;
 742         }
 743       chunk-&gt;next = NULL;
 744       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
 745       *countp = i;
 746       return head;
 747     }
 748   else
 749     {
 750       ChunkLink *current = allocator-&gt;magazines[ix];
 751       ChunkLink *prev = magazine_chain_prev (current);
 752       ChunkLink *next = magazine_chain_next (current);
 753       /* unlink */
 754       magazine_chain_next (prev) = next;
 755       magazine_chain_prev (next) = prev;
 756       allocator-&gt;magazines[ix] = next == current ? NULL : next;
 757       g_mutex_unlock (&amp;allocator-&gt;magazine_mutex);
 758       /* clear special fields and hand out */
 759       *countp = (gsize) magazine_chain_count (current);
 760       magazine_chain_prev (current) = NULL;
 761       magazine_chain_next (current) = NULL;
 762       magazine_chain_count (current) = NULL;
 763       magazine_chain_stamp (current) = NULL;
 764       return current;
 765     }
 766 }
 767 
 768 /* --- thread magazines --- */
 769 static void
 770 private_thread_memory_cleanup (gpointer data)
 771 {
 772   ThreadMemory *tmem = data;
 773   const guint n_magazines = MAX_SLAB_INDEX (allocator);
 774   guint ix;
 775   for (ix = 0; ix &lt; n_magazines; ix++)
 776     {
 777       Magazine *mags[2];
 778       guint j;
 779       mags[0] = &amp;tmem-&gt;magazine1[ix];
 780       mags[1] = &amp;tmem-&gt;magazine2[ix];
 781       for (j = 0; j &lt; 2; j++)
 782         {
 783           Magazine *mag = mags[j];
 784           if (mag-&gt;count &gt;= MIN_MAGAZINE_SIZE)
 785             magazine_cache_push_magazine (ix, mag-&gt;chunks, mag-&gt;count);
 786           else
 787             {
 788               const gsize chunk_size = SLAB_CHUNK_SIZE (allocator, ix);
 789               g_mutex_lock (&amp;allocator-&gt;slab_mutex);
 790               while (mag-&gt;chunks)
 791                 {
 792                   ChunkLink *chunk = magazine_chain_pop_head (&amp;mag-&gt;chunks);
 793                   slab_allocator_free_chunk (chunk_size, chunk);
 794                 }
 795               g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
 796             }
 797         }
 798     }
 799   g_free (tmem);
 800 }
 801 
 802 static void
 803 thread_memory_magazine1_reload (ThreadMemory *tmem,
 804                                 guint         ix)
 805 {
 806   Magazine *mag = &amp;tmem-&gt;magazine1[ix];
 807   mem_assert (mag-&gt;chunks == NULL); /* ensure that we may reset mag-&gt;count */
 808   mag-&gt;count = 0;
 809   mag-&gt;chunks = magazine_cache_pop_magazine (ix, &amp;mag-&gt;count);
 810 }
 811 
 812 static void
 813 thread_memory_magazine2_unload (ThreadMemory *tmem,
 814                                 guint         ix)
 815 {
 816   Magazine *mag = &amp;tmem-&gt;magazine2[ix];
 817   magazine_cache_push_magazine (ix, mag-&gt;chunks, mag-&gt;count);
 818   mag-&gt;chunks = NULL;
 819   mag-&gt;count = 0;
 820 }
 821 
 822 static inline void
 823 thread_memory_swap_magazines (ThreadMemory *tmem,
 824                               guint         ix)
 825 {
 826   Magazine xmag = tmem-&gt;magazine1[ix];
 827   tmem-&gt;magazine1[ix] = tmem-&gt;magazine2[ix];
 828   tmem-&gt;magazine2[ix] = xmag;
 829 }
 830 
 831 static inline gboolean
 832 thread_memory_magazine1_is_empty (ThreadMemory *tmem,
 833                                   guint         ix)
 834 {
 835   return tmem-&gt;magazine1[ix].chunks == NULL;
 836 }
 837 
 838 static inline gboolean
 839 thread_memory_magazine2_is_full (ThreadMemory *tmem,
 840                                  guint         ix)
 841 {
 842   return tmem-&gt;magazine2[ix].count &gt;= allocator_get_magazine_threshold (allocator, ix);
 843 }
 844 
 845 static inline gpointer
 846 thread_memory_magazine1_alloc (ThreadMemory *tmem,
 847                                guint         ix)
 848 {
 849   Magazine *mag = &amp;tmem-&gt;magazine1[ix];
 850   ChunkLink *chunk = magazine_chain_pop_head (&amp;mag-&gt;chunks);
 851   if (G_LIKELY (mag-&gt;count &gt; 0))
 852     mag-&gt;count--;
 853   return chunk;
 854 }
 855 
 856 static inline void
 857 thread_memory_magazine2_free (ThreadMemory *tmem,
 858                               guint         ix,
 859                               gpointer      mem)
 860 {
 861   Magazine *mag = &amp;tmem-&gt;magazine2[ix];
 862   ChunkLink *chunk = mem;
 863   chunk-&gt;data = NULL;
 864   chunk-&gt;next = mag-&gt;chunks;
 865   mag-&gt;chunks = chunk;
 866   mag-&gt;count++;
 867 }
 868 
 869 /* --- API functions --- */
 870 
 871 /**
 872  * g_slice_new:
 873  * @type: the type to allocate, typically a structure name
 874  *
 875  * A convenience macro to allocate a block of memory from the
 876  * slice allocator.
 877  *
 878  * It calls g_slice_alloc() with `sizeof (@type)` and casts the
 879  * returned pointer to a pointer of the given type, avoiding a type
 880  * cast in the source code. Note that the underlying slice allocation
 881  * mechanism can be changed with the [`G_SLICE=always-malloc`][G_SLICE]
 882  * environment variable.
 883  *
 884  * This can never return %NULL as the minimum allocation size from
 885  * `sizeof (@type)` is 1 byte.
 886  *
 887  * Returns: (not nullable): a pointer to the allocated block, cast to a pointer
 888  *    to @type
 889  *
 890  * Since: 2.10
 891  */
 892 
 893 /**
 894  * g_slice_new0:
 895  * @type: the type to allocate, typically a structure name
 896  *
 897  * A convenience macro to allocate a block of memory from the
 898  * slice allocator and set the memory to 0.
 899  *
 900  * It calls g_slice_alloc0() with `sizeof (@type)`
 901  * and casts the returned pointer to a pointer of the given type,
 902  * avoiding a type cast in the source code.
 903  * Note that the underlying slice allocation mechanism can
 904  * be changed with the [`G_SLICE=always-malloc`][G_SLICE]
 905  * environment variable.
 906  *
 907  * This can never return %NULL as the minimum allocation size from
 908  * `sizeof (@type)` is 1 byte.
 909  *
 910  * Returns: (not nullable): a pointer to the allocated block, cast to a pointer
 911  *    to @type
 912  *
 913  * Since: 2.10
 914  */
 915 
 916 /**
 917  * g_slice_dup:
 918  * @type: the type to duplicate, typically a structure name
 919  * @mem: (not nullable): the memory to copy into the allocated block
 920  *
 921  * A convenience macro to duplicate a block of memory using
 922  * the slice allocator.
 923  *
 924  * It calls g_slice_copy() with `sizeof (@type)`
 925  * and casts the returned pointer to a pointer of the given type,
 926  * avoiding a type cast in the source code.
 927  * Note that the underlying slice allocation mechanism can
 928  * be changed with the [`G_SLICE=always-malloc`][G_SLICE]
 929  * environment variable.
 930  *
 931  * This can never return %NULL.
 932  *
 933  * Returns: (not nullable): a pointer to the allocated block, cast to a pointer
 934  *    to @type
 935  *
 936  * Since: 2.14
 937  */
 938 
 939 /**
 940  * g_slice_free:
 941  * @type: the type of the block to free, typically a structure name
 942  * @mem: a pointer to the block to free
 943  *
 944  * A convenience macro to free a block of memory that has
 945  * been allocated from the slice allocator.
 946  *
 947  * It calls g_slice_free1() using `sizeof (type)`
 948  * as the block size.
 949  * Note that the exact release behaviour can be changed with the
 950  * [`G_DEBUG=gc-friendly`][G_DEBUG] environment variable, also see
 951  * [`G_SLICE`][G_SLICE] for related debugging options.
 952  *
 953  * If @mem is %NULL, this macro does nothing.
 954  *
 955  * Since: 2.10
 956  */
 957 
 958 /**
 959  * g_slice_free_chain:
 960  * @type: the type of the @mem_chain blocks
 961  * @mem_chain: a pointer to the first block of the chain
 962  * @next: the field name of the next pointer in @type
 963  *
 964  * Frees a linked list of memory blocks of structure type @type.
 965  * The memory blocks must be equal-sized, allocated via
 966  * g_slice_alloc() or g_slice_alloc0() and linked together by
 967  * a @next pointer (similar to #GSList). The name of the
 968  * @next field in @type is passed as third argument.
 969  * Note that the exact release behaviour can be changed with the
 970  * [`G_DEBUG=gc-friendly`][G_DEBUG] environment variable, also see
 971  * [`G_SLICE`][G_SLICE] for related debugging options.
 972  *
 973  * If @mem_chain is %NULL, this function does nothing.
 974  *
 975  * Since: 2.10
 976  */
 977 
 978 /**
 979  * g_slice_alloc:
 980  * @block_size: the number of bytes to allocate
 981  *
 982  * Allocates a block of memory from the slice allocator.
 983  * The block address handed out can be expected to be aligned
 984  * to at least 1 * sizeof (void*),
 985  * though in general slices are 2 * sizeof (void*) bytes aligned,
 986  * if a malloc() fallback implementation is used instead,
 987  * the alignment may be reduced in a libc dependent fashion.
 988  * Note that the underlying slice allocation mechanism can
 989  * be changed with the [`G_SLICE=always-malloc`][G_SLICE]
 990  * environment variable.
 991  *
 992  * Returns: a pointer to the allocated memory block, which will be %NULL if and
 993  *    only if @mem_size is 0
 994  *
 995  * Since: 2.10
 996  */
 997 gpointer
 998 g_slice_alloc (gsize mem_size)
 999 {
1000   ThreadMemory *tmem;
1001   gsize chunk_size;
1002   gpointer mem;
1003   guint acat;
1004 
1005   /* This gets the private structure for this thread.  If the private
1006    * structure does not yet exist, it is created.
1007    *
1008    * This has a side effect of causing GSlice to be initialised, so it
1009    * must come first.
1010    */
1011   tmem = thread_memory_from_self ();
1012 #ifdef GSTREAMER_LITE
1013       if (tmem == NULL)
1014           return NULL;
1015 #endif // GSTREAMER_LITE
1016 
1017   chunk_size = P2ALIGN (mem_size);
1018   acat = allocator_categorize (chunk_size);
1019   if (G_LIKELY (acat == 1))     /* allocate through magazine layer */
1020     {
1021       guint ix = SLAB_INDEX (allocator, chunk_size);
1022       if (G_UNLIKELY (thread_memory_magazine1_is_empty (tmem, ix)))
1023         {
1024           thread_memory_swap_magazines (tmem, ix);
1025           if (G_UNLIKELY (thread_memory_magazine1_is_empty (tmem, ix)))
1026             thread_memory_magazine1_reload (tmem, ix);
1027         }
1028       mem = thread_memory_magazine1_alloc (tmem, ix);
1029     }
1030   else if (acat == 2)           /* allocate through slab allocator */
1031     {
1032       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
1033       mem = slab_allocator_alloc_chunk (chunk_size);
1034       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
1035     }
1036   else                          /* delegate to system malloc */
1037     mem = g_malloc (mem_size);
1038   if (G_UNLIKELY (allocator-&gt;config.debug_blocks))
1039     smc_notify_alloc (mem, mem_size);
1040 
1041   TRACE (GLIB_SLICE_ALLOC((void*)mem, mem_size));
1042 
1043   return mem;
1044 }
1045 
1046 /**
1047  * g_slice_alloc0:
1048  * @block_size: the number of bytes to allocate
1049  *
1050  * Allocates a block of memory via g_slice_alloc() and initializes
1051  * the returned memory to 0. Note that the underlying slice allocation
1052  * mechanism can be changed with the [`G_SLICE=always-malloc`][G_SLICE]
1053  * environment variable.
1054  *
1055  * Returns: a pointer to the allocated block, which will be %NULL if and only
1056  *    if @mem_size is 0
1057  *
1058  * Since: 2.10
1059  */
1060 gpointer
1061 g_slice_alloc0 (gsize mem_size)
1062 {
1063   gpointer mem = g_slice_alloc (mem_size);
1064   if (mem)
1065     memset (mem, 0, mem_size);
1066   return mem;
1067 }
1068 
1069 /**
1070  * g_slice_copy:
1071  * @block_size: the number of bytes to allocate
1072  * @mem_block: the memory to copy
1073  *
1074  * Allocates a block of memory from the slice allocator
1075  * and copies @block_size bytes into it from @mem_block.
1076  *
1077  * @mem_block must be non-%NULL if @block_size is non-zero.
1078  *
1079  * Returns: a pointer to the allocated memory block, which will be %NULL if and
1080  *    only if @mem_size is 0
1081  *
1082  * Since: 2.14
1083  */
1084 gpointer
1085 g_slice_copy (gsize         mem_size,
1086               gconstpointer mem_block)
1087 {
1088   gpointer mem = g_slice_alloc (mem_size);
1089   if (mem)
1090     memcpy (mem, mem_block, mem_size);
1091   return mem;
1092 }
1093 
1094 /**
1095  * g_slice_free1:
1096  * @block_size: the size of the block
1097  * @mem_block: a pointer to the block to free
1098  *
1099  * Frees a block of memory.
1100  *
1101  * The memory must have been allocated via g_slice_alloc() or
1102  * g_slice_alloc0() and the @block_size has to match the size
1103  * specified upon allocation. Note that the exact release behaviour
1104  * can be changed with the [`G_DEBUG=gc-friendly`][G_DEBUG] environment
1105  * variable, also see [`G_SLICE`][G_SLICE] for related debugging options.
1106  *
1107  * If @mem_block is %NULL, this function does nothing.
1108  *
1109  * Since: 2.10
1110  */
1111 void
1112 g_slice_free1 (gsize    mem_size,
1113                gpointer mem_block)
1114 {
1115   gsize chunk_size = P2ALIGN (mem_size);
1116   guint acat = allocator_categorize (chunk_size);
1117   if (G_UNLIKELY (!mem_block))
1118     return;
1119   if (G_UNLIKELY (allocator-&gt;config.debug_blocks) &amp;&amp;
1120       !smc_notify_free (mem_block, mem_size))
1121     abort();
1122   if (G_LIKELY (acat == 1))             /* allocate through magazine layer */
1123     {
1124       ThreadMemory *tmem = thread_memory_from_self();
1125       guint ix = SLAB_INDEX (allocator, chunk_size);
1126 #ifdef GSTREAMER_LITE
1127       if (tmem == NULL)
1128           return; // Nothing to free
1129 #endif // GSTREAMER_LITE
1130       if (G_UNLIKELY (thread_memory_magazine2_is_full (tmem, ix)))
1131         {
1132           thread_memory_swap_magazines (tmem, ix);
1133           if (G_UNLIKELY (thread_memory_magazine2_is_full (tmem, ix)))
1134             thread_memory_magazine2_unload (tmem, ix);
1135         }
1136       if (G_UNLIKELY (g_mem_gc_friendly))
1137         memset (mem_block, 0, chunk_size);
1138       thread_memory_magazine2_free (tmem, ix, mem_block);
1139     }
1140   else if (acat == 2)                   /* allocate through slab allocator */
1141     {
1142       if (G_UNLIKELY (g_mem_gc_friendly))
1143         memset (mem_block, 0, chunk_size);
1144       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
1145       slab_allocator_free_chunk (chunk_size, mem_block);
1146       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
1147     }
1148   else                                  /* delegate to system malloc */
1149     {
1150       if (G_UNLIKELY (g_mem_gc_friendly))
1151         memset (mem_block, 0, mem_size);
1152       g_free (mem_block);
1153     }
1154   TRACE (GLIB_SLICE_FREE((void*)mem_block, mem_size));
1155 }
1156 
1157 /**
1158  * g_slice_free_chain_with_offset:
1159  * @block_size: the size of the blocks
1160  * @mem_chain:  a pointer to the first block of the chain
1161  * @next_offset: the offset of the @next field in the blocks
1162  *
1163  * Frees a linked list of memory blocks of structure type @type.
1164  *
1165  * The memory blocks must be equal-sized, allocated via
1166  * g_slice_alloc() or g_slice_alloc0() and linked together by a
1167  * @next pointer (similar to #GSList). The offset of the @next
1168  * field in each block is passed as third argument.
1169  * Note that the exact release behaviour can be changed with the
1170  * [`G_DEBUG=gc-friendly`][G_DEBUG] environment variable, also see
1171  * [`G_SLICE`][G_SLICE] for related debugging options.
1172  *
1173  * If @mem_chain is %NULL, this function does nothing.
1174  *
1175  * Since: 2.10
1176  */
1177 void
1178 g_slice_free_chain_with_offset (gsize    mem_size,
1179                                 gpointer mem_chain,
1180                                 gsize    next_offset)
1181 {
1182   gpointer slice = mem_chain;
1183   /* while the thread magazines and the magazine cache are implemented so that
1184    * they can easily be extended to allow for free lists containing more free
1185    * lists for the first level nodes, which would allow O(1) freeing in this
1186    * function, the benefit of such an extension is questionable, because:
1187    * - the magazine size counts will become mere lower bounds which confuses
1188    *   the code adapting to lock contention;
1189    * - freeing a single node to the thread magazines is very fast, so this
1190    *   O(list_length) operation is multiplied by a fairly small factor;
1191    * - memory usage histograms on larger applications seem to indicate that
1192    *   the amount of released multi node lists is negligible in comparison
1193    *   to single node releases.
1194    * - the major performance bottle neck, namely g_private_get() or
1195    *   g_mutex_lock()/g_mutex_unlock() has already been moved out of the
1196    *   inner loop for freeing chained slices.
1197    */
1198   gsize chunk_size = P2ALIGN (mem_size);
1199   guint acat = allocator_categorize (chunk_size);
1200   if (G_LIKELY (acat == 1))             /* allocate through magazine layer */
1201     {
1202       ThreadMemory *tmem = thread_memory_from_self();
1203       guint ix = SLAB_INDEX (allocator, chunk_size);
1204 #ifdef GSTREAMER_LITE
1205       if (tmem == NULL)
1206           return; // Nothing to free
1207 #endif // GSTREAMER_LITE
1208       while (slice)
1209         {
1210           guint8 *current = slice;
1211           slice = *(gpointer*) (current + next_offset);
1212           if (G_UNLIKELY (allocator-&gt;config.debug_blocks) &amp;&amp;
1213               !smc_notify_free (current, mem_size))
1214             abort();
1215           if (G_UNLIKELY (thread_memory_magazine2_is_full (tmem, ix)))
1216             {
1217               thread_memory_swap_magazines (tmem, ix);
1218               if (G_UNLIKELY (thread_memory_magazine2_is_full (tmem, ix)))
1219                 thread_memory_magazine2_unload (tmem, ix);
1220             }
1221           if (G_UNLIKELY (g_mem_gc_friendly))
1222             memset (current, 0, chunk_size);
1223           thread_memory_magazine2_free (tmem, ix, current);
1224         }
1225     }
1226   else if (acat == 2)                   /* allocate through slab allocator */
1227     {
1228       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
1229       while (slice)
1230         {
1231           guint8 *current = slice;
1232           slice = *(gpointer*) (current + next_offset);
1233           if (G_UNLIKELY (allocator-&gt;config.debug_blocks) &amp;&amp;
1234               !smc_notify_free (current, mem_size))
1235             abort();
1236           if (G_UNLIKELY (g_mem_gc_friendly))
1237             memset (current, 0, chunk_size);
1238           slab_allocator_free_chunk (chunk_size, current);
1239         }
1240       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
1241     }
1242   else                                  /* delegate to system malloc */
1243     while (slice)
1244       {
1245         guint8 *current = slice;
1246         slice = *(gpointer*) (current + next_offset);
1247         if (G_UNLIKELY (allocator-&gt;config.debug_blocks) &amp;&amp;
1248             !smc_notify_free (current, mem_size))
1249           abort();
1250         if (G_UNLIKELY (g_mem_gc_friendly))
1251           memset (current, 0, mem_size);
1252         g_free (current);
1253       }
1254 }
1255 
1256 /* --- single page allocator --- */
1257 static void
1258 allocator_slab_stack_push (Allocator *allocator,
1259                            guint      ix,
1260                            SlabInfo  *sinfo)
1261 {
1262   /* insert slab at slab ring head */
1263   if (!allocator-&gt;slab_stack[ix])
1264     {
1265       sinfo-&gt;next = sinfo;
1266       sinfo-&gt;prev = sinfo;
1267     }
1268   else
1269     {
1270       SlabInfo *next = allocator-&gt;slab_stack[ix], *prev = next-&gt;prev;
1271       next-&gt;prev = sinfo;
1272       prev-&gt;next = sinfo;
1273       sinfo-&gt;next = next;
1274       sinfo-&gt;prev = prev;
1275     }
1276   allocator-&gt;slab_stack[ix] = sinfo;
1277 }
1278 
1279 static gsize
1280 allocator_aligned_page_size (Allocator *allocator,
1281                              gsize      n_bytes)
1282 {
1283   gsize val = 1 &lt;&lt; g_bit_storage (n_bytes - 1);
1284   val = MAX (val, allocator-&gt;min_page_size);
1285   return val;
1286 }
1287 
1288 static void
1289 allocator_add_slab (Allocator *allocator,
1290                     guint      ix,
1291                     gsize      chunk_size)
1292 {
1293   ChunkLink *chunk;
1294   SlabInfo *sinfo;
1295   gsize addr, padding, n_chunks, color = 0;
1296   gsize page_size;
1297   int errsv;
1298   gpointer aligned_memory;
1299   guint8 *mem;
1300   guint i;
1301 
1302   page_size = allocator_aligned_page_size (allocator, SLAB_BPAGE_SIZE (allocator, chunk_size));
1303   /* allocate 1 page for the chunks and the slab */
1304   aligned_memory = allocator_memalign (page_size, page_size - NATIVE_MALLOC_PADDING);
1305   errsv = errno;
1306   mem = aligned_memory;
1307 
1308   if (!mem)
1309     {
1310       const gchar *syserr = strerror (errsv);
1311       mem_error (&quot;failed to allocate %u bytes (alignment: %u): %s\n&quot;,
1312                  (guint) (page_size - NATIVE_MALLOC_PADDING), (guint) page_size, syserr);
1313     }
1314   /* mask page address */
1315   addr = ((gsize) mem / page_size) * page_size;
1316   /* assert alignment */
1317   mem_assert (aligned_memory == (gpointer) addr);
1318   /* basic slab info setup */
1319   sinfo = (SlabInfo*) (mem + page_size - SLAB_INFO_SIZE);
1320   sinfo-&gt;n_allocated = 0;
1321   sinfo-&gt;chunks = NULL;
1322   /* figure cache colorization */
1323   n_chunks = ((guint8*) sinfo - mem) / chunk_size;
1324   padding = ((guint8*) sinfo - mem) - n_chunks * chunk_size;
1325   if (padding)
1326     {
1327       color = (allocator-&gt;color_accu * P2ALIGNMENT) % padding;
1328       allocator-&gt;color_accu += allocator-&gt;config.color_increment;
1329     }
1330   /* add chunks to free list */
1331   chunk = (ChunkLink*) (mem + color);
1332   sinfo-&gt;chunks = chunk;
1333   for (i = 0; i &lt; n_chunks - 1; i++)
1334     {
1335       chunk-&gt;next = (ChunkLink*) ((guint8*) chunk + chunk_size);
1336       chunk = chunk-&gt;next;
1337     }
1338   chunk-&gt;next = NULL;   /* last chunk */
1339   /* add slab to slab ring */
1340   allocator_slab_stack_push (allocator, ix, sinfo);
1341 }
1342 
1343 static gpointer
1344 slab_allocator_alloc_chunk (gsize chunk_size)
1345 {
1346   ChunkLink *chunk;
1347   guint ix = SLAB_INDEX (allocator, chunk_size);
1348   /* ensure non-empty slab */
1349   if (!allocator-&gt;slab_stack[ix] || !allocator-&gt;slab_stack[ix]-&gt;chunks)
1350     allocator_add_slab (allocator, ix, chunk_size);
1351   /* allocate chunk */
1352   chunk = allocator-&gt;slab_stack[ix]-&gt;chunks;
1353   allocator-&gt;slab_stack[ix]-&gt;chunks = chunk-&gt;next;
1354   allocator-&gt;slab_stack[ix]-&gt;n_allocated++;
1355   /* rotate empty slabs */
1356   if (!allocator-&gt;slab_stack[ix]-&gt;chunks)
1357     allocator-&gt;slab_stack[ix] = allocator-&gt;slab_stack[ix]-&gt;next;
1358   return chunk;
1359 }
1360 
1361 static void
1362 slab_allocator_free_chunk (gsize    chunk_size,
1363                            gpointer mem)
1364 {
1365   ChunkLink *chunk;
1366   gboolean was_empty;
1367   guint ix = SLAB_INDEX (allocator, chunk_size);
1368   gsize page_size = allocator_aligned_page_size (allocator, SLAB_BPAGE_SIZE (allocator, chunk_size));
1369   gsize addr = ((gsize) mem / page_size) * page_size;
1370   /* mask page address */
1371   guint8 *page = (guint8*) addr;
1372   SlabInfo *sinfo = (SlabInfo*) (page + page_size - SLAB_INFO_SIZE);
1373   /* assert valid chunk count */
1374   mem_assert (sinfo-&gt;n_allocated &gt; 0);
1375   /* add chunk to free list */
1376   was_empty = sinfo-&gt;chunks == NULL;
1377   chunk = (ChunkLink*) mem;
1378   chunk-&gt;next = sinfo-&gt;chunks;
1379   sinfo-&gt;chunks = chunk;
1380   sinfo-&gt;n_allocated--;
1381   /* keep slab ring partially sorted, empty slabs at end */
1382   if (was_empty)
1383     {
1384       /* unlink slab */
1385       SlabInfo *next = sinfo-&gt;next, *prev = sinfo-&gt;prev;
1386       next-&gt;prev = prev;
1387       prev-&gt;next = next;
1388       if (allocator-&gt;slab_stack[ix] == sinfo)
1389         allocator-&gt;slab_stack[ix] = next == sinfo ? NULL : next;
1390       /* insert slab at head */
1391       allocator_slab_stack_push (allocator, ix, sinfo);
1392     }
1393   /* eagerly free complete unused slabs */
1394   if (!sinfo-&gt;n_allocated)
1395     {
1396       /* unlink slab */
1397       SlabInfo *next = sinfo-&gt;next, *prev = sinfo-&gt;prev;
1398       next-&gt;prev = prev;
1399       prev-&gt;next = next;
1400       if (allocator-&gt;slab_stack[ix] == sinfo)
1401         allocator-&gt;slab_stack[ix] = next == sinfo ? NULL : next;
1402       /* free slab */
1403       allocator_memfree (page_size, page);
1404     }
1405 }
1406 
1407 /* --- memalign implementation --- */
1408 #ifdef HAVE_MALLOC_H
1409 #include &lt;malloc.h&gt;             /* memalign() */
1410 #endif
1411 
1412 /* from config.h:
1413  * define HAVE_POSIX_MEMALIGN           1 // if free(posix_memalign(3)) works, &lt;stdlib.h&gt;
1414  * define HAVE_MEMALIGN                 1 // if free(memalign(3)) works, &lt;malloc.h&gt;
1415  * define HAVE_VALLOC                   1 // if free(valloc(3)) works, &lt;stdlib.h&gt; or &lt;malloc.h&gt;
1416  * if none is provided, we implement malloc(3)-based alloc-only page alignment
1417  */
1418 
1419 #if !(HAVE_POSIX_MEMALIGN || HAVE_MEMALIGN || HAVE_VALLOC)
<a name="7" id="anc7"></a><span class="line-added">1420 G_GNUC_BEGIN_IGNORE_DEPRECATIONS</span>
1421 static GTrashStack *compat_valloc_trash = NULL;
<a name="8" id="anc8"></a><span class="line-added">1422 G_GNUC_END_IGNORE_DEPRECATIONS</span>
1423 #endif
1424 
1425 static gpointer
1426 allocator_memalign (gsize alignment,
1427                     gsize memsize)
1428 {
1429   gpointer aligned_memory = NULL;
1430   gint err = ENOMEM;
1431 #if     HAVE_POSIX_MEMALIGN
1432   err = posix_memalign (&amp;aligned_memory, alignment, memsize);
1433 #elif   HAVE_MEMALIGN
1434   errno = 0;
1435   aligned_memory = memalign (alignment, memsize);
1436   err = errno;
1437 #elif   HAVE_VALLOC
1438   errno = 0;
1439   aligned_memory = valloc (memsize);
1440   err = errno;
1441 #else
1442   /* simplistic non-freeing page allocator */
1443   mem_assert (alignment == sys_page_size);
1444   mem_assert (memsize &lt;= sys_page_size);
1445   if (!compat_valloc_trash)
1446     {
1447       const guint n_pages = 16;
1448       guint8 *mem = malloc (n_pages * sys_page_size);
1449       err = errno;
1450       if (mem)
1451         {
1452           gint i = n_pages;
1453           guint8 *amem = (guint8*) ALIGN ((gsize) mem, sys_page_size);
1454           if (amem != mem)
1455             i--;        /* mem wasn&#39;t page aligned */
1456           G_GNUC_BEGIN_IGNORE_DEPRECATIONS
1457           while (--i &gt;= 0)
1458             g_trash_stack_push (&amp;compat_valloc_trash, amem + i * sys_page_size);
1459           G_GNUC_END_IGNORE_DEPRECATIONS
1460         }
1461     }
1462   G_GNUC_BEGIN_IGNORE_DEPRECATIONS
1463   aligned_memory = g_trash_stack_pop (&amp;compat_valloc_trash);
1464   G_GNUC_END_IGNORE_DEPRECATIONS
1465 #endif
1466   if (!aligned_memory)
1467     errno = err;
1468   return aligned_memory;
1469 }
1470 
1471 static void
1472 allocator_memfree (gsize    memsize,
1473                    gpointer mem)
1474 {
1475 #if     HAVE_POSIX_MEMALIGN || HAVE_MEMALIGN || HAVE_VALLOC
1476   free (mem);
1477 #else
1478   mem_assert (memsize &lt;= sys_page_size);
1479   G_GNUC_BEGIN_IGNORE_DEPRECATIONS
1480   g_trash_stack_push (&amp;compat_valloc_trash, mem);
1481   G_GNUC_END_IGNORE_DEPRECATIONS
1482 #endif
1483 }
1484 
1485 static void
1486 mem_error (const char *format,
1487            ...)
1488 {
1489   const char *pname;
1490   va_list args;
1491   /* at least, put out &quot;MEMORY-ERROR&quot;, in case we segfault during the rest of the function */
1492   fputs (&quot;\n***MEMORY-ERROR***: &quot;, stderr);
1493   pname = g_get_prgname();
1494   g_fprintf (stderr, &quot;%s[%ld]: GSlice: &quot;, pname ? pname : &quot;&quot;, (long)getpid());
1495   va_start (args, format);
1496   g_vfprintf (stderr, format, args);
1497   va_end (args);
1498   fputs (&quot;\n&quot;, stderr);
1499   abort();
1500   _exit (1);
1501 }
1502 
1503 /* --- g-slice memory checker tree --- */
1504 typedef size_t SmcKType;                /* key type */
1505 typedef size_t SmcVType;                /* value type */
1506 typedef struct {
1507   SmcKType key;
1508   SmcVType value;
1509 } SmcEntry;
1510 static void             smc_tree_insert      (SmcKType  key,
1511                                               SmcVType  value);
1512 static gboolean         smc_tree_lookup      (SmcKType  key,
1513                                               SmcVType *value_p);
1514 static gboolean         smc_tree_remove      (SmcKType  key);
1515 
1516 
1517 /* --- g-slice memory checker implementation --- */
1518 static void
1519 smc_notify_alloc (void   *pointer,
1520                   size_t  size)
1521 {
1522   size_t address = (size_t) pointer;
1523   if (pointer)
1524     smc_tree_insert (address, size);
1525 }
1526 
1527 #if 0
1528 static void
1529 smc_notify_ignore (void *pointer)
1530 {
1531   size_t address = (size_t) pointer;
1532   if (pointer)
1533     smc_tree_remove (address);
1534 }
1535 #endif
1536 
1537 static int
1538 smc_notify_free (void   *pointer,
1539                  size_t  size)
1540 {
1541   size_t address = (size_t) pointer;
1542   SmcVType real_size;
1543   gboolean found_one;
1544 
1545   if (!pointer)
1546     return 1; /* ignore */
1547   found_one = smc_tree_lookup (address, &amp;real_size);
1548   if (!found_one)
1549     {
1550       g_fprintf (stderr, &quot;GSlice: MemChecker: attempt to release non-allocated block: %p size=%&quot; G_GSIZE_FORMAT &quot;\n&quot;, pointer, size);
1551       return 0;
1552     }
1553   if (real_size != size &amp;&amp; (real_size || size))
1554     {
1555       g_fprintf (stderr, &quot;GSlice: MemChecker: attempt to release block with invalid size: %p size=%&quot; G_GSIZE_FORMAT &quot; invalid-size=%&quot; G_GSIZE_FORMAT &quot;\n&quot;, pointer, real_size, size);
1556       return 0;
1557     }
1558   if (!smc_tree_remove (address))
1559     {
1560       g_fprintf (stderr, &quot;GSlice: MemChecker: attempt to release non-allocated block: %p size=%&quot; G_GSIZE_FORMAT &quot;\n&quot;, pointer, size);
1561       return 0;
1562     }
1563   return 1; /* all fine */
1564 }
1565 
1566 /* --- g-slice memory checker tree implementation --- */
1567 #define SMC_TRUNK_COUNT     (4093 /* 16381 */)          /* prime, to distribute trunk collisions (big, allocated just once) */
1568 #define SMC_BRANCH_COUNT    (511)                       /* prime, to distribute branch collisions */
1569 #define SMC_TRUNK_EXTENT    (SMC_BRANCH_COUNT * 2039)   /* key address space per trunk, should distribute uniformly across BRANCH_COUNT */
1570 #define SMC_TRUNK_HASH(k)   ((k / SMC_TRUNK_EXTENT) % SMC_TRUNK_COUNT)  /* generate new trunk hash per megabyte (roughly) */
1571 #define SMC_BRANCH_HASH(k)  (k % SMC_BRANCH_COUNT)
1572 
1573 typedef struct {
1574   SmcEntry    *entries;
1575   unsigned int n_entries;
1576 } SmcBranch;
1577 
1578 static SmcBranch     **smc_tree_root = NULL;
1579 
1580 static void
1581 smc_tree_abort (int errval)
1582 {
1583   const char *syserr = strerror (errval);
1584   mem_error (&quot;MemChecker: failure in debugging tree: %s&quot;, syserr);
1585 }
1586 
1587 static inline SmcEntry*
1588 smc_tree_branch_grow_L (SmcBranch   *branch,
1589                         unsigned int index)
1590 {
1591   unsigned int old_size = branch-&gt;n_entries * sizeof (branch-&gt;entries[0]);
1592   unsigned int new_size = old_size + sizeof (branch-&gt;entries[0]);
1593   SmcEntry *entry;
1594   mem_assert (index &lt;= branch-&gt;n_entries);
1595   branch-&gt;entries = (SmcEntry*) realloc (branch-&gt;entries, new_size);
1596   if (!branch-&gt;entries)
1597     smc_tree_abort (errno);
1598   entry = branch-&gt;entries + index;
1599   memmove (entry + 1, entry, (branch-&gt;n_entries - index) * sizeof (entry[0]));
1600   branch-&gt;n_entries += 1;
1601   return entry;
1602 }
1603 
1604 static inline SmcEntry*
1605 smc_tree_branch_lookup_nearest_L (SmcBranch *branch,
1606                                   SmcKType   key)
1607 {
1608   unsigned int n_nodes = branch-&gt;n_entries, offs = 0;
1609   SmcEntry *check = branch-&gt;entries;
1610   int cmp = 0;
1611   while (offs &lt; n_nodes)
1612     {
1613       unsigned int i = (offs + n_nodes) &gt;&gt; 1;
1614       check = branch-&gt;entries + i;
1615       cmp = key &lt; check-&gt;key ? -1 : key != check-&gt;key;
1616       if (cmp == 0)
1617         return check;                   /* return exact match */
1618       else if (cmp &lt; 0)
1619         n_nodes = i;
1620       else /* (cmp &gt; 0) */
1621         offs = i + 1;
1622     }
1623   /* check points at last mismatch, cmp &gt; 0 indicates greater key */
1624   return cmp &gt; 0 ? check + 1 : check;   /* return insertion position for inexact match */
1625 }
1626 
1627 static void
1628 smc_tree_insert (SmcKType key,
1629                  SmcVType value)
1630 {
1631   unsigned int ix0, ix1;
1632   SmcEntry *entry;
1633 
1634   g_mutex_lock (&amp;smc_tree_mutex);
1635   ix0 = SMC_TRUNK_HASH (key);
1636   ix1 = SMC_BRANCH_HASH (key);
1637   if (!smc_tree_root)
1638     {
1639       smc_tree_root = calloc (SMC_TRUNK_COUNT, sizeof (smc_tree_root[0]));
1640       if (!smc_tree_root)
1641         smc_tree_abort (errno);
1642     }
1643   if (!smc_tree_root[ix0])
1644     {
1645       smc_tree_root[ix0] = calloc (SMC_BRANCH_COUNT, sizeof (smc_tree_root[0][0]));
1646       if (!smc_tree_root[ix0])
1647         smc_tree_abort (errno);
1648     }
1649   entry = smc_tree_branch_lookup_nearest_L (&amp;smc_tree_root[ix0][ix1], key);
1650   if (!entry ||                                                                         /* need create */
1651       entry &gt;= smc_tree_root[ix0][ix1].entries + smc_tree_root[ix0][ix1].n_entries ||   /* need append */
1652       entry-&gt;key != key)                                                                /* need insert */
1653     entry = smc_tree_branch_grow_L (&amp;smc_tree_root[ix0][ix1], entry - smc_tree_root[ix0][ix1].entries);
1654   entry-&gt;key = key;
1655   entry-&gt;value = value;
1656   g_mutex_unlock (&amp;smc_tree_mutex);
1657 }
1658 
1659 static gboolean
1660 smc_tree_lookup (SmcKType  key,
1661                  SmcVType *value_p)
1662 {
1663   SmcEntry *entry = NULL;
1664   unsigned int ix0 = SMC_TRUNK_HASH (key), ix1 = SMC_BRANCH_HASH (key);
1665   gboolean found_one = FALSE;
1666   *value_p = 0;
1667   g_mutex_lock (&amp;smc_tree_mutex);
1668   if (smc_tree_root &amp;&amp; smc_tree_root[ix0])
1669     {
1670       entry = smc_tree_branch_lookup_nearest_L (&amp;smc_tree_root[ix0][ix1], key);
1671       if (entry &amp;&amp;
1672           entry &lt; smc_tree_root[ix0][ix1].entries + smc_tree_root[ix0][ix1].n_entries &amp;&amp;
1673           entry-&gt;key == key)
1674         {
1675           found_one = TRUE;
1676           *value_p = entry-&gt;value;
1677         }
1678     }
1679   g_mutex_unlock (&amp;smc_tree_mutex);
1680   return found_one;
1681 }
1682 
1683 static gboolean
1684 smc_tree_remove (SmcKType key)
1685 {
1686   unsigned int ix0 = SMC_TRUNK_HASH (key), ix1 = SMC_BRANCH_HASH (key);
1687   gboolean found_one = FALSE;
1688   g_mutex_lock (&amp;smc_tree_mutex);
1689   if (smc_tree_root &amp;&amp; smc_tree_root[ix0])
1690     {
1691       SmcEntry *entry = smc_tree_branch_lookup_nearest_L (&amp;smc_tree_root[ix0][ix1], key);
1692       if (entry &amp;&amp;
1693           entry &lt; smc_tree_root[ix0][ix1].entries + smc_tree_root[ix0][ix1].n_entries &amp;&amp;
1694           entry-&gt;key == key)
1695         {
1696           unsigned int i = entry - smc_tree_root[ix0][ix1].entries;
1697           smc_tree_root[ix0][ix1].n_entries -= 1;
1698           memmove (entry, entry + 1, (smc_tree_root[ix0][ix1].n_entries - i) * sizeof (entry[0]));
1699           if (!smc_tree_root[ix0][ix1].n_entries)
1700             {
1701               /* avoid useless pressure on the memory system */
1702               free (smc_tree_root[ix0][ix1].entries);
1703               smc_tree_root[ix0][ix1].entries = NULL;
1704             }
1705           found_one = TRUE;
1706         }
1707     }
1708   g_mutex_unlock (&amp;smc_tree_mutex);
1709   return found_one;
1710 }
1711 
1712 #ifdef G_ENABLE_DEBUG
1713 void
1714 g_slice_debug_tree_statistics (void)
1715 {
1716   g_mutex_lock (&amp;smc_tree_mutex);
1717   if (smc_tree_root)
1718     {
1719       unsigned int i, j, t = 0, o = 0, b = 0, su = 0, ex = 0, en = 4294967295u;
1720       double tf, bf;
1721       for (i = 0; i &lt; SMC_TRUNK_COUNT; i++)
1722         if (smc_tree_root[i])
1723           {
1724             t++;
1725             for (j = 0; j &lt; SMC_BRANCH_COUNT; j++)
1726               if (smc_tree_root[i][j].n_entries)
1727                 {
1728                   b++;
1729                   su += smc_tree_root[i][j].n_entries;
1730                   en = MIN (en, smc_tree_root[i][j].n_entries);
1731                   ex = MAX (ex, smc_tree_root[i][j].n_entries);
1732                 }
1733               else if (smc_tree_root[i][j].entries)
1734                 o++; /* formerly used, now empty */
1735           }
1736       en = b ? en : 0;
1737       tf = MAX (t, 1.0); /* max(1) to be a valid divisor */
1738       bf = MAX (b, 1.0); /* max(1) to be a valid divisor */
1739       g_fprintf (stderr, &quot;GSlice: MemChecker: %u trunks, %u branches, %u old branches\n&quot;, t, b, o);
1740       g_fprintf (stderr, &quot;GSlice: MemChecker: %f branches per trunk, %.2f%% utilization\n&quot;,
1741                b / tf,
1742                100.0 - (SMC_BRANCH_COUNT - b / tf) / (0.01 * SMC_BRANCH_COUNT));
1743       g_fprintf (stderr, &quot;GSlice: MemChecker: %f entries per branch, %u minimum, %u maximum\n&quot;,
1744                su / bf, en, ex);
1745     }
1746   else
1747     g_fprintf (stderr, &quot;GSlice: MemChecker: root=NULL\n&quot;);
1748   g_mutex_unlock (&amp;smc_tree_mutex);
1749 
1750   /* sample statistics (beast + GSLice + 24h scripted core &amp; GUI activity):
1751    *  PID %CPU %MEM   VSZ  RSS      COMMAND
1752    * 8887 30.3 45.8 456068 414856   beast-0.7.1 empty.bse
1753    * $ cat /proc/8887/statm # total-program-size resident-set-size shared-pages text/code data/stack library dirty-pages
1754    * 114017 103714 2354 344 0 108676 0
1755    * $ cat /proc/8887/status
1756    * Name:   beast-0.7.1
1757    * VmSize:   456068 kB
1758    * VmLck:         0 kB
1759    * VmRSS:    414856 kB
1760    * VmData:   434620 kB
1761    * VmStk:        84 kB
1762    * VmExe:      1376 kB
1763    * VmLib:     13036 kB
1764    * VmPTE:       456 kB
1765    * Threads:        3
1766    * (gdb) print g_slice_debug_tree_statistics ()
1767    * GSlice: MemChecker: 422 trunks, 213068 branches, 0 old branches
1768    * GSlice: MemChecker: 504.900474 branches per trunk, 98.81% utilization
1769    * GSlice: MemChecker: 4.965039 entries per branch, 1 minimum, 37 maximum
1770    */
1771 }
1772 #endif /* G_ENABLE_DEBUG */
<a name="9" id="anc9"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="9" type="hidden" />
</body>
</html>