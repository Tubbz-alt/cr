<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.media/src/main/native/gstreamer/3rd_party/glib/glib/gslice.c</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /* GLIB sliced memory - fast concurrent memory chunk allocator
   2  * Copyright (C) 2005 Tim Janik
   3  *
   4  * This library is free software; you can redistribute it and/or
   5  * modify it under the terms of the GNU Lesser General Public
   6  * License as published by the Free Software Foundation; either
   7  * version 2.1 of the License, or (at your option) any later version.
   8  *
   9  * This library is distributed in the hope that it will be useful,
  10  * but WITHOUT ANY WARRANTY; without even the implied warranty of
  11  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
  12  * Lesser General Public License for more details.
  13  *
  14  * You should have received a copy of the GNU Lesser General Public
  15  * License along with this library; if not, see &lt;http://www.gnu.org/licenses/&gt;.
  16  */
  17 /* MT safe */
  18 
  19 #include &quot;config.h&quot;
  20 #include &quot;glibconfig.h&quot;
  21 
  22 #if defined(HAVE_POSIX_MEMALIGN) &amp;&amp; !defined(_XOPEN_SOURCE)
  23 #define _XOPEN_SOURCE 600       /* posix_memalign() */
  24 #endif
  25 #include &lt;stdlib.h&gt;             /* posix_memalign() */
  26 #include &lt;string.h&gt;
  27 #include &lt;errno.h&gt;
  28 
  29 #ifdef G_OS_UNIX
  30 #include &lt;unistd.h&gt;             /* sysconf() */
  31 #endif
  32 #ifdef G_OS_WIN32
  33 #include &lt;windows.h&gt;
  34 #include &lt;process.h&gt;
  35 #endif
  36 
  37 #include &lt;stdio.h&gt;              /* fputs */
  38 
  39 #include &quot;gslice.h&quot;
  40 
  41 #include &quot;gmain.h&quot;
  42 #include &quot;gmem.h&quot;               /* gslice.h */
  43 #include &quot;gstrfuncs.h&quot;
  44 #include &quot;gutils.h&quot;
  45 #include &quot;gtrashstack.h&quot;
  46 #include &quot;gtestutils.h&quot;
  47 #include &quot;gthread.h&quot;
<a name="1" id="anc1"></a>
  48 #include &quot;glib_trace.h&quot;
  49 #include &quot;gprintf.h&quot;
  50 
  51 #include &quot;gvalgrind.h&quot;
  52 
  53 /**
  54  * SECTION:memory_slices
  55  * @title: Memory Slices
  56  * @short_description: efficient way to allocate groups of equal-sized
  57  *     chunks of memory
  58  *
  59  * Memory slices provide a space-efficient and multi-processing scalable
  60  * way to allocate equal-sized pieces of memory, just like the original
  61  * #GMemChunks (from GLib 2.8), while avoiding their excessive
  62  * memory-waste, scalability and performance problems.
  63  *
  64  * To achieve these goals, the slice allocator uses a sophisticated,
  65  * layered design that has been inspired by Bonwick&#39;s slab allocator
  66  * ([Bonwick94](http://citeseer.ist.psu.edu/bonwick94slab.html)
  67  * Jeff Bonwick, The slab allocator: An object-caching kernel
  68  * memory allocator. USENIX 1994, and
  69  * [Bonwick01](http://citeseer.ist.psu.edu/bonwick01magazines.html)
  70  * Bonwick and Jonathan Adams, Magazines and vmem: Extending the
  71  * slab allocator to many cpu&#39;s and arbitrary resources. USENIX 2001)
  72  *
  73  * It uses posix_memalign() to optimize allocations of many equally-sized
  74  * chunks, and has per-thread free lists (the so-called magazine layer)
  75  * to quickly satisfy allocation requests of already known structure sizes.
  76  * This is accompanied by extra caching logic to keep freed memory around
  77  * for some time before returning it to the system. Memory that is unused
  78  * due to alignment constraints is used for cache colorization (random
  79  * distribution of chunk addresses) to improve CPU cache utilization. The
  80  * caching layer of the slice allocator adapts itself to high lock contention
  81  * to improve scalability.
  82  *
  83  * The slice allocator can allocate blocks as small as two pointers, and
  84  * unlike malloc(), it does not reserve extra space per block. For large block
  85  * sizes, g_slice_new() and g_slice_alloc() will automatically delegate to the
  86  * system malloc() implementation. For newly written code it is recommended
  87  * to use the new `g_slice` API instead of g_malloc() and
  88  * friends, as long as objects are not resized during their lifetime and the
  89  * object size used at allocation time is still available when freeing.
  90  *
  91  * Here is an example for using the slice allocator:
  92  * |[&lt;!-- language=&quot;C&quot; --&gt;
  93  * gchar *mem[10000];
  94  * gint i;
  95  *
  96  * // Allocate 10000 blocks.
  97  * for (i = 0; i &lt; 10000; i++)
  98  *   {
  99  *     mem[i] = g_slice_alloc (50);
 100  *
 101  *     // Fill in the memory with some junk.
 102  *     for (j = 0; j &lt; 50; j++)
 103  *       mem[i][j] = i * j;
 104  *   }
 105  *
 106  * // Now free all of the blocks.
 107  * for (i = 0; i &lt; 10000; i++)
 108  *   g_slice_free1 (50, mem[i]);
 109  * ]|
 110  *
 111  * And here is an example for using the using the slice allocator
 112  * with data structures:
 113  * |[&lt;!-- language=&quot;C&quot; --&gt;
 114  * GRealArray *array;
 115  *
 116  * // Allocate one block, using the g_slice_new() macro.
 117  * array = g_slice_new (GRealArray);
 118  *
 119  * // We can now use array just like a normal pointer to a structure.
 120  * array-&gt;data            = NULL;
 121  * array-&gt;len             = 0;
 122  * array-&gt;alloc           = 0;
 123  * array-&gt;zero_terminated = (zero_terminated ? 1 : 0);
 124  * array-&gt;clear           = (clear ? 1 : 0);
 125  * array-&gt;elt_size        = elt_size;
 126  *
 127  * // We can free the block, so it can be reused.
 128  * g_slice_free (GRealArray, array);
 129  * ]|
 130  */
 131 
 132 /* the GSlice allocator is split up into 4 layers, roughly modelled after the slab
 133  * allocator and magazine extensions as outlined in:
 134  * + [Bonwick94] Jeff Bonwick, The slab allocator: An object-caching kernel
 135  *   memory allocator. USENIX 1994, http://citeseer.ist.psu.edu/bonwick94slab.html
 136  * + [Bonwick01] Bonwick and Jonathan Adams, Magazines and vmem: Extending the
 137  *   slab allocator to many cpu&#39;s and arbitrary resources.
 138  *   USENIX 2001, http://citeseer.ist.psu.edu/bonwick01magazines.html
 139  * the layers are:
 140  * - the thread magazines. for each (aligned) chunk size, a magazine (a list)
 141  *   of recently freed and soon to be allocated chunks is maintained per thread.
 142  *   this way, most alloc/free requests can be quickly satisfied from per-thread
 143  *   free lists which only require one g_private_get() call to retrive the
 144  *   thread handle.
 145  * - the magazine cache. allocating and freeing chunks to/from threads only
 146  *   occours at magazine sizes from a global depot of magazines. the depot
 147  *   maintaines a 15 second working set of allocated magazines, so full
 148  *   magazines are not allocated and released too often.
 149  *   the chunk size dependent magazine sizes automatically adapt (within limits,
 150  *   see [3]) to lock contention to properly scale performance across a variety
 151  *   of SMP systems.
 152  * - the slab allocator. this allocator allocates slabs (blocks of memory) close
 153  *   to the system page size or multiples thereof which have to be page aligned.
 154  *   the blocks are divided into smaller chunks which are used to satisfy
 155  *   allocations from the upper layers. the space provided by the reminder of
 156  *   the chunk size division is used for cache colorization (random distribution
 157  *   of chunk addresses) to improve processor cache utilization. multiple slabs
 158  *   with the same chunk size are kept in a partially sorted ring to allow O(1)
 159  *   freeing and allocation of chunks (as long as the allocation of an entirely
 160  *   new slab can be avoided).
 161  * - the page allocator. on most modern systems, posix_memalign(3) or
 162  *   memalign(3) should be available, so this is used to allocate blocks with
 163  *   system page size based alignments and sizes or multiples thereof.
 164  *   if no memalign variant is provided, valloc() is used instead and
 165  *   block sizes are limited to the system page size (no multiples thereof).
 166  *   as a fallback, on system without even valloc(), a malloc(3)-based page
 167  *   allocator with alloc-only behaviour is used.
 168  *
 169  * NOTES:
 170  * [1] some systems memalign(3) implementations may rely on boundary tagging for
 171  *     the handed out memory chunks. to avoid excessive page-wise fragmentation,
 172  *     we reserve 2 * sizeof (void*) per block size for the systems memalign(3),
 173  *     specified in NATIVE_MALLOC_PADDING.
 174  * [2] using the slab allocator alone already provides for a fast and efficient
 175  *     allocator, it doesn&#39;t properly scale beyond single-threaded uses though.
 176  *     also, the slab allocator implements eager free(3)-ing, i.e. does not
 177  *     provide any form of caching or working set maintenance. so if used alone,
 178  *     it&#39;s vulnerable to trashing for sequences of balanced (alloc, free) pairs
 179  *     at certain thresholds.
 180  * [3] magazine sizes are bound by an implementation specific minimum size and
 181  *     a chunk size specific maximum to limit magazine storage sizes to roughly
 182  *     16KB.
 183  * [4] allocating ca. 8 chunks per block/page keeps a good balance between
 184  *     external and internal fragmentation (&lt;= 12.5%). [Bonwick94]
 185  */
 186 
 187 /* --- macros and constants --- */
 188 #define LARGEALIGNMENT          (256)
 189 #define P2ALIGNMENT             (2 * sizeof (gsize))                            /* fits 2 pointers (assumed to be 2 * GLIB_SIZEOF_SIZE_T below) */
 190 #define ALIGN(size, base)       ((base) * (gsize) (((size) + (base) - 1) / (base)))
 191 #define NATIVE_MALLOC_PADDING   P2ALIGNMENT                                     /* per-page padding left for native malloc(3) see [1] */
 192 #define SLAB_INFO_SIZE          P2ALIGN (sizeof (SlabInfo) + NATIVE_MALLOC_PADDING)
 193 #define MAX_MAGAZINE_SIZE       (256)                                           /* see [3] and allocator_get_magazine_threshold() for this */
 194 #define MIN_MAGAZINE_SIZE       (4)
 195 #define MAX_STAMP_COUNTER       (7)                                             /* distributes the load of gettimeofday() */
 196 #define MAX_SLAB_CHUNK_SIZE(al) (((al)-&gt;max_page_size - SLAB_INFO_SIZE) / 8)    /* we want at last 8 chunks per page, see [4] */
 197 #define MAX_SLAB_INDEX(al)      (SLAB_INDEX (al, MAX_SLAB_CHUNK_SIZE (al)) + 1)
 198 #define SLAB_INDEX(al, asize)   ((asize) / P2ALIGNMENT - 1)                     /* asize must be P2ALIGNMENT aligned */
 199 #define SLAB_CHUNK_SIZE(al, ix) (((ix) + 1) * P2ALIGNMENT)
 200 #define SLAB_BPAGE_SIZE(al,csz) (8 * (csz) + SLAB_INFO_SIZE)
 201 
 202 /* optimized version of ALIGN (size, P2ALIGNMENT) */
 203 #if     GLIB_SIZEOF_SIZE_T * 2 == 8  /* P2ALIGNMENT */
 204 #define P2ALIGN(size)   (((size) + 0x7) &amp; ~(gsize) 0x7)
 205 #elif   GLIB_SIZEOF_SIZE_T * 2 == 16 /* P2ALIGNMENT */
 206 #define P2ALIGN(size)   (((size) + 0xf) &amp; ~(gsize) 0xf)
 207 #else
 208 #define P2ALIGN(size)   ALIGN (size, P2ALIGNMENT)
 209 #endif
 210 
 211 /* special helpers to avoid gmessage.c dependency */
 212 static void mem_error (const char *format, ...) G_GNUC_PRINTF (1,2);
 213 #define mem_assert(cond)    do { if (G_LIKELY (cond)) ; else mem_error (&quot;assertion failed: %s&quot;, #cond); } while (0)
 214 
 215 /* --- structures --- */
 216 typedef struct _ChunkLink      ChunkLink;
 217 typedef struct _SlabInfo       SlabInfo;
 218 typedef struct _CachedMagazine CachedMagazine;
 219 struct _ChunkLink {
 220   ChunkLink *next;
 221   ChunkLink *data;
 222 };
 223 struct _SlabInfo {
 224   ChunkLink *chunks;
 225   guint n_allocated;
 226   SlabInfo *next, *prev;
 227 };
 228 typedef struct {
 229   ChunkLink *chunks;
 230   gsize      count;                     /* approximative chunks list length */
 231 } Magazine;
 232 typedef struct {
 233   Magazine   *magazine1;                /* array of MAX_SLAB_INDEX (allocator) */
 234   Magazine   *magazine2;                /* array of MAX_SLAB_INDEX (allocator) */
 235 } ThreadMemory;
 236 typedef struct {
 237   gboolean always_malloc;
 238   gboolean bypass_magazines;
 239   gboolean debug_blocks;
 240   gsize    working_set_msecs;
 241   guint    color_increment;
 242 } SliceConfig;
 243 typedef struct {
 244   /* const after initialization */
 245   gsize         min_page_size, max_page_size;
 246   SliceConfig   config;
 247   gsize         max_slab_chunk_size_for_magazine_cache;
 248   /* magazine cache */
 249   GMutex        magazine_mutex;
 250   ChunkLink   **magazines;                /* array of MAX_SLAB_INDEX (allocator) */
 251   guint        *contention_counters;      /* array of MAX_SLAB_INDEX (allocator) */
 252   gint          mutex_counter;
 253   guint         stamp_counter;
 254   guint         last_stamp;
 255   /* slab allocator */
 256   GMutex        slab_mutex;
 257   SlabInfo    **slab_stack;                /* array of MAX_SLAB_INDEX (allocator) */
 258   guint        color_accu;
 259 } Allocator;
 260 
 261 /* --- g-slice prototypes --- */
 262 static gpointer     slab_allocator_alloc_chunk       (gsize      chunk_size);
 263 static void         slab_allocator_free_chunk        (gsize      chunk_size,
 264                                                       gpointer   mem);
 265 static void         private_thread_memory_cleanup    (gpointer   data);
 266 static gpointer     allocator_memalign               (gsize      alignment,
 267                                                       gsize      memsize);
 268 static void         allocator_memfree                (gsize      memsize,
 269                                                       gpointer   mem);
 270 static inline void  magazine_cache_update_stamp      (void);
 271 static inline gsize allocator_get_magazine_threshold (Allocator *allocator,
 272                                                       guint      ix);
 273 
 274 /* --- g-slice memory checker --- */
 275 static void     smc_notify_alloc  (void   *pointer,
 276                                    size_t  size);
 277 static int      smc_notify_free   (void   *pointer,
 278                                    size_t  size);
 279 
 280 /* --- variables --- */
 281 static GPrivate    private_thread_memory = G_PRIVATE_INIT (private_thread_memory_cleanup);
 282 static gsize       sys_page_size = 0;
 283 static Allocator   allocator[1] = { { 0, }, };
 284 static SliceConfig slice_config = {
 285   FALSE,        /* always_malloc */
 286   FALSE,        /* bypass_magazines */
 287   FALSE,        /* debug_blocks */
 288   15 * 1000,    /* working_set_msecs */
 289   1,            /* color increment, alt: 0x7fffffff */
 290 };
 291 static GMutex      smc_tree_mutex; /* mutex for G_SLICE=debug-blocks */
 292 
 293 /* --- auxiliary funcitons --- */
 294 void
 295 g_slice_set_config (GSliceConfig ckey,
 296                     gint64       value)
 297 {
 298   g_return_if_fail (sys_page_size == 0);
 299   switch (ckey)
 300     {
 301     case G_SLICE_CONFIG_ALWAYS_MALLOC:
 302       slice_config.always_malloc = value != 0;
 303       break;
 304     case G_SLICE_CONFIG_BYPASS_MAGAZINES:
 305       slice_config.bypass_magazines = value != 0;
 306       break;
 307     case G_SLICE_CONFIG_WORKING_SET_MSECS:
 308       slice_config.working_set_msecs = value;
 309       break;
 310     case G_SLICE_CONFIG_COLOR_INCREMENT:
 311       slice_config.color_increment = value;
 312     default: ;
 313     }
 314 }
 315 
 316 gint64
 317 g_slice_get_config (GSliceConfig ckey)
 318 {
 319   switch (ckey)
 320     {
 321     case G_SLICE_CONFIG_ALWAYS_MALLOC:
 322       return slice_config.always_malloc;
 323     case G_SLICE_CONFIG_BYPASS_MAGAZINES:
 324       return slice_config.bypass_magazines;
 325     case G_SLICE_CONFIG_WORKING_SET_MSECS:
 326       return slice_config.working_set_msecs;
 327     case G_SLICE_CONFIG_CHUNK_SIZES:
 328       return MAX_SLAB_INDEX (allocator);
 329     case G_SLICE_CONFIG_COLOR_INCREMENT:
 330       return slice_config.color_increment;
 331     default:
 332       return 0;
 333     }
 334 }
 335 
 336 gint64*
 337 g_slice_get_config_state (GSliceConfig ckey,
 338                           gint64       address,
 339                           guint       *n_values)
 340 {
 341   guint i = 0;
 342   g_return_val_if_fail (n_values != NULL, NULL);
 343   *n_values = 0;
 344   switch (ckey)
 345     {
 346       gint64 array[64];
 347     case G_SLICE_CONFIG_CONTENTION_COUNTER:
 348       array[i++] = SLAB_CHUNK_SIZE (allocator, address);
 349       array[i++] = allocator-&gt;contention_counters[address];
 350       array[i++] = allocator_get_magazine_threshold (allocator, address);
 351       *n_values = i;
 352       return g_memdup (array, sizeof (array[0]) * *n_values);
 353     default:
 354       return NULL;
 355     }
 356 }
 357 
 358 static void
 359 slice_config_init (SliceConfig *config)
 360 {
 361 #ifndef GSTREAMER_LITE
 362   const gchar *val;
 363 
 364   *config = slice_config;
 365 
 366   val = getenv (&quot;G_SLICE&quot;);
 367   if (val != NULL)
 368     {
 369       gint flags;
 370       const GDebugKey keys[] = {
 371         { &quot;always-malloc&quot;, 1 &lt;&lt; 0 },
 372         { &quot;debug-blocks&quot;,  1 &lt;&lt; 1 },
 373       };
 374 
 375       flags = g_parse_debug_string (val, keys, G_N_ELEMENTS (keys));
 376       if (flags &amp; (1 &lt;&lt; 0))
 377         config-&gt;always_malloc = TRUE;
 378       if (flags &amp; (1 &lt;&lt; 1))
 379         config-&gt;debug_blocks = TRUE;
 380     }
 381   else
 382     {
 383       /* G_SLICE was not specified, so check if valgrind is running and
 384        * disable ourselves if it is.
 385        *
 386        * This way it&#39;s possible to force gslice to be enabled under
 387        * valgrind just by setting G_SLICE to the empty string.
 388        */
 389 #ifdef ENABLE_VALGRIND
 390       if (RUNNING_ON_VALGRIND)
 391         config-&gt;always_malloc = TRUE;
 392 #endif
 393     }
 394 #else // GSTREAMER_LITE
 395   *config = slice_config;
 396   config-&gt;always_malloc = TRUE;
 397 #endif // GSTREAMER_LITE
 398 }
 399 
 400 static void
 401 g_slice_init_nomessage (void)
 402 {
 403   /* we may not use g_error() or friends here */
 404   mem_assert (sys_page_size == 0);
 405   mem_assert (MIN_MAGAZINE_SIZE &gt;= 4);
 406 
 407 #ifdef G_OS_WIN32
 408   {
 409     SYSTEM_INFO system_info;
 410     GetSystemInfo (&amp;system_info);
 411     sys_page_size = system_info.dwPageSize;
 412   }
 413 #else
 414   sys_page_size = sysconf (_SC_PAGESIZE); /* = sysconf (_SC_PAGE_SIZE); = getpagesize(); */
 415 #endif
 416   mem_assert (sys_page_size &gt;= 2 * LARGEALIGNMENT);
 417   mem_assert ((sys_page_size &amp; (sys_page_size - 1)) == 0);
 418   slice_config_init (&amp;allocator-&gt;config);
 419   allocator-&gt;min_page_size = sys_page_size;
 420 #if HAVE_POSIX_MEMALIGN || HAVE_MEMALIGN
 421   /* allow allocation of pages up to 8KB (with 8KB alignment).
 422    * this is useful because many medium to large sized structures
 423    * fit less than 8 times (see [4]) into 4KB pages.
 424    * we allow very small page sizes here, to reduce wastage in
 425    * threads if only small allocations are required (this does
 426    * bear the risk of increasing allocation times and fragmentation
 427    * though).
 428    */
 429   allocator-&gt;min_page_size = MAX (allocator-&gt;min_page_size, 4096);
 430   allocator-&gt;max_page_size = MAX (allocator-&gt;min_page_size, 8192);
 431   allocator-&gt;min_page_size = MIN (allocator-&gt;min_page_size, 128);
 432 #else
 433   /* we can only align to system page size */
 434   allocator-&gt;max_page_size = sys_page_size;
 435 #endif
 436   if (allocator-&gt;config.always_malloc)
 437     {
 438       allocator-&gt;contention_counters = NULL;
 439       allocator-&gt;magazines = NULL;
 440       allocator-&gt;slab_stack = NULL;
 441     }
 442   else
 443     {
 444       allocator-&gt;contention_counters = g_new0 (guint, MAX_SLAB_INDEX (allocator));
 445       allocator-&gt;magazines = g_new0 (ChunkLink*, MAX_SLAB_INDEX (allocator));
 446       allocator-&gt;slab_stack = g_new0 (SlabInfo*, MAX_SLAB_INDEX (allocator));
 447     }
 448 
 449   allocator-&gt;mutex_counter = 0;
 450   allocator-&gt;stamp_counter = MAX_STAMP_COUNTER; /* force initial update */
 451   allocator-&gt;last_stamp = 0;
 452   allocator-&gt;color_accu = 0;
 453   magazine_cache_update_stamp();
 454   /* values cached for performance reasons */
 455   allocator-&gt;max_slab_chunk_size_for_magazine_cache = MAX_SLAB_CHUNK_SIZE (allocator);
 456   if (allocator-&gt;config.always_malloc || allocator-&gt;config.bypass_magazines)
 457     allocator-&gt;max_slab_chunk_size_for_magazine_cache = 0;      /* non-optimized cases */
 458 }
 459 
 460 static inline guint
 461 allocator_categorize (gsize aligned_chunk_size)
 462 {
 463   /* speed up the likely path */
 464   if (G_LIKELY (aligned_chunk_size &amp;&amp; aligned_chunk_size &lt;= allocator-&gt;max_slab_chunk_size_for_magazine_cache))
 465     return 1;           /* use magazine cache */
 466 
 467   if (!allocator-&gt;config.always_malloc &amp;&amp;
 468       aligned_chunk_size &amp;&amp;
 469       aligned_chunk_size &lt;= MAX_SLAB_CHUNK_SIZE (allocator))
 470     {
 471       if (allocator-&gt;config.bypass_magazines)
 472         return 2;       /* use slab allocator, see [2] */
 473       return 1;         /* use magazine cache */
 474     }
 475   return 0;             /* use malloc() */
 476 }
 477 
 478 static inline void
 479 g_mutex_lock_a (GMutex *mutex,
 480                 guint  *contention_counter)
 481 {
 482   gboolean contention = FALSE;
 483   if (!g_mutex_trylock (mutex))
 484     {
 485       g_mutex_lock (mutex);
 486       contention = TRUE;
 487     }
 488   if (contention)
 489     {
 490       allocator-&gt;mutex_counter++;
 491       if (allocator-&gt;mutex_counter &gt;= 1)        /* quickly adapt to contention */
 492         {
 493           allocator-&gt;mutex_counter = 0;
 494           *contention_counter = MIN (*contention_counter + 1, MAX_MAGAZINE_SIZE);
 495         }
 496     }
 497   else /* !contention */
 498     {
 499       allocator-&gt;mutex_counter--;
 500       if (allocator-&gt;mutex_counter &lt; -11)       /* moderately recover magazine sizes */
 501         {
 502           allocator-&gt;mutex_counter = 0;
 503           *contention_counter = MAX (*contention_counter, 1) - 1;
 504         }
 505     }
 506 }
 507 
 508 static inline ThreadMemory*
 509 thread_memory_from_self (void)
 510 {
 511   ThreadMemory *tmem = g_private_get (&amp;private_thread_memory);
 512   if (G_UNLIKELY (!tmem))
 513     {
 514       static GMutex init_mutex;
 515       guint n_magazines;
 516 
 517       g_mutex_lock (&amp;init_mutex);
 518       if G_UNLIKELY (sys_page_size == 0)
 519         g_slice_init_nomessage ();
 520       g_mutex_unlock (&amp;init_mutex);
 521 
 522       n_magazines = MAX_SLAB_INDEX (allocator);
<a name="2" id="anc2"></a><span class="line-modified"> 523       tmem = g_malloc0 (sizeof (ThreadMemory) + sizeof (Magazine) * 2 * n_magazines);</span>
 524 #ifdef GSTREAMER_LITE
 525       if (tmem == NULL)
<a name="3" id="anc3"></a><span class="line-modified"> 526           return NULL;</span>
 527 #endif // GSTREAMER_LITE
 528       tmem-&gt;magazine1 = (Magazine*) (tmem + 1);
 529       tmem-&gt;magazine2 = &amp;tmem-&gt;magazine1[n_magazines];
<a name="4" id="anc4"></a><span class="line-removed"> 530       g_private_set (&amp;private_thread_memory, tmem);</span>
 531     }
 532   return tmem;
 533 }
 534 
 535 static inline ChunkLink*
 536 magazine_chain_pop_head (ChunkLink **magazine_chunks)
 537 {
 538   /* magazine chains are linked via ChunkLink-&gt;next.
 539    * each ChunkLink-&gt;data of the toplevel chain may point to a subchain,
 540    * linked via ChunkLink-&gt;next. ChunkLink-&gt;data of the subchains just
 541    * contains uninitialized junk.
 542    */
 543   ChunkLink *chunk = (*magazine_chunks)-&gt;data;
 544   if (G_UNLIKELY (chunk))
 545     {
 546       /* allocating from freed list */
 547       (*magazine_chunks)-&gt;data = chunk-&gt;next;
 548     }
 549   else
 550     {
 551       chunk = *magazine_chunks;
 552       *magazine_chunks = chunk-&gt;next;
 553     }
 554   return chunk;
 555 }
 556 
 557 #if 0 /* useful for debugging */
 558 static guint
 559 magazine_count (ChunkLink *head)
 560 {
 561   guint count = 0;
 562   if (!head)
 563     return 0;
 564   while (head)
 565     {
 566       ChunkLink *child = head-&gt;data;
 567       count += 1;
 568       for (child = head-&gt;data; child; child = child-&gt;next)
 569         count += 1;
 570       head = head-&gt;next;
 571     }
 572   return count;
 573 }
 574 #endif
 575 
 576 static inline gsize
 577 allocator_get_magazine_threshold (Allocator *allocator,
 578                                   guint      ix)
 579 {
 580   /* the magazine size calculated here has a lower bound of MIN_MAGAZINE_SIZE,
 581    * which is required by the implementation. also, for moderately sized chunks
 582    * (say &gt;= 64 bytes), magazine sizes shouldn&#39;t be much smaller then the number
 583    * of chunks available per page/2 to avoid excessive traffic in the magazine
 584    * cache for small to medium sized structures.
 585    * the upper bound of the magazine size is effectively provided by
 586    * MAX_MAGAZINE_SIZE. for larger chunks, this number is scaled down so that
 587    * the content of a single magazine doesn&#39;t exceed ca. 16KB.
 588    */
 589   gsize chunk_size = SLAB_CHUNK_SIZE (allocator, ix);
 590   guint threshold = MAX (MIN_MAGAZINE_SIZE, allocator-&gt;max_page_size / MAX (5 * chunk_size, 5 * 32));
 591   guint contention_counter = allocator-&gt;contention_counters[ix];
 592   if (G_UNLIKELY (contention_counter))  /* single CPU bias */
 593     {
 594       /* adapt contention counter thresholds to chunk sizes */
 595       contention_counter = contention_counter * 64 / chunk_size;
 596       threshold = MAX (threshold, contention_counter);
 597     }
 598   return threshold;
 599 }
 600 
 601 /* --- magazine cache --- */
 602 static inline void
 603 magazine_cache_update_stamp (void)
 604 {
 605   if (allocator-&gt;stamp_counter &gt;= MAX_STAMP_COUNTER)
 606     {
<a name="5" id="anc5"></a><span class="line-modified"> 607       GTimeVal tv;</span>
<span class="line-modified"> 608       g_get_current_time (&amp;tv);</span>
<span class="line-removed"> 609       allocator-&gt;last_stamp = tv.tv_sec * 1000 + tv.tv_usec / 1000; /* milli seconds */</span>
 610       allocator-&gt;stamp_counter = 0;
 611     }
 612   else
 613     allocator-&gt;stamp_counter++;
 614 }
 615 
 616 static inline ChunkLink*
 617 magazine_chain_prepare_fields (ChunkLink *magazine_chunks)
 618 {
 619   ChunkLink *chunk1;
 620   ChunkLink *chunk2;
 621   ChunkLink *chunk3;
 622   ChunkLink *chunk4;
 623   /* checked upon initialization: mem_assert (MIN_MAGAZINE_SIZE &gt;= 4); */
 624   /* ensure a magazine with at least 4 unused data pointers */
 625   chunk1 = magazine_chain_pop_head (&amp;magazine_chunks);
 626   chunk2 = magazine_chain_pop_head (&amp;magazine_chunks);
 627   chunk3 = magazine_chain_pop_head (&amp;magazine_chunks);
 628   chunk4 = magazine_chain_pop_head (&amp;magazine_chunks);
 629   chunk4-&gt;next = magazine_chunks;
 630   chunk3-&gt;next = chunk4;
 631   chunk2-&gt;next = chunk3;
 632   chunk1-&gt;next = chunk2;
 633   return chunk1;
 634 }
 635 
 636 /* access the first 3 fields of a specially prepared magazine chain */
 637 #define magazine_chain_prev(mc)         ((mc)-&gt;data)
 638 #define magazine_chain_stamp(mc)        ((mc)-&gt;next-&gt;data)
 639 #define magazine_chain_uint_stamp(mc)   GPOINTER_TO_UINT ((mc)-&gt;next-&gt;data)
 640 #define magazine_chain_next(mc)         ((mc)-&gt;next-&gt;next-&gt;data)
 641 #define magazine_chain_count(mc)        ((mc)-&gt;next-&gt;next-&gt;next-&gt;data)
 642 
 643 static void
 644 magazine_cache_trim (Allocator *allocator,
 645                      guint      ix,
 646                      guint      stamp)
 647 {
 648   /* g_mutex_lock (allocator-&gt;mutex); done by caller */
 649   /* trim magazine cache from tail */
 650   ChunkLink *current = magazine_chain_prev (allocator-&gt;magazines[ix]);
 651   ChunkLink *trash = NULL;
<a name="6" id="anc6"></a><span class="line-modified"> 652   while (ABS (stamp - magazine_chain_uint_stamp (current)) &gt;= allocator-&gt;config.working_set_msecs)</span>

 653     {
 654       /* unlink */
 655       ChunkLink *prev = magazine_chain_prev (current);
 656       ChunkLink *next = magazine_chain_next (current);
 657       magazine_chain_next (prev) = next;
 658       magazine_chain_prev (next) = prev;
 659       /* clear special fields, put on trash stack */
 660       magazine_chain_next (current) = NULL;
 661       magazine_chain_count (current) = NULL;
 662       magazine_chain_stamp (current) = NULL;
 663       magazine_chain_prev (current) = trash;
 664       trash = current;
 665       /* fixup list head if required */
 666       if (current == allocator-&gt;magazines[ix])
 667         {
 668           allocator-&gt;magazines[ix] = NULL;
 669           break;
 670         }
 671       current = prev;
 672     }
 673   g_mutex_unlock (&amp;allocator-&gt;magazine_mutex);
 674   /* free trash */
 675   if (trash)
 676     {
 677       const gsize chunk_size = SLAB_CHUNK_SIZE (allocator, ix);
 678       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
 679       while (trash)
 680         {
 681           current = trash;
 682           trash = magazine_chain_prev (current);
 683           magazine_chain_prev (current) = NULL; /* clear special field */
 684           while (current)
 685             {
 686               ChunkLink *chunk = magazine_chain_pop_head (&amp;current);
 687               slab_allocator_free_chunk (chunk_size, chunk);
 688             }
 689         }
 690       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
 691     }
 692 }
 693 
 694 static void
 695 magazine_cache_push_magazine (guint      ix,
 696                               ChunkLink *magazine_chunks,
 697                               gsize      count) /* must be &gt;= MIN_MAGAZINE_SIZE */
 698 {
 699   ChunkLink *current = magazine_chain_prepare_fields (magazine_chunks);
 700   ChunkLink *next, *prev;
 701   g_mutex_lock (&amp;allocator-&gt;magazine_mutex);
 702   /* add magazine at head */
 703   next = allocator-&gt;magazines[ix];
 704   if (next)
 705     prev = magazine_chain_prev (next);
 706   else
 707     next = prev = current;
 708   magazine_chain_next (prev) = current;
 709   magazine_chain_prev (next) = current;
 710   magazine_chain_prev (current) = prev;
 711   magazine_chain_next (current) = next;
 712   magazine_chain_count (current) = (gpointer) count;
 713   /* stamp magazine */
 714   magazine_cache_update_stamp();
 715   magazine_chain_stamp (current) = GUINT_TO_POINTER (allocator-&gt;last_stamp);
 716   allocator-&gt;magazines[ix] = current;
 717   /* free old magazines beyond a certain threshold */
 718   magazine_cache_trim (allocator, ix, allocator-&gt;last_stamp);
 719   /* g_mutex_unlock (allocator-&gt;mutex); was done by magazine_cache_trim() */
 720 }
 721 
 722 static ChunkLink*
 723 magazine_cache_pop_magazine (guint  ix,
 724                              gsize *countp)
 725 {
 726   g_mutex_lock_a (&amp;allocator-&gt;magazine_mutex, &amp;allocator-&gt;contention_counters[ix]);
 727   if (!allocator-&gt;magazines[ix])
 728     {
 729       guint magazine_threshold = allocator_get_magazine_threshold (allocator, ix);
 730       gsize i, chunk_size = SLAB_CHUNK_SIZE (allocator, ix);
 731       ChunkLink *chunk, *head;
 732       g_mutex_unlock (&amp;allocator-&gt;magazine_mutex);
 733       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
 734       head = slab_allocator_alloc_chunk (chunk_size);
 735       head-&gt;data = NULL;
 736       chunk = head;
 737       for (i = 1; i &lt; magazine_threshold; i++)
 738         {
 739           chunk-&gt;next = slab_allocator_alloc_chunk (chunk_size);
 740           chunk = chunk-&gt;next;
 741           chunk-&gt;data = NULL;
 742         }
 743       chunk-&gt;next = NULL;
 744       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
 745       *countp = i;
 746       return head;
 747     }
 748   else
 749     {
 750       ChunkLink *current = allocator-&gt;magazines[ix];
 751       ChunkLink *prev = magazine_chain_prev (current);
 752       ChunkLink *next = magazine_chain_next (current);
 753       /* unlink */
 754       magazine_chain_next (prev) = next;
 755       magazine_chain_prev (next) = prev;
 756       allocator-&gt;magazines[ix] = next == current ? NULL : next;
 757       g_mutex_unlock (&amp;allocator-&gt;magazine_mutex);
 758       /* clear special fields and hand out */
 759       *countp = (gsize) magazine_chain_count (current);
 760       magazine_chain_prev (current) = NULL;
 761       magazine_chain_next (current) = NULL;
 762       magazine_chain_count (current) = NULL;
 763       magazine_chain_stamp (current) = NULL;
 764       return current;
 765     }
 766 }
 767 
 768 /* --- thread magazines --- */
 769 static void
 770 private_thread_memory_cleanup (gpointer data)
 771 {
 772   ThreadMemory *tmem = data;
 773   const guint n_magazines = MAX_SLAB_INDEX (allocator);
 774   guint ix;
 775   for (ix = 0; ix &lt; n_magazines; ix++)
 776     {
 777       Magazine *mags[2];
 778       guint j;
 779       mags[0] = &amp;tmem-&gt;magazine1[ix];
 780       mags[1] = &amp;tmem-&gt;magazine2[ix];
 781       for (j = 0; j &lt; 2; j++)
 782         {
 783           Magazine *mag = mags[j];
 784           if (mag-&gt;count &gt;= MIN_MAGAZINE_SIZE)
 785             magazine_cache_push_magazine (ix, mag-&gt;chunks, mag-&gt;count);
 786           else
 787             {
 788               const gsize chunk_size = SLAB_CHUNK_SIZE (allocator, ix);
 789               g_mutex_lock (&amp;allocator-&gt;slab_mutex);
 790               while (mag-&gt;chunks)
 791                 {
 792                   ChunkLink *chunk = magazine_chain_pop_head (&amp;mag-&gt;chunks);
 793                   slab_allocator_free_chunk (chunk_size, chunk);
 794                 }
 795               g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
 796             }
 797         }
 798     }
 799   g_free (tmem);
 800 }
 801 
 802 static void
 803 thread_memory_magazine1_reload (ThreadMemory *tmem,
 804                                 guint         ix)
 805 {
 806   Magazine *mag = &amp;tmem-&gt;magazine1[ix];
 807   mem_assert (mag-&gt;chunks == NULL); /* ensure that we may reset mag-&gt;count */
 808   mag-&gt;count = 0;
 809   mag-&gt;chunks = magazine_cache_pop_magazine (ix, &amp;mag-&gt;count);
 810 }
 811 
 812 static void
 813 thread_memory_magazine2_unload (ThreadMemory *tmem,
 814                                 guint         ix)
 815 {
 816   Magazine *mag = &amp;tmem-&gt;magazine2[ix];
 817   magazine_cache_push_magazine (ix, mag-&gt;chunks, mag-&gt;count);
 818   mag-&gt;chunks = NULL;
 819   mag-&gt;count = 0;
 820 }
 821 
 822 static inline void
 823 thread_memory_swap_magazines (ThreadMemory *tmem,
 824                               guint         ix)
 825 {
 826   Magazine xmag = tmem-&gt;magazine1[ix];
 827   tmem-&gt;magazine1[ix] = tmem-&gt;magazine2[ix];
 828   tmem-&gt;magazine2[ix] = xmag;
 829 }
 830 
 831 static inline gboolean
 832 thread_memory_magazine1_is_empty (ThreadMemory *tmem,
 833                                   guint         ix)
 834 {
 835   return tmem-&gt;magazine1[ix].chunks == NULL;
 836 }
 837 
 838 static inline gboolean
 839 thread_memory_magazine2_is_full (ThreadMemory *tmem,
 840                                  guint         ix)
 841 {
 842   return tmem-&gt;magazine2[ix].count &gt;= allocator_get_magazine_threshold (allocator, ix);
 843 }
 844 
 845 static inline gpointer
 846 thread_memory_magazine1_alloc (ThreadMemory *tmem,
 847                                guint         ix)
 848 {
 849   Magazine *mag = &amp;tmem-&gt;magazine1[ix];
 850   ChunkLink *chunk = magazine_chain_pop_head (&amp;mag-&gt;chunks);
 851   if (G_LIKELY (mag-&gt;count &gt; 0))
 852     mag-&gt;count--;
 853   return chunk;
 854 }
 855 
 856 static inline void
 857 thread_memory_magazine2_free (ThreadMemory *tmem,
 858                               guint         ix,
 859                               gpointer      mem)
 860 {
 861   Magazine *mag = &amp;tmem-&gt;magazine2[ix];
 862   ChunkLink *chunk = mem;
 863   chunk-&gt;data = NULL;
 864   chunk-&gt;next = mag-&gt;chunks;
 865   mag-&gt;chunks = chunk;
 866   mag-&gt;count++;
 867 }
 868 
 869 /* --- API functions --- */
 870 
 871 /**
 872  * g_slice_new:
 873  * @type: the type to allocate, typically a structure name
 874  *
 875  * A convenience macro to allocate a block of memory from the
 876  * slice allocator.
 877  *
 878  * It calls g_slice_alloc() with `sizeof (@type)` and casts the
 879  * returned pointer to a pointer of the given type, avoiding a type
 880  * cast in the source code. Note that the underlying slice allocation
 881  * mechanism can be changed with the [`G_SLICE=always-malloc`][G_SLICE]
 882  * environment variable.
 883  *
 884  * This can never return %NULL as the minimum allocation size from
 885  * `sizeof (@type)` is 1 byte.
 886  *
 887  * Returns: (not nullable): a pointer to the allocated block, cast to a pointer
 888  *    to @type
 889  *
 890  * Since: 2.10
 891  */
 892 
 893 /**
 894  * g_slice_new0:
 895  * @type: the type to allocate, typically a structure name
 896  *
 897  * A convenience macro to allocate a block of memory from the
 898  * slice allocator and set the memory to 0.
 899  *
 900  * It calls g_slice_alloc0() with `sizeof (@type)`
 901  * and casts the returned pointer to a pointer of the given type,
 902  * avoiding a type cast in the source code.
 903  * Note that the underlying slice allocation mechanism can
 904  * be changed with the [`G_SLICE=always-malloc`][G_SLICE]
 905  * environment variable.
 906  *
 907  * This can never return %NULL as the minimum allocation size from
 908  * `sizeof (@type)` is 1 byte.
 909  *
 910  * Returns: (not nullable): a pointer to the allocated block, cast to a pointer
 911  *    to @type
 912  *
 913  * Since: 2.10
 914  */
 915 
 916 /**
 917  * g_slice_dup:
 918  * @type: the type to duplicate, typically a structure name
 919  * @mem: (not nullable): the memory to copy into the allocated block
 920  *
 921  * A convenience macro to duplicate a block of memory using
 922  * the slice allocator.
 923  *
 924  * It calls g_slice_copy() with `sizeof (@type)`
 925  * and casts the returned pointer to a pointer of the given type,
 926  * avoiding a type cast in the source code.
 927  * Note that the underlying slice allocation mechanism can
 928  * be changed with the [`G_SLICE=always-malloc`][G_SLICE]
 929  * environment variable.
 930  *
 931  * This can never return %NULL.
 932  *
 933  * Returns: (not nullable): a pointer to the allocated block, cast to a pointer
 934  *    to @type
 935  *
 936  * Since: 2.14
 937  */
 938 
 939 /**
 940  * g_slice_free:
 941  * @type: the type of the block to free, typically a structure name
 942  * @mem: a pointer to the block to free
 943  *
 944  * A convenience macro to free a block of memory that has
 945  * been allocated from the slice allocator.
 946  *
 947  * It calls g_slice_free1() using `sizeof (type)`
 948  * as the block size.
 949  * Note that the exact release behaviour can be changed with the
 950  * [`G_DEBUG=gc-friendly`][G_DEBUG] environment variable, also see
 951  * [`G_SLICE`][G_SLICE] for related debugging options.
 952  *
 953  * If @mem is %NULL, this macro does nothing.
 954  *
 955  * Since: 2.10
 956  */
 957 
 958 /**
 959  * g_slice_free_chain:
 960  * @type: the type of the @mem_chain blocks
 961  * @mem_chain: a pointer to the first block of the chain
 962  * @next: the field name of the next pointer in @type
 963  *
 964  * Frees a linked list of memory blocks of structure type @type.
 965  * The memory blocks must be equal-sized, allocated via
 966  * g_slice_alloc() or g_slice_alloc0() and linked together by
 967  * a @next pointer (similar to #GSList). The name of the
 968  * @next field in @type is passed as third argument.
 969  * Note that the exact release behaviour can be changed with the
 970  * [`G_DEBUG=gc-friendly`][G_DEBUG] environment variable, also see
 971  * [`G_SLICE`][G_SLICE] for related debugging options.
 972  *
 973  * If @mem_chain is %NULL, this function does nothing.
 974  *
 975  * Since: 2.10
 976  */
 977 
 978 /**
 979  * g_slice_alloc:
 980  * @block_size: the number of bytes to allocate
 981  *
 982  * Allocates a block of memory from the slice allocator.
 983  * The block address handed out can be expected to be aligned
 984  * to at least 1 * sizeof (void*),
 985  * though in general slices are 2 * sizeof (void*) bytes aligned,
 986  * if a malloc() fallback implementation is used instead,
 987  * the alignment may be reduced in a libc dependent fashion.
 988  * Note that the underlying slice allocation mechanism can
 989  * be changed with the [`G_SLICE=always-malloc`][G_SLICE]
 990  * environment variable.
 991  *
 992  * Returns: a pointer to the allocated memory block, which will be %NULL if and
 993  *    only if @mem_size is 0
 994  *
 995  * Since: 2.10
 996  */
 997 gpointer
 998 g_slice_alloc (gsize mem_size)
 999 {
1000   ThreadMemory *tmem;
1001   gsize chunk_size;
1002   gpointer mem;
1003   guint acat;
1004 
1005   /* This gets the private structure for this thread.  If the private
1006    * structure does not yet exist, it is created.
1007    *
1008    * This has a side effect of causing GSlice to be initialised, so it
1009    * must come first.
1010    */
1011   tmem = thread_memory_from_self ();
1012 #ifdef GSTREAMER_LITE
1013       if (tmem == NULL)
1014           return NULL;
1015 #endif // GSTREAMER_LITE
1016 
1017   chunk_size = P2ALIGN (mem_size);
1018   acat = allocator_categorize (chunk_size);
1019   if (G_LIKELY (acat == 1))     /* allocate through magazine layer */
1020     {
1021       guint ix = SLAB_INDEX (allocator, chunk_size);
1022       if (G_UNLIKELY (thread_memory_magazine1_is_empty (tmem, ix)))
1023         {
1024           thread_memory_swap_magazines (tmem, ix);
1025           if (G_UNLIKELY (thread_memory_magazine1_is_empty (tmem, ix)))
1026             thread_memory_magazine1_reload (tmem, ix);
1027         }
1028       mem = thread_memory_magazine1_alloc (tmem, ix);
1029     }
1030   else if (acat == 2)           /* allocate through slab allocator */
1031     {
1032       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
1033       mem = slab_allocator_alloc_chunk (chunk_size);
1034       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
1035     }
1036   else                          /* delegate to system malloc */
1037     mem = g_malloc (mem_size);
1038   if (G_UNLIKELY (allocator-&gt;config.debug_blocks))
1039     smc_notify_alloc (mem, mem_size);
1040 
1041   TRACE (GLIB_SLICE_ALLOC((void*)mem, mem_size));
1042 
1043   return mem;
1044 }
1045 
1046 /**
1047  * g_slice_alloc0:
1048  * @block_size: the number of bytes to allocate
1049  *
1050  * Allocates a block of memory via g_slice_alloc() and initializes
1051  * the returned memory to 0. Note that the underlying slice allocation
1052  * mechanism can be changed with the [`G_SLICE=always-malloc`][G_SLICE]
1053  * environment variable.
1054  *
1055  * Returns: a pointer to the allocated block, which will be %NULL if and only
1056  *    if @mem_size is 0
1057  *
1058  * Since: 2.10
1059  */
1060 gpointer
1061 g_slice_alloc0 (gsize mem_size)
1062 {
1063   gpointer mem = g_slice_alloc (mem_size);
1064   if (mem)
1065     memset (mem, 0, mem_size);
1066   return mem;
1067 }
1068 
1069 /**
1070  * g_slice_copy:
1071  * @block_size: the number of bytes to allocate
1072  * @mem_block: the memory to copy
1073  *
1074  * Allocates a block of memory from the slice allocator
1075  * and copies @block_size bytes into it from @mem_block.
1076  *
1077  * @mem_block must be non-%NULL if @block_size is non-zero.
1078  *
1079  * Returns: a pointer to the allocated memory block, which will be %NULL if and
1080  *    only if @mem_size is 0
1081  *
1082  * Since: 2.14
1083  */
1084 gpointer
1085 g_slice_copy (gsize         mem_size,
1086               gconstpointer mem_block)
1087 {
1088   gpointer mem = g_slice_alloc (mem_size);
1089   if (mem)
1090     memcpy (mem, mem_block, mem_size);
1091   return mem;
1092 }
1093 
1094 /**
1095  * g_slice_free1:
1096  * @block_size: the size of the block
1097  * @mem_block: a pointer to the block to free
1098  *
1099  * Frees a block of memory.
1100  *
1101  * The memory must have been allocated via g_slice_alloc() or
1102  * g_slice_alloc0() and the @block_size has to match the size
1103  * specified upon allocation. Note that the exact release behaviour
1104  * can be changed with the [`G_DEBUG=gc-friendly`][G_DEBUG] environment
1105  * variable, also see [`G_SLICE`][G_SLICE] for related debugging options.
1106  *
1107  * If @mem_block is %NULL, this function does nothing.
1108  *
1109  * Since: 2.10
1110  */
1111 void
1112 g_slice_free1 (gsize    mem_size,
1113                gpointer mem_block)
1114 {
1115   gsize chunk_size = P2ALIGN (mem_size);
1116   guint acat = allocator_categorize (chunk_size);
1117   if (G_UNLIKELY (!mem_block))
1118     return;
1119   if (G_UNLIKELY (allocator-&gt;config.debug_blocks) &amp;&amp;
1120       !smc_notify_free (mem_block, mem_size))
1121     abort();
1122   if (G_LIKELY (acat == 1))             /* allocate through magazine layer */
1123     {
1124       ThreadMemory *tmem = thread_memory_from_self();
1125       guint ix = SLAB_INDEX (allocator, chunk_size);
1126 #ifdef GSTREAMER_LITE
1127       if (tmem == NULL)
1128           return; // Nothing to free
1129 #endif // GSTREAMER_LITE
1130       if (G_UNLIKELY (thread_memory_magazine2_is_full (tmem, ix)))
1131         {
1132           thread_memory_swap_magazines (tmem, ix);
1133           if (G_UNLIKELY (thread_memory_magazine2_is_full (tmem, ix)))
1134             thread_memory_magazine2_unload (tmem, ix);
1135         }
1136       if (G_UNLIKELY (g_mem_gc_friendly))
1137         memset (mem_block, 0, chunk_size);
1138       thread_memory_magazine2_free (tmem, ix, mem_block);
1139     }
1140   else if (acat == 2)                   /* allocate through slab allocator */
1141     {
1142       if (G_UNLIKELY (g_mem_gc_friendly))
1143         memset (mem_block, 0, chunk_size);
1144       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
1145       slab_allocator_free_chunk (chunk_size, mem_block);
1146       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
1147     }
1148   else                                  /* delegate to system malloc */
1149     {
1150       if (G_UNLIKELY (g_mem_gc_friendly))
1151         memset (mem_block, 0, mem_size);
1152       g_free (mem_block);
1153     }
1154   TRACE (GLIB_SLICE_FREE((void*)mem_block, mem_size));
1155 }
1156 
1157 /**
1158  * g_slice_free_chain_with_offset:
1159  * @block_size: the size of the blocks
1160  * @mem_chain:  a pointer to the first block of the chain
1161  * @next_offset: the offset of the @next field in the blocks
1162  *
1163  * Frees a linked list of memory blocks of structure type @type.
1164  *
1165  * The memory blocks must be equal-sized, allocated via
1166  * g_slice_alloc() or g_slice_alloc0() and linked together by a
1167  * @next pointer (similar to #GSList). The offset of the @next
1168  * field in each block is passed as third argument.
1169  * Note that the exact release behaviour can be changed with the
1170  * [`G_DEBUG=gc-friendly`][G_DEBUG] environment variable, also see
1171  * [`G_SLICE`][G_SLICE] for related debugging options.
1172  *
1173  * If @mem_chain is %NULL, this function does nothing.
1174  *
1175  * Since: 2.10
1176  */
1177 void
1178 g_slice_free_chain_with_offset (gsize    mem_size,
1179                                 gpointer mem_chain,
1180                                 gsize    next_offset)
1181 {
1182   gpointer slice = mem_chain;
1183   /* while the thread magazines and the magazine cache are implemented so that
1184    * they can easily be extended to allow for free lists containing more free
1185    * lists for the first level nodes, which would allow O(1) freeing in this
1186    * function, the benefit of such an extension is questionable, because:
1187    * - the magazine size counts will become mere lower bounds which confuses
1188    *   the code adapting to lock contention;
1189    * - freeing a single node to the thread magazines is very fast, so this
1190    *   O(list_length) operation is multiplied by a fairly small factor;
1191    * - memory usage histograms on larger applications seem to indicate that
1192    *   the amount of released multi node lists is negligible in comparison
1193    *   to single node releases.
1194    * - the major performance bottle neck, namely g_private_get() or
1195    *   g_mutex_lock()/g_mutex_unlock() has already been moved out of the
1196    *   inner loop for freeing chained slices.
1197    */
1198   gsize chunk_size = P2ALIGN (mem_size);
1199   guint acat = allocator_categorize (chunk_size);
1200   if (G_LIKELY (acat == 1))             /* allocate through magazine layer */
1201     {
1202       ThreadMemory *tmem = thread_memory_from_self();
1203       guint ix = SLAB_INDEX (allocator, chunk_size);
1204 #ifdef GSTREAMER_LITE
1205       if (tmem == NULL)
1206           return; // Nothing to free
1207 #endif // GSTREAMER_LITE
1208       while (slice)
1209         {
1210           guint8 *current = slice;
1211           slice = *(gpointer*) (current + next_offset);
1212           if (G_UNLIKELY (allocator-&gt;config.debug_blocks) &amp;&amp;
1213               !smc_notify_free (current, mem_size))
1214             abort();
1215           if (G_UNLIKELY (thread_memory_magazine2_is_full (tmem, ix)))
1216             {
1217               thread_memory_swap_magazines (tmem, ix);
1218               if (G_UNLIKELY (thread_memory_magazine2_is_full (tmem, ix)))
1219                 thread_memory_magazine2_unload (tmem, ix);
1220             }
1221           if (G_UNLIKELY (g_mem_gc_friendly))
1222             memset (current, 0, chunk_size);
1223           thread_memory_magazine2_free (tmem, ix, current);
1224         }
1225     }
1226   else if (acat == 2)                   /* allocate through slab allocator */
1227     {
1228       g_mutex_lock (&amp;allocator-&gt;slab_mutex);
1229       while (slice)
1230         {
1231           guint8 *current = slice;
1232           slice = *(gpointer*) (current + next_offset);
1233           if (G_UNLIKELY (allocator-&gt;config.debug_blocks) &amp;&amp;
1234               !smc_notify_free (current, mem_size))
1235             abort();
1236           if (G_UNLIKELY (g_mem_gc_friendly))
1237             memset (current, 0, chunk_size);
1238           slab_allocator_free_chunk (chunk_size, current);
1239         }
1240       g_mutex_unlock (&amp;allocator-&gt;slab_mutex);
1241     }
1242   else                                  /* delegate to system malloc */
1243     while (slice)
1244       {
1245         guint8 *current = slice;
1246         slice = *(gpointer*) (current + next_offset);
1247         if (G_UNLIKELY (allocator-&gt;config.debug_blocks) &amp;&amp;
1248             !smc_notify_free (current, mem_size))
1249           abort();
1250         if (G_UNLIKELY (g_mem_gc_friendly))
1251           memset (current, 0, mem_size);
1252         g_free (current);
1253       }
1254 }
1255 
1256 /* --- single page allocator --- */
1257 static void
1258 allocator_slab_stack_push (Allocator *allocator,
1259                            guint      ix,
1260                            SlabInfo  *sinfo)
1261 {
1262   /* insert slab at slab ring head */
1263   if (!allocator-&gt;slab_stack[ix])
1264     {
1265       sinfo-&gt;next = sinfo;
1266       sinfo-&gt;prev = sinfo;
1267     }
1268   else
1269     {
1270       SlabInfo *next = allocator-&gt;slab_stack[ix], *prev = next-&gt;prev;
1271       next-&gt;prev = sinfo;
1272       prev-&gt;next = sinfo;
1273       sinfo-&gt;next = next;
1274       sinfo-&gt;prev = prev;
1275     }
1276   allocator-&gt;slab_stack[ix] = sinfo;
1277 }
1278 
1279 static gsize
1280 allocator_aligned_page_size (Allocator *allocator,
1281                              gsize      n_bytes)
1282 {
1283   gsize val = 1 &lt;&lt; g_bit_storage (n_bytes - 1);
1284   val = MAX (val, allocator-&gt;min_page_size);
1285   return val;
1286 }
1287 
1288 static void
1289 allocator_add_slab (Allocator *allocator,
1290                     guint      ix,
1291                     gsize      chunk_size)
1292 {
1293   ChunkLink *chunk;
1294   SlabInfo *sinfo;
1295   gsize addr, padding, n_chunks, color = 0;
1296   gsize page_size;
1297   int errsv;
1298   gpointer aligned_memory;
1299   guint8 *mem;
1300   guint i;
1301 
1302   page_size = allocator_aligned_page_size (allocator, SLAB_BPAGE_SIZE (allocator, chunk_size));
1303   /* allocate 1 page for the chunks and the slab */
1304   aligned_memory = allocator_memalign (page_size, page_size - NATIVE_MALLOC_PADDING);
1305   errsv = errno;
1306   mem = aligned_memory;
1307 
1308   if (!mem)
1309     {
1310       const gchar *syserr = strerror (errsv);
1311       mem_error (&quot;failed to allocate %u bytes (alignment: %u): %s\n&quot;,
1312                  (guint) (page_size - NATIVE_MALLOC_PADDING), (guint) page_size, syserr);
1313     }
1314   /* mask page address */
1315   addr = ((gsize) mem / page_size) * page_size;
1316   /* assert alignment */
1317   mem_assert (aligned_memory == (gpointer) addr);
1318   /* basic slab info setup */
1319   sinfo = (SlabInfo*) (mem + page_size - SLAB_INFO_SIZE);
1320   sinfo-&gt;n_allocated = 0;
1321   sinfo-&gt;chunks = NULL;
1322   /* figure cache colorization */
1323   n_chunks = ((guint8*) sinfo - mem) / chunk_size;
1324   padding = ((guint8*) sinfo - mem) - n_chunks * chunk_size;
1325   if (padding)
1326     {
1327       color = (allocator-&gt;color_accu * P2ALIGNMENT) % padding;
1328       allocator-&gt;color_accu += allocator-&gt;config.color_increment;
1329     }
1330   /* add chunks to free list */
1331   chunk = (ChunkLink*) (mem + color);
1332   sinfo-&gt;chunks = chunk;
1333   for (i = 0; i &lt; n_chunks - 1; i++)
1334     {
1335       chunk-&gt;next = (ChunkLink*) ((guint8*) chunk + chunk_size);
1336       chunk = chunk-&gt;next;
1337     }
1338   chunk-&gt;next = NULL;   /* last chunk */
1339   /* add slab to slab ring */
1340   allocator_slab_stack_push (allocator, ix, sinfo);
1341 }
1342 
1343 static gpointer
1344 slab_allocator_alloc_chunk (gsize chunk_size)
1345 {
1346   ChunkLink *chunk;
1347   guint ix = SLAB_INDEX (allocator, chunk_size);
1348   /* ensure non-empty slab */
1349   if (!allocator-&gt;slab_stack[ix] || !allocator-&gt;slab_stack[ix]-&gt;chunks)
1350     allocator_add_slab (allocator, ix, chunk_size);
1351   /* allocate chunk */
1352   chunk = allocator-&gt;slab_stack[ix]-&gt;chunks;
1353   allocator-&gt;slab_stack[ix]-&gt;chunks = chunk-&gt;next;
1354   allocator-&gt;slab_stack[ix]-&gt;n_allocated++;
1355   /* rotate empty slabs */
1356   if (!allocator-&gt;slab_stack[ix]-&gt;chunks)
1357     allocator-&gt;slab_stack[ix] = allocator-&gt;slab_stack[ix]-&gt;next;
1358   return chunk;
1359 }
1360 
1361 static void
1362 slab_allocator_free_chunk (gsize    chunk_size,
1363                            gpointer mem)
1364 {
1365   ChunkLink *chunk;
1366   gboolean was_empty;
1367   guint ix = SLAB_INDEX (allocator, chunk_size);
1368   gsize page_size = allocator_aligned_page_size (allocator, SLAB_BPAGE_SIZE (allocator, chunk_size));
1369   gsize addr = ((gsize) mem / page_size) * page_size;
1370   /* mask page address */
1371   guint8 *page = (guint8*) addr;
1372   SlabInfo *sinfo = (SlabInfo*) (page + page_size - SLAB_INFO_SIZE);
1373   /* assert valid chunk count */
1374   mem_assert (sinfo-&gt;n_allocated &gt; 0);
1375   /* add chunk to free list */
1376   was_empty = sinfo-&gt;chunks == NULL;
1377   chunk = (ChunkLink*) mem;
1378   chunk-&gt;next = sinfo-&gt;chunks;
1379   sinfo-&gt;chunks = chunk;
1380   sinfo-&gt;n_allocated--;
1381   /* keep slab ring partially sorted, empty slabs at end */
1382   if (was_empty)
1383     {
1384       /* unlink slab */
1385       SlabInfo *next = sinfo-&gt;next, *prev = sinfo-&gt;prev;
1386       next-&gt;prev = prev;
1387       prev-&gt;next = next;
1388       if (allocator-&gt;slab_stack[ix] == sinfo)
1389         allocator-&gt;slab_stack[ix] = next == sinfo ? NULL : next;
1390       /* insert slab at head */
1391       allocator_slab_stack_push (allocator, ix, sinfo);
1392     }
1393   /* eagerly free complete unused slabs */
1394   if (!sinfo-&gt;n_allocated)
1395     {
1396       /* unlink slab */
1397       SlabInfo *next = sinfo-&gt;next, *prev = sinfo-&gt;prev;
1398       next-&gt;prev = prev;
1399       prev-&gt;next = next;
1400       if (allocator-&gt;slab_stack[ix] == sinfo)
1401         allocator-&gt;slab_stack[ix] = next == sinfo ? NULL : next;
1402       /* free slab */
1403       allocator_memfree (page_size, page);
1404     }
1405 }
1406 
1407 /* --- memalign implementation --- */
1408 #ifdef HAVE_MALLOC_H
1409 #include &lt;malloc.h&gt;             /* memalign() */
1410 #endif
1411 
1412 /* from config.h:
1413  * define HAVE_POSIX_MEMALIGN           1 // if free(posix_memalign(3)) works, &lt;stdlib.h&gt;
1414  * define HAVE_MEMALIGN                 1 // if free(memalign(3)) works, &lt;malloc.h&gt;
1415  * define HAVE_VALLOC                   1 // if free(valloc(3)) works, &lt;stdlib.h&gt; or &lt;malloc.h&gt;
1416  * if none is provided, we implement malloc(3)-based alloc-only page alignment
1417  */
1418 
1419 #if !(HAVE_POSIX_MEMALIGN || HAVE_MEMALIGN || HAVE_VALLOC)
<a name="7" id="anc7"></a>
1420 static GTrashStack *compat_valloc_trash = NULL;
<a name="8" id="anc8"></a>
1421 #endif
1422 
1423 static gpointer
1424 allocator_memalign (gsize alignment,
1425                     gsize memsize)
1426 {
1427   gpointer aligned_memory = NULL;
1428   gint err = ENOMEM;
1429 #if     HAVE_POSIX_MEMALIGN
1430   err = posix_memalign (&amp;aligned_memory, alignment, memsize);
1431 #elif   HAVE_MEMALIGN
1432   errno = 0;
1433   aligned_memory = memalign (alignment, memsize);
1434   err = errno;
1435 #elif   HAVE_VALLOC
1436   errno = 0;
1437   aligned_memory = valloc (memsize);
1438   err = errno;
1439 #else
1440   /* simplistic non-freeing page allocator */
1441   mem_assert (alignment == sys_page_size);
1442   mem_assert (memsize &lt;= sys_page_size);
1443   if (!compat_valloc_trash)
1444     {
1445       const guint n_pages = 16;
1446       guint8 *mem = malloc (n_pages * sys_page_size);
1447       err = errno;
1448       if (mem)
1449         {
1450           gint i = n_pages;
1451           guint8 *amem = (guint8*) ALIGN ((gsize) mem, sys_page_size);
1452           if (amem != mem)
1453             i--;        /* mem wasn&#39;t page aligned */
1454           G_GNUC_BEGIN_IGNORE_DEPRECATIONS
1455           while (--i &gt;= 0)
1456             g_trash_stack_push (&amp;compat_valloc_trash, amem + i * sys_page_size);
1457           G_GNUC_END_IGNORE_DEPRECATIONS
1458         }
1459     }
1460   G_GNUC_BEGIN_IGNORE_DEPRECATIONS
1461   aligned_memory = g_trash_stack_pop (&amp;compat_valloc_trash);
1462   G_GNUC_END_IGNORE_DEPRECATIONS
1463 #endif
1464   if (!aligned_memory)
1465     errno = err;
1466   return aligned_memory;
1467 }
1468 
1469 static void
1470 allocator_memfree (gsize    memsize,
1471                    gpointer mem)
1472 {
1473 #if     HAVE_POSIX_MEMALIGN || HAVE_MEMALIGN || HAVE_VALLOC
1474   free (mem);
1475 #else
1476   mem_assert (memsize &lt;= sys_page_size);
1477   G_GNUC_BEGIN_IGNORE_DEPRECATIONS
1478   g_trash_stack_push (&amp;compat_valloc_trash, mem);
1479   G_GNUC_END_IGNORE_DEPRECATIONS
1480 #endif
1481 }
1482 
1483 static void
1484 mem_error (const char *format,
1485            ...)
1486 {
1487   const char *pname;
1488   va_list args;
1489   /* at least, put out &quot;MEMORY-ERROR&quot;, in case we segfault during the rest of the function */
1490   fputs (&quot;\n***MEMORY-ERROR***: &quot;, stderr);
1491   pname = g_get_prgname();
1492   g_fprintf (stderr, &quot;%s[%ld]: GSlice: &quot;, pname ? pname : &quot;&quot;, (long)getpid());
1493   va_start (args, format);
1494   g_vfprintf (stderr, format, args);
1495   va_end (args);
1496   fputs (&quot;\n&quot;, stderr);
1497   abort();
1498   _exit (1);
1499 }
1500 
1501 /* --- g-slice memory checker tree --- */
1502 typedef size_t SmcKType;                /* key type */
1503 typedef size_t SmcVType;                /* value type */
1504 typedef struct {
1505   SmcKType key;
1506   SmcVType value;
1507 } SmcEntry;
1508 static void             smc_tree_insert      (SmcKType  key,
1509                                               SmcVType  value);
1510 static gboolean         smc_tree_lookup      (SmcKType  key,
1511                                               SmcVType *value_p);
1512 static gboolean         smc_tree_remove      (SmcKType  key);
1513 
1514 
1515 /* --- g-slice memory checker implementation --- */
1516 static void
1517 smc_notify_alloc (void   *pointer,
1518                   size_t  size)
1519 {
1520   size_t address = (size_t) pointer;
1521   if (pointer)
1522     smc_tree_insert (address, size);
1523 }
1524 
1525 #if 0
1526 static void
1527 smc_notify_ignore (void *pointer)
1528 {
1529   size_t address = (size_t) pointer;
1530   if (pointer)
1531     smc_tree_remove (address);
1532 }
1533 #endif
1534 
1535 static int
1536 smc_notify_free (void   *pointer,
1537                  size_t  size)
1538 {
1539   size_t address = (size_t) pointer;
1540   SmcVType real_size;
1541   gboolean found_one;
1542 
1543   if (!pointer)
1544     return 1; /* ignore */
1545   found_one = smc_tree_lookup (address, &amp;real_size);
1546   if (!found_one)
1547     {
1548       g_fprintf (stderr, &quot;GSlice: MemChecker: attempt to release non-allocated block: %p size=%&quot; G_GSIZE_FORMAT &quot;\n&quot;, pointer, size);
1549       return 0;
1550     }
1551   if (real_size != size &amp;&amp; (real_size || size))
1552     {
1553       g_fprintf (stderr, &quot;GSlice: MemChecker: attempt to release block with invalid size: %p size=%&quot; G_GSIZE_FORMAT &quot; invalid-size=%&quot; G_GSIZE_FORMAT &quot;\n&quot;, pointer, real_size, size);
1554       return 0;
1555     }
1556   if (!smc_tree_remove (address))
1557     {
1558       g_fprintf (stderr, &quot;GSlice: MemChecker: attempt to release non-allocated block: %p size=%&quot; G_GSIZE_FORMAT &quot;\n&quot;, pointer, size);
1559       return 0;
1560     }
1561   return 1; /* all fine */
1562 }
1563 
1564 /* --- g-slice memory checker tree implementation --- */
1565 #define SMC_TRUNK_COUNT     (4093 /* 16381 */)          /* prime, to distribute trunk collisions (big, allocated just once) */
1566 #define SMC_BRANCH_COUNT    (511)                       /* prime, to distribute branch collisions */
1567 #define SMC_TRUNK_EXTENT    (SMC_BRANCH_COUNT * 2039)   /* key address space per trunk, should distribute uniformly across BRANCH_COUNT */
1568 #define SMC_TRUNK_HASH(k)   ((k / SMC_TRUNK_EXTENT) % SMC_TRUNK_COUNT)  /* generate new trunk hash per megabyte (roughly) */
1569 #define SMC_BRANCH_HASH(k)  (k % SMC_BRANCH_COUNT)
1570 
1571 typedef struct {
1572   SmcEntry    *entries;
1573   unsigned int n_entries;
1574 } SmcBranch;
1575 
1576 static SmcBranch     **smc_tree_root = NULL;
1577 
1578 static void
1579 smc_tree_abort (int errval)
1580 {
1581   const char *syserr = strerror (errval);
1582   mem_error (&quot;MemChecker: failure in debugging tree: %s&quot;, syserr);
1583 }
1584 
1585 static inline SmcEntry*
1586 smc_tree_branch_grow_L (SmcBranch   *branch,
1587                         unsigned int index)
1588 {
1589   unsigned int old_size = branch-&gt;n_entries * sizeof (branch-&gt;entries[0]);
1590   unsigned int new_size = old_size + sizeof (branch-&gt;entries[0]);
1591   SmcEntry *entry;
1592   mem_assert (index &lt;= branch-&gt;n_entries);
1593   branch-&gt;entries = (SmcEntry*) realloc (branch-&gt;entries, new_size);
1594   if (!branch-&gt;entries)
1595     smc_tree_abort (errno);
1596   entry = branch-&gt;entries + index;
1597   memmove (entry + 1, entry, (branch-&gt;n_entries - index) * sizeof (entry[0]));
1598   branch-&gt;n_entries += 1;
1599   return entry;
1600 }
1601 
1602 static inline SmcEntry*
1603 smc_tree_branch_lookup_nearest_L (SmcBranch *branch,
1604                                   SmcKType   key)
1605 {
1606   unsigned int n_nodes = branch-&gt;n_entries, offs = 0;
1607   SmcEntry *check = branch-&gt;entries;
1608   int cmp = 0;
1609   while (offs &lt; n_nodes)
1610     {
1611       unsigned int i = (offs + n_nodes) &gt;&gt; 1;
1612       check = branch-&gt;entries + i;
1613       cmp = key &lt; check-&gt;key ? -1 : key != check-&gt;key;
1614       if (cmp == 0)
1615         return check;                   /* return exact match */
1616       else if (cmp &lt; 0)
1617         n_nodes = i;
1618       else /* (cmp &gt; 0) */
1619         offs = i + 1;
1620     }
1621   /* check points at last mismatch, cmp &gt; 0 indicates greater key */
1622   return cmp &gt; 0 ? check + 1 : check;   /* return insertion position for inexact match */
1623 }
1624 
1625 static void
1626 smc_tree_insert (SmcKType key,
1627                  SmcVType value)
1628 {
1629   unsigned int ix0, ix1;
1630   SmcEntry *entry;
1631 
1632   g_mutex_lock (&amp;smc_tree_mutex);
1633   ix0 = SMC_TRUNK_HASH (key);
1634   ix1 = SMC_BRANCH_HASH (key);
1635   if (!smc_tree_root)
1636     {
1637       smc_tree_root = calloc (SMC_TRUNK_COUNT, sizeof (smc_tree_root[0]));
1638       if (!smc_tree_root)
1639         smc_tree_abort (errno);
1640     }
1641   if (!smc_tree_root[ix0])
1642     {
1643       smc_tree_root[ix0] = calloc (SMC_BRANCH_COUNT, sizeof (smc_tree_root[0][0]));
1644       if (!smc_tree_root[ix0])
1645         smc_tree_abort (errno);
1646     }
1647   entry = smc_tree_branch_lookup_nearest_L (&amp;smc_tree_root[ix0][ix1], key);
1648   if (!entry ||                                                                         /* need create */
1649       entry &gt;= smc_tree_root[ix0][ix1].entries + smc_tree_root[ix0][ix1].n_entries ||   /* need append */
1650       entry-&gt;key != key)                                                                /* need insert */
1651     entry = smc_tree_branch_grow_L (&amp;smc_tree_root[ix0][ix1], entry - smc_tree_root[ix0][ix1].entries);
1652   entry-&gt;key = key;
1653   entry-&gt;value = value;
1654   g_mutex_unlock (&amp;smc_tree_mutex);
1655 }
1656 
1657 static gboolean
1658 smc_tree_lookup (SmcKType  key,
1659                  SmcVType *value_p)
1660 {
1661   SmcEntry *entry = NULL;
1662   unsigned int ix0 = SMC_TRUNK_HASH (key), ix1 = SMC_BRANCH_HASH (key);
1663   gboolean found_one = FALSE;
1664   *value_p = 0;
1665   g_mutex_lock (&amp;smc_tree_mutex);
1666   if (smc_tree_root &amp;&amp; smc_tree_root[ix0])
1667     {
1668       entry = smc_tree_branch_lookup_nearest_L (&amp;smc_tree_root[ix0][ix1], key);
1669       if (entry &amp;&amp;
1670           entry &lt; smc_tree_root[ix0][ix1].entries + smc_tree_root[ix0][ix1].n_entries &amp;&amp;
1671           entry-&gt;key == key)
1672         {
1673           found_one = TRUE;
1674           *value_p = entry-&gt;value;
1675         }
1676     }
1677   g_mutex_unlock (&amp;smc_tree_mutex);
1678   return found_one;
1679 }
1680 
1681 static gboolean
1682 smc_tree_remove (SmcKType key)
1683 {
1684   unsigned int ix0 = SMC_TRUNK_HASH (key), ix1 = SMC_BRANCH_HASH (key);
1685   gboolean found_one = FALSE;
1686   g_mutex_lock (&amp;smc_tree_mutex);
1687   if (smc_tree_root &amp;&amp; smc_tree_root[ix0])
1688     {
1689       SmcEntry *entry = smc_tree_branch_lookup_nearest_L (&amp;smc_tree_root[ix0][ix1], key);
1690       if (entry &amp;&amp;
1691           entry &lt; smc_tree_root[ix0][ix1].entries + smc_tree_root[ix0][ix1].n_entries &amp;&amp;
1692           entry-&gt;key == key)
1693         {
1694           unsigned int i = entry - smc_tree_root[ix0][ix1].entries;
1695           smc_tree_root[ix0][ix1].n_entries -= 1;
1696           memmove (entry, entry + 1, (smc_tree_root[ix0][ix1].n_entries - i) * sizeof (entry[0]));
1697           if (!smc_tree_root[ix0][ix1].n_entries)
1698             {
1699               /* avoid useless pressure on the memory system */
1700               free (smc_tree_root[ix0][ix1].entries);
1701               smc_tree_root[ix0][ix1].entries = NULL;
1702             }
1703           found_one = TRUE;
1704         }
1705     }
1706   g_mutex_unlock (&amp;smc_tree_mutex);
1707   return found_one;
1708 }
1709 
1710 #ifdef G_ENABLE_DEBUG
1711 void
1712 g_slice_debug_tree_statistics (void)
1713 {
1714   g_mutex_lock (&amp;smc_tree_mutex);
1715   if (smc_tree_root)
1716     {
1717       unsigned int i, j, t = 0, o = 0, b = 0, su = 0, ex = 0, en = 4294967295u;
1718       double tf, bf;
1719       for (i = 0; i &lt; SMC_TRUNK_COUNT; i++)
1720         if (smc_tree_root[i])
1721           {
1722             t++;
1723             for (j = 0; j &lt; SMC_BRANCH_COUNT; j++)
1724               if (smc_tree_root[i][j].n_entries)
1725                 {
1726                   b++;
1727                   su += smc_tree_root[i][j].n_entries;
1728                   en = MIN (en, smc_tree_root[i][j].n_entries);
1729                   ex = MAX (ex, smc_tree_root[i][j].n_entries);
1730                 }
1731               else if (smc_tree_root[i][j].entries)
1732                 o++; /* formerly used, now empty */
1733           }
1734       en = b ? en : 0;
1735       tf = MAX (t, 1.0); /* max(1) to be a valid divisor */
1736       bf = MAX (b, 1.0); /* max(1) to be a valid divisor */
1737       g_fprintf (stderr, &quot;GSlice: MemChecker: %u trunks, %u branches, %u old branches\n&quot;, t, b, o);
1738       g_fprintf (stderr, &quot;GSlice: MemChecker: %f branches per trunk, %.2f%% utilization\n&quot;,
1739                b / tf,
1740                100.0 - (SMC_BRANCH_COUNT - b / tf) / (0.01 * SMC_BRANCH_COUNT));
1741       g_fprintf (stderr, &quot;GSlice: MemChecker: %f entries per branch, %u minimum, %u maximum\n&quot;,
1742                su / bf, en, ex);
1743     }
1744   else
1745     g_fprintf (stderr, &quot;GSlice: MemChecker: root=NULL\n&quot;);
1746   g_mutex_unlock (&amp;smc_tree_mutex);
1747 
1748   /* sample statistics (beast + GSLice + 24h scripted core &amp; GUI activity):
1749    *  PID %CPU %MEM   VSZ  RSS      COMMAND
1750    * 8887 30.3 45.8 456068 414856   beast-0.7.1 empty.bse
1751    * $ cat /proc/8887/statm # total-program-size resident-set-size shared-pages text/code data/stack library dirty-pages
1752    * 114017 103714 2354 344 0 108676 0
1753    * $ cat /proc/8887/status
1754    * Name:   beast-0.7.1
1755    * VmSize:   456068 kB
1756    * VmLck:         0 kB
1757    * VmRSS:    414856 kB
1758    * VmData:   434620 kB
1759    * VmStk:        84 kB
1760    * VmExe:      1376 kB
1761    * VmLib:     13036 kB
1762    * VmPTE:       456 kB
1763    * Threads:        3
1764    * (gdb) print g_slice_debug_tree_statistics ()
1765    * GSlice: MemChecker: 422 trunks, 213068 branches, 0 old branches
1766    * GSlice: MemChecker: 504.900474 branches per trunk, 98.81% utilization
1767    * GSlice: MemChecker: 4.965039 entries per branch, 1 minimum, 37 maximum
1768    */
1769 }
1770 #endif /* G_ENABLE_DEBUG */
<a name="9" id="anc9"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="9" type="hidden" />
</body>
</html>