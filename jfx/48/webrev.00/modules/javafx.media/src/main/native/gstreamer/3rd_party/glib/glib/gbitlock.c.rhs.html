<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.media/src/main/native/gstreamer/3rd_party/glib/glib/gbitlock.c</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
<a name="1" id="anc1"></a><span class="line-modified">  2  * Copyright (C) 2008 Ryan Lortie</span>
<span class="line-modified">  3  * Copyright (C) 2010 Codethink Limited</span>
  4  *
  5  * This library is free software; you can redistribute it and/or
  6  * modify it under the terms of the GNU Lesser General Public
  7  * License as published by the Free Software Foundation; either
  8  * version 2.1 of the License, or (at your option) any later version.
  9  *
 10  * This library is distributed in the hope that it will be useful,
 11  * but WITHOUT ANY WARRANTY; without even the implied warranty of
 12  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 13  * Lesser General Public License for more details.
 14  *
 15  * You should have received a copy of the GNU Lesser General Public
 16  * License along with this library; if not, see &lt;http://www.gnu.org/licenses/&gt;.
 17  *
 18  * Author: Ryan Lortie &lt;desrt@desrt.ca&gt;
 19  */
 20 
 21 #include &quot;config.h&quot;
 22 
 23 #include &quot;gbitlock.h&quot;
 24 
 25 #include &lt;glib/gmessages.h&gt;
 26 #include &lt;glib/gatomic.h&gt;
 27 #include &lt;glib/gslist.h&gt;
 28 #include &lt;glib/gthread.h&gt;
 29 #include &lt;glib/gslice.h&gt;
 30 
 31 #include &quot;gthreadprivate.h&quot;
 32 
 33 #ifdef G_BIT_LOCK_FORCE_FUTEX_EMULATION
 34 #undef HAVE_FUTEX
 35 #endif
 36 
 37 #ifndef HAVE_FUTEX
 38 static GMutex g_futex_mutex;
 39 static GSList *g_futex_address_list = NULL;
 40 #endif
 41 
 42 #ifdef HAVE_FUTEX
 43 /*
 44  * We have headers for futex(2) on the build machine.  This does not
 45  * imply that every system that ever runs the resulting glib will have
 46  * kernel support for futex, but you&#39;d have to have a pretty old
 47  * kernel in order for that not to be the case.
 48  *
 49  * If anyone actually gets bit by this, please file a bug. :)
 50  */
 51 #include &lt;linux/futex.h&gt;
 52 #include &lt;sys/syscall.h&gt;
 53 #include &lt;unistd.h&gt;
 54 
 55 #ifndef FUTEX_WAIT_PRIVATE
 56 #define FUTEX_WAIT_PRIVATE FUTEX_WAIT
 57 #define FUTEX_WAKE_PRIVATE FUTEX_WAKE
 58 #endif
 59 
 60 /* &lt; private &gt;
 61  * g_futex_wait:
 62  * @address: a pointer to an integer
 63  * @value: the value that should be at @address
 64  *
 65  * Atomically checks that the value stored at @address is equal to
 66  * @value and then blocks.  If the value stored at @address is not
 67  * equal to @value then this function returns immediately.
 68  *
 69  * To unblock, call g_futex_wake() on @address.
 70  *
 71  * This call may spuriously unblock (for example, in response to the
 72  * process receiving a signal) but this is not guaranteed.  Unlike the
 73  * Linux system call of a similar name, there is no guarantee that a
 74  * waiting process will unblock due to a g_futex_wake() call in a
 75  * separate process.
 76  */
 77 static void
 78 g_futex_wait (const volatile gint *address,
 79               gint                 value)
 80 {
 81   syscall (__NR_futex, address, (gsize) FUTEX_WAIT_PRIVATE, (gsize) value, NULL);
 82 }
 83 
 84 /* &lt; private &gt;
 85  * g_futex_wake:
 86  * @address: a pointer to an integer
 87  *
 88  * Nominally, wakes one thread that is blocked in g_futex_wait() on
 89  * @address (if any thread is currently waiting).
 90  *
 91  * As mentioned in the documention for g_futex_wait(), spurious
 92  * wakeups may occur.  As such, this call may result in more than one
 93  * thread being woken up.
 94  */
 95 static void
 96 g_futex_wake (const volatile gint *address)
 97 {
 98   syscall (__NR_futex, address, (gsize) FUTEX_WAKE_PRIVATE, (gsize) 1, NULL);
 99 }
100 
101 #else
102 
103 /* emulate futex(2) */
104 typedef struct
105 {
106   const volatile gint *address;
107   gint                 ref_count;
108   GCond                wait_queue;
109 } WaitAddress;
110 
111 static WaitAddress *
112 g_futex_find_address (const volatile gint *address)
113 {
114   GSList *node;
115 
116   for (node = g_futex_address_list; node; node = node-&gt;next)
117     {
118       WaitAddress *waiter = node-&gt;data;
119 
120       if (waiter-&gt;address == address)
121         return waiter;
122     }
123 
124   return NULL;
125 }
126 
127 static void
128 g_futex_wait (const volatile gint *address,
129               gint                 value)
130 {
131   g_mutex_lock (&amp;g_futex_mutex);
132   if G_LIKELY (g_atomic_int_get (address) == value)
133     {
134       WaitAddress *waiter;
135 
136       if ((waiter = g_futex_find_address (address)) == NULL)
137         {
138           waiter = g_slice_new (WaitAddress);
139 #ifdef GSTREAMER_LITE
140           if (waiter == NULL) {
141             g_mutex_unlock (&amp;g_futex_mutex);
142             return;
143           }
144 #endif // GSTREAMER_LITE
145           waiter-&gt;address = address;
146           g_cond_init (&amp;waiter-&gt;wait_queue);
147           waiter-&gt;ref_count = 0;
148           g_futex_address_list =
149             g_slist_prepend (g_futex_address_list, waiter);
150         }
151 
152       waiter-&gt;ref_count++;
153       g_cond_wait (&amp;waiter-&gt;wait_queue, &amp;g_futex_mutex);
154 
155       if (!--waiter-&gt;ref_count)
156         {
157           g_futex_address_list =
158             g_slist_remove (g_futex_address_list, waiter);
159           g_cond_clear (&amp;waiter-&gt;wait_queue);
160           g_slice_free (WaitAddress, waiter);
161         }
162     }
163   g_mutex_unlock (&amp;g_futex_mutex);
164 }
165 
166 static void
167 g_futex_wake (const volatile gint *address)
168 {
169   WaitAddress *waiter;
170 
171   /* need to lock here for two reasons:
172    *   1) need to acquire/release lock to ensure waiter is not in
173    *      the process of registering a wait
174    *   2) need to -stay- locked until the end to ensure a wake()
175    *      in another thread doesn&#39;t cause &#39;waiter&#39; to stop existing
176    */
177   g_mutex_lock (&amp;g_futex_mutex);
178   if ((waiter = g_futex_find_address (address)))
179     g_cond_signal (&amp;waiter-&gt;wait_queue);
180   g_mutex_unlock (&amp;g_futex_mutex);
181 }
182 #endif
183 
184 #define CONTENTION_CLASSES 11
185 static volatile gint g_bit_lock_contended[CONTENTION_CLASSES];
186 
187 #if (defined (i386) || defined (__amd64__))
188   #if __GNUC__ &gt; 4 || (__GNUC__ == 4 &amp;&amp; __GNUC_MINOR__ &gt;= 5)
189     #define USE_ASM_GOTO 1
190   #endif
191 #endif
192 
193 /**
194  * g_bit_lock:
195  * @address: a pointer to an integer
196  * @lock_bit: a bit value between 0 and 31
197  *
198  * Sets the indicated @lock_bit in @address.  If the bit is already
199  * set, this call will block until g_bit_unlock() unsets the
200  * corresponding bit.
201  *
202  * Attempting to lock on two different bits within the same integer is
203  * not supported and will very probably cause deadlocks.
204  *
205  * The value of the bit that is set is (1u &lt;&lt; @bit).  If @bit is not
206  * between 0 and 31 then the result is undefined.
207  *
208  * This function accesses @address atomically.  All other accesses to
209  * @address must be atomic in order for this function to work
210  * reliably.
211  *
212  * Since: 2.24
213  **/
214 void
215 g_bit_lock (volatile gint *address,
216             gint           lock_bit)
217 {
218 #ifdef USE_ASM_GOTO
219  retry:
220   __asm__ volatile goto (&quot;lock bts %1, (%0)\n&quot;
221                          &quot;jc %l[contended]&quot;
222                          : /* no output */
223                          : &quot;r&quot; (address), &quot;r&quot; (lock_bit)
224                          : &quot;cc&quot;, &quot;memory&quot;
225                          : contended);
226   return;
227 
228  contended:
229   {
230     guint mask = 1u &lt;&lt; lock_bit;
231     guint v;
232 
233     v = g_atomic_int_get (address);
234     if (v &amp; mask)
235       {
236         guint class = ((gsize) address) % G_N_ELEMENTS (g_bit_lock_contended);
237 
238         g_atomic_int_add (&amp;g_bit_lock_contended[class], +1);
239         g_futex_wait (address, v);
240         g_atomic_int_add (&amp;g_bit_lock_contended[class], -1);
241       }
242   }
243   goto retry;
244 #else
245   guint mask = 1u &lt;&lt; lock_bit;
246   guint v;
247 
248  retry:
249   v = g_atomic_int_or (address, mask);
250   if (v &amp; mask)
251     /* already locked */
252     {
253       guint class = ((gsize) address) % G_N_ELEMENTS (g_bit_lock_contended);
254 
255       g_atomic_int_add (&amp;g_bit_lock_contended[class], +1);
256       g_futex_wait (address, v);
257       g_atomic_int_add (&amp;g_bit_lock_contended[class], -1);
258 
259       goto retry;
260     }
261 #endif
262 }
263 
264 /**
265  * g_bit_trylock:
266  * @address: a pointer to an integer
267  * @lock_bit: a bit value between 0 and 31
268  *
269  * Sets the indicated @lock_bit in @address, returning %TRUE if
270  * successful.  If the bit is already set, returns %FALSE immediately.
271  *
272  * Attempting to lock on two different bits within the same integer is
273  * not supported.
274  *
275  * The value of the bit that is set is (1u &lt;&lt; @bit).  If @bit is not
276  * between 0 and 31 then the result is undefined.
277  *
278  * This function accesses @address atomically.  All other accesses to
279  * @address must be atomic in order for this function to work
280  * reliably.
281  *
282  * Returns: %TRUE if the lock was acquired
283  *
284  * Since: 2.24
285  **/
286 gboolean
287 g_bit_trylock (volatile gint *address,
288                gint           lock_bit)
289 {
290 #ifdef USE_ASM_GOTO
291   gboolean result;
292 
293   __asm__ volatile (&quot;lock bts %2, (%1)\n&quot;
294                     &quot;setnc %%al\n&quot;
295                     &quot;movzx %%al, %0&quot;
296                     : &quot;=r&quot; (result)
297                     : &quot;r&quot; (address), &quot;r&quot; (lock_bit)
298                     : &quot;cc&quot;, &quot;memory&quot;);
299 
300   return result;
301 #else
302   guint mask = 1u &lt;&lt; lock_bit;
303   guint v;
304 
305   v = g_atomic_int_or (address, mask);
306 
307   return ~v &amp; mask;
308 #endif
309 }
310 
311 /**
312  * g_bit_unlock:
313  * @address: a pointer to an integer
314  * @lock_bit: a bit value between 0 and 31
315  *
316  * Clears the indicated @lock_bit in @address.  If another thread is
317  * currently blocked in g_bit_lock() on this same bit then it will be
318  * woken up.
319  *
320  * This function accesses @address atomically.  All other accesses to
321  * @address must be atomic in order for this function to work
322  * reliably.
323  *
324  * Since: 2.24
325  **/
326 void
327 g_bit_unlock (volatile gint *address,
328               gint           lock_bit)
329 {
330 #ifdef USE_ASM_GOTO
331   asm volatile (&quot;lock btr %1, (%0)&quot;
332                 : /* no output */
333                 : &quot;r&quot; (address), &quot;r&quot; (lock_bit)
334                 : &quot;cc&quot;, &quot;memory&quot;);
335 #else
336   guint mask = 1u &lt;&lt; lock_bit;
337 
338   g_atomic_int_and (address, ~mask);
339 #endif
340 
341   {
342     guint class = ((gsize) address) % G_N_ELEMENTS (g_bit_lock_contended);
343 
344     if (g_atomic_int_get (&amp;g_bit_lock_contended[class]))
345       g_futex_wake (address);
346   }
347 }
348 
349 
350 /* We emulate pointer-sized futex(2) because the kernel API only
351  * supports integers.
352  *
353  * We assume that the &#39;interesting&#39; part is always the lower order bits.
354  * This assumption holds because pointer bitlocks are restricted to
355  * using the low order bits of the pointer as the lock.
356  *
357  * On 32 bits, there is nothing to do since the pointer size is equal to
358  * the integer size.  On little endian the lower-order bits don&#39;t move,
359  * so do nothing.  Only on 64bit big endian do we need to do a bit of
360  * pointer arithmetic: the low order bits are shifted by 4 bytes.  We
361  * have a helper function that always does the right thing here.
362  *
363  * Since we always consider the low-order bits of the integer value, a
364  * simple cast from (gsize) to (guint) always takes care of that.
365  *
366  * After that, pointer-sized futex becomes as simple as:
367  *
368  *   g_futex_wait (g_futex_int_address (address), (guint) value);
369  *
370  * and
371  *
372  *   g_futex_wake (g_futex_int_address (int_address));
373  */
374 static const volatile gint *
375 g_futex_int_address (const volatile void *address)
376 {
377   const volatile gint *int_address = address;
378 
379   /* this implementation makes these (reasonable) assumptions: */
380   G_STATIC_ASSERT (G_BYTE_ORDER == G_LITTLE_ENDIAN ||
381       (G_BYTE_ORDER == G_BIG_ENDIAN &amp;&amp;
382        sizeof (int) == 4 &amp;&amp;
383        (sizeof (gpointer) == 4 || sizeof (gpointer) == 8)));
384 
385 #if G_BYTE_ORDER == G_BIG_ENDIAN &amp;&amp; GLIB_SIZEOF_VOID_P == 8
386   int_address++;
387 #endif
388 
389   return int_address;
390 }
391 
392 /**
393  * g_pointer_bit_lock:
394  * @address: (not nullable): a pointer to a #gpointer-sized value
395  * @lock_bit: a bit value between 0 and 31
396  *
397  * This is equivalent to g_bit_lock, but working on pointers (or other
398  * pointer-sized values).
399  *
400  * For portability reasons, you may only lock on the bottom 32 bits of
401  * the pointer.
402  *
403  * Since: 2.30
404  **/
405 void
406 (g_pointer_bit_lock) (volatile void *address,
407                       gint           lock_bit)
408 {
409   g_return_if_fail (lock_bit &lt; 32);
410 
411   {
412 #ifdef USE_ASM_GOTO
413  retry:
414     asm volatile goto (&quot;lock bts %1, (%0)\n&quot;
415                        &quot;jc %l[contended]&quot;
416                        : /* no output */
417                        : &quot;r&quot; (address), &quot;r&quot; ((gsize) lock_bit)
418                        : &quot;cc&quot;, &quot;memory&quot;
419                        : contended);
420     return;
421 
422  contended:
423     {
424       volatile gsize *pointer_address = address;
425       gsize mask = 1u &lt;&lt; lock_bit;
426       gsize v;
427 
428       v = (gsize) g_atomic_pointer_get (pointer_address);
429       if (v &amp; mask)
430         {
431           guint class = ((gsize) address) % G_N_ELEMENTS (g_bit_lock_contended);
432 
433           g_atomic_int_add (&amp;g_bit_lock_contended[class], +1);
434           g_futex_wait (g_futex_int_address (address), v);
435           g_atomic_int_add (&amp;g_bit_lock_contended[class], -1);
436         }
437     }
438     goto retry;
439 #else
440   volatile gsize *pointer_address = address;
441   gsize mask = 1u &lt;&lt; lock_bit;
442   gsize v;
443 
444  retry:
445   v = g_atomic_pointer_or (pointer_address, mask);
446   if (v &amp; mask)
447     /* already locked */
448     {
449       guint class = ((gsize) address) % G_N_ELEMENTS (g_bit_lock_contended);
450 
451       g_atomic_int_add (&amp;g_bit_lock_contended[class], +1);
452       g_futex_wait (g_futex_int_address (address), (guint) v);
453       g_atomic_int_add (&amp;g_bit_lock_contended[class], -1);
454 
455       goto retry;
456     }
457 #endif
458   }
459 }
460 
461 /**
462  * g_pointer_bit_trylock:
463  * @address: (not nullable): a pointer to a #gpointer-sized value
464  * @lock_bit: a bit value between 0 and 31
465  *
466  * This is equivalent to g_bit_trylock, but working on pointers (or
467  * other pointer-sized values).
468  *
469  * For portability reasons, you may only lock on the bottom 32 bits of
470  * the pointer.
471  *
472  * Returns: %TRUE if the lock was acquired
473  *
474  * Since: 2.30
475  **/
476 gboolean
477 (g_pointer_bit_trylock) (volatile void *address,
478                          gint           lock_bit)
479 {
480   g_return_val_if_fail (lock_bit &lt; 32, FALSE);
481 
482   {
483 #ifdef USE_ASM_GOTO
484     gboolean result;
485 
486     asm volatile (&quot;lock bts %2, (%1)\n&quot;
487                   &quot;setnc %%al\n&quot;
488                   &quot;movzx %%al, %0&quot;
489                   : &quot;=r&quot; (result)
490                   : &quot;r&quot; (address), &quot;r&quot; ((gsize) lock_bit)
491                   : &quot;cc&quot;, &quot;memory&quot;);
492 
493     return result;
494 #else
495     volatile gsize *pointer_address = address;
496     gsize mask = 1u &lt;&lt; lock_bit;
497     gsize v;
498 
499     g_return_val_if_fail (lock_bit &lt; 32, FALSE);
500 
501     v = g_atomic_pointer_or (pointer_address, mask);
502 
503     return ~v &amp; mask;
504 #endif
505   }
506 }
507 
508 /**
509  * g_pointer_bit_unlock:
510  * @address: (not nullable): a pointer to a #gpointer-sized value
511  * @lock_bit: a bit value between 0 and 31
512  *
513  * This is equivalent to g_bit_unlock, but working on pointers (or other
514  * pointer-sized values).
515  *
516  * For portability reasons, you may only lock on the bottom 32 bits of
517  * the pointer.
518  *
519  * Since: 2.30
520  **/
521 void
522 (g_pointer_bit_unlock) (volatile void *address,
523                         gint           lock_bit)
524 {
525   g_return_if_fail (lock_bit &lt; 32);
526 
527   {
528 #ifdef USE_ASM_GOTO
529     asm volatile (&quot;lock btr %1, (%0)&quot;
530                   : /* no output */
531                   : &quot;r&quot; (address), &quot;r&quot; ((gsize) lock_bit)
532                   : &quot;cc&quot;, &quot;memory&quot;);
533 #else
534     volatile gsize *pointer_address = address;
535     gsize mask = 1u &lt;&lt; lock_bit;
536 
537     g_atomic_pointer_and (pointer_address, ~mask);
538 #endif
539 
540     {
541       guint class = ((gsize) address) % G_N_ELEMENTS (g_bit_lock_contended);
542       if (g_atomic_int_get (&amp;g_bit_lock_contended[class]))
543         g_futex_wake (g_futex_int_address (address));
544     }
545   }
546 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>